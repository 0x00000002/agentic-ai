{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agentic-AI","text":"<p>A modular framework for building AI applications with tool integration capabilities.</p>"},{"location":"#overview","title":"Overview","text":"<p>Agentic-AI is a Python library designed to create AI-powered applications that can:</p> <ul> <li>Use multiple AI model providers (OpenAI, Anthropic, Google, Ollama)</li> <li>Dynamically discover and call tools based on user input</li> <li>Manage conversations and maintain context</li> <li>Template and version prompts with metrics tracking</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multiple Provider Support: Use models from OpenAI, Anthropic, Google, and Ollama seamlessly</li> <li>Tool Integration: Register Python functions as tools the AI can use</li> <li>Automatic Tool Discovery: AI-powered selection of relevant tools based on user queries</li> <li>Prompt Management: Create, version, and track performance of prompt templates</li> <li>Conversation Management: Maintain context across multiple interactions</li> </ul>"},{"location":"PROMPTS/","title":"Prompt Templates","text":"<p>This document describes the template system in Agentic-AI, which provides a way to create, manage, and reuse prompt templates with variable substitution and performance tracking.</p>"},{"location":"PROMPTS/#overview","title":"Overview","text":"<p>The prompt template system allows you to:</p> <ol> <li>Define reusable prompt patterns with variable placeholders</li> <li>Version and track prompt performance</li> <li>Substitute variables into templates</li> <li>Collect metrics on template usage</li> </ol>"},{"location":"PROMPTS/#template-format","title":"Template Format","text":"<p>Templates are defined in YAML files in the <code>src/prompts/templates</code> directory. Each template has:</p> <ul> <li>ID: Unique identifier for the template</li> <li>Name: Human-readable name</li> <li>Description: Description of the template's purpose</li> <li>Versions: Multiple versions of the template (for A/B testing or iteration)</li> <li>Default Version: Which version to use by default</li> </ul> <p>Example template file (<code>analysis.yml</code>):</p> <pre><code>analyze_request:\n  name: \"Request Analysis\"\n  description: \"Template for analyzing user requests to determine appropriate agents\"\n  default_version: \"v1\"\n  versions:\n    - version: \"v1\"\n      template: |\n        Analyze this user request and determine which specialized agents should handle it:\n\n        Request: {{prompt}}\n\n        Available agents:\n        {{agent_list}}\n\n        Return a JSON list of [agent_id, confidence] pairs, where confidence is 0.0-1.0.\n        Only include agents with confidence &gt; {{confidence_threshold}}. If no agents are appropriate, return [].\n</code></pre>"},{"location":"PROMPTS/#variable-substitution","title":"Variable Substitution","text":"<p>Templates use a simple variable substitution syntax with double curly braces:</p> <ul> <li><code>{{variable_name}}</code> - Will be replaced with the value of <code>variable_name</code></li> </ul>"},{"location":"PROMPTS/#using-templates-in-code","title":"Using Templates in Code","text":""},{"location":"PROMPTS/#basic-usage","title":"Basic Usage","text":"<pre><code>from src.prompts.prompt_template import PromptTemplate\n\n# Create a template service\ntemplate_service = PromptTemplate()\n\n# Render a template with variables\nprompt, usage_id = template_service.render_prompt(\n    template_id=\"analyze_request\",\n    variables={\n        \"prompt\": \"What's the weather like in Paris?\",\n        \"agent_list\": \"- weather_agent: Gets weather information\\n- translator: Translates text\",\n        \"confidence_threshold\": 0.6\n    }\n)\n\nprint(prompt)\n\n# Record performance metrics\ntemplate_service.record_prompt_performance(\n    usage_id=usage_id,\n    metrics={\n        \"success\": True,\n        \"tokens\": 150,\n        \"latency\": 0.75\n    }\n)\n</code></pre>"},{"location":"PROMPTS/#with-specific-version","title":"With Specific Version","text":"<pre><code>prompt, usage_id = template_service.render_prompt(\n    template_id=\"analyze_request\",\n    variables={\"prompt\": \"What's the weather like in Paris?\"},\n    version=\"v2\"  # Use a specific version\n)\n</code></pre>"},{"location":"PROMPTS/#with-additional-context","title":"With Additional Context","text":"<pre><code>prompt, usage_id = template_service.render_prompt(\n    template_id=\"analyze_request\",\n    variables={\"prompt\": \"What's the weather like in Paris?\"},\n    context={\"model\": \"gpt-4\", \"user_id\": \"user123\"}  # Additional context\n)\n</code></pre>"},{"location":"PROMPTS/#creating-new-templates","title":"Creating New Templates","text":"<p>To create a new template:</p> <ol> <li>Create or edit a YAML file in <code>src/prompts/templates/</code></li> <li>Define your template with an ID, name, description, and versions</li> <li>Use <code>{{variable_name}}</code> syntax for variables</li> <li>Set a default version if you have multiple versions</li> </ol> <p>Or programmatically:</p> <pre><code>template_service = PromptTemplate()\n\ntemplate_service.add_template(\n    template_id=\"my_template\",\n    template_data={\n        \"name\": \"My Template\",\n        \"description\": \"A template for...\",\n        \"default_version\": \"v1\",\n        \"versions\": [\n            {\n                \"version\": \"v1\",\n                \"template\": \"This is a template with {{variable}}\"\n            }\n        ]\n    }\n)\n</code></pre>"},{"location":"PROMPTS/#performance-tracking","title":"Performance Tracking","text":"<p>The template system automatically tracks:</p> <ul> <li>Start time: When the template was rendered</li> <li>Variables used: What values were substituted</li> <li>Template version: Which version was used</li> </ul> <p>You can add additional metrics when recording performance:</p> <pre><code>template_service.record_prompt_performance(\n    usage_id=usage_id,\n    metrics={\n        \"success\": True,        # Whether the response was successful\n        \"latency\": 1.25,        # Processing time in seconds\n        \"tokens\": 150,          # Tokens used\n        \"model\": \"claude-3-5\",  # Model used\n        \"custom_metric\": 0.95   # Any custom metrics you want to track\n    }\n)\n</code></pre> <p>Metrics are saved to <code>src/prompts/templates/metrics/</code> as JSON files, which can be analyzed to improve prompts over time.</p>"},{"location":"agent_descriptions/","title":"Agent Descriptions","text":"<p>This document explains how agent descriptions are managed in the Agentic-AI framework.</p>"},{"location":"agent_descriptions/#overview","title":"Overview","text":"<p>Agent descriptions are metadata that describe the capabilities and purpose of each agent type in the system. These descriptions are used by the Orchestrator to make intelligent decisions about which agents should handle user requests.</p>"},{"location":"agent_descriptions/#configuration","title":"Configuration","text":"<p>Agent descriptions are stored in the <code>agents.yml</code> file under the <code>agent_descriptions</code> section:</p> <pre><code># Agent descriptions for AI prompting and routing\nagent_descriptions:\n  listener: \"Handles audio processing and speech recognition\"\n  translator: \"Translates between languages\"\n  website_parser: \"Searches websites for information\"\n  content_generator: \"Creates multimedia content (images, videos, audio)\"\n  action_planner: \"Breaks complex tasks into subtasks\"\n  mcp_searcher: \"Finds relevant Model-Centric Processes\"\n  paralleliser: \"Executes tasks in parallel\"\n  tool_finder: \"Identifies relevant tools for user requests\"\n  orchestrator: \"Routes requests to appropriate specialized agents\"\n</code></pre>"},{"location":"agent_descriptions/#accessing-agent-descriptions","title":"Accessing Agent Descriptions","text":"<p>The <code>ConfigManager</code> provides methods to access agent descriptions:</p> <pre><code># Get description for a specific agent\ndescription = config_manager.get_agent_description(\"tool_finder\")\nprint(description)  # \"Identifies relevant tools for user requests\"\n\n# Get all agent descriptions\nall_descriptions = config_manager.get_all_agent_descriptions()\nfor agent_type, description in all_descriptions.items():\n    print(f\"{agent_type}: {description}\")\n</code></pre>"},{"location":"agent_descriptions/#usage-in-the-orchestrator","title":"Usage in the Orchestrator","text":"<p>The Orchestrator uses agent descriptions to create prompts for the AI model when analyzing user requests:</p> <pre><code># Get agent descriptions from the config manager\nagent_descriptions = config_manager.get_all_agent_descriptions()\n\n# Format agent list for prompt\nagent_list = \"\"\nfor agent in available_agents:\n    description = agent_descriptions.get(agent, f\"Agent type: {agent}\")\n    agent_list += f\"- {agent}: {description}\\n\"\n\n# Create prompt for AI\nprompt = f\"\"\"Analyze this user request and determine which specialized agents should handle it:\nRequest: {user_request}\n\nAvailable agents:\n{agent_list}\n\nReturn a JSON list of [agent_name, confidence] pairs, where confidence is 0.0-1.0.\nOnly include agents with confidence &gt; {confidence_threshold}. If no agents are appropriate, return [].\n\"\"\"\n</code></pre>"},{"location":"agent_descriptions/#benefits-of-configuration-based-descriptions","title":"Benefits of Configuration-Based Descriptions","text":"<ol> <li>Separation of Concerns: Agent descriptions are configuration data, not implementation details.</li> <li>Easier Maintenance: Descriptions can be updated without changing code.</li> <li>Consistency: All components access the same descriptions from a single source.</li> <li>Flexibility: Descriptions can be customized for different deployments.</li> </ol>"},{"location":"agent_descriptions/#example","title":"Example","text":"<p>See the <code>examples/agent_descriptions_example.py</code> script for a complete demonstration of how to use agent descriptions from the configuration system.</p>"},{"location":"architecture/","title":"Agentic-AI Architecture","text":"<p>This document describes the architecture of the Agentic-AI framework, focusing on the core design principles and component relationships.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>Agentic-AI is a modular framework for building AI applications with tool integration capabilities. The architecture follows solid software engineering principles including:</p> <ul> <li>Separation of Concerns: Each component has a clear, focused responsibility</li> <li>Dependency Injection: Components receive their dependencies rather than creating them</li> <li>Interface-Based Design: Components interact through well-defined interfaces</li> <li>Error Handling: Standardized error handling across the framework</li> <li>Configuration Management: Modular, externalized configuration</li> </ul>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#ai-core","title":"AI Core","text":"<p>The AI core provides the foundation for interacting with language models:</p> <ul> <li>AIBase: Base implementation of the AI interface.</li> <li>ToolEnabledAI: Extended implementation of <code>AIBase</code> with tool integration capabilities.</li> <li>Providers: Abstraction layer for different AI providers (<code>BaseProvider</code>, <code>OpenAIProvider</code>, etc.).</li> <li>ProviderFactory: Creates provider instances based on configuration.</li> <li>Interfaces: Clear contracts for component interactions (e.g., <code>AIInterface</code>, <code>ProviderInterface</code>).</li> </ul>"},{"location":"architecture/#multi-agent-system","title":"Multi-Agent System","text":"<p>The multi-agent system enables specialized processing of user requests:</p> <ul> <li>Coordinator: Coordinates the workflow between specialized agents (current implementation).</li> <li>RequestAnalyzer: Analyzes requests to determine appropriate agents and tools.</li> <li>ResponseAggregator: Combines responses from multiple agents.</li> <li>BaseAgent: Common functionality for all agents.</li> </ul>"},{"location":"architecture/#tool-subsystem","title":"Tool Subsystem","text":"<p>Handles tool definition, management, and execution:</p> <ul> <li>ToolManager: Coordinates tool registration, discovery (via registry), and execution.</li> <li>ToolRegistry: Manages tool definitions and provider-specific formatting.</li> <li>ToolExecutor: Executes tool functions with timeout and retries.</li> <li>Models: Pydantic models for <code>ToolDefinition</code>, <code>ToolCall</code>, <code>ToolResult</code>.</li> </ul>"},{"location":"architecture/#configuration-management","title":"Configuration Management","text":"<p>Configuration is modularized for better maintainability:</p> <ul> <li>UnifiedConfig: Central singleton class for accessing merged configuration data.</li> <li>Modular Config Files: Separate files for models, providers, agents, tools, etc.</li> </ul>"},{"location":"architecture/#error-handling","title":"Error Handling","text":"<p>The error handling system provides consistent error management:</p> <ul> <li>ErrorHandler: Centralized error handling with standardized responses</li> <li>Exception Hierarchy: Well-defined exception types for different error scenarios</li> </ul>"},{"location":"architecture/#dependency-management","title":"Dependency Management","text":"<p>Dependencies between components are primarily managed through:</p> <ul> <li>Dependency Injection: Components like <code>Coordinator</code> and <code>ToolEnabledAI</code> receive dependencies (e.g., <code>ToolManager</code>, <code>ProviderInterface</code>) via their constructors.</li> <li>Factories: Specific factories like <code>ProviderFactory</code> are used to create instances of components (e.g., providers) based on configuration.</li> <li>Singleton Access: Core configuration (<code>UnifiedConfig</code>) and logging (<code>LoggerFactory</code>) often use singleton patterns for easy access.</li> </ul> <p>This approach promotes loose coupling and testability.</p>"},{"location":"architecture/#component-relationships","title":"Component Relationships","text":"<p>(Diagram needs update to reflect current structure, including ToolManager, ToolEnabledAI, Coordinator, ProviderFactory, UnifiedConfig)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  \u2502     \u2502                   \u2502     \u2502                   \u2502\n\u2502  ConfigFactory   \u2502\u25c4\u2500\u2500\u2500\u2500\u2524   AppContainer    \u2502\u2500\u2500\u2500\u2500\u25ba\u2502   ErrorHandler    \u2502\n\u2502                  \u2502     \u2502                   \u2502     \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                        \u2502\n        \u2502                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  \u2502     \u2502                   \u2502\n\u2502  ProviderFactory \u2502     \u2502   AIBase / AI     \u2502\n\u2502                  \u2502     \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                        \u2502\n        \u2502                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  \u2502     \u2502                   \u2502     \u2502                   \u2502\n\u2502    Providers     \u2502     \u2502   Orchestrator    \u2502\u2500\u2500\u2500\u2500\u25ba\u2502  RequestAnalyzer  \u2502\n\u2502                  \u2502     \u2502                   \u2502     \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502                   \u2502     \u2502                   \u2502\n                         \u2502  Specialized      \u2502\u2500\u2500\u2500\u2500\u25ba\u2502 ResponseAggregator\u2502\n                         \u2502  Agents           \u2502     \u2502                   \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#key-architectural-improvements","title":"Key Architectural Improvements","text":""},{"location":"architecture/#1-refactored-coordinator-orchestration","title":"1. Refactored Coordinator / Orchestration","text":"<p>The orchestration logic, now primarily in the <code>Coordinator</code> agent, is supported by focused components:</p> <ul> <li>Coordinator: Coordinates the overall workflow (delegating or handling directly).</li> <li>RequestAnalyzer: Handles request intent classification.</li> <li>ResponseAggregator: (If used) Handles response aggregation.</li> </ul> <p>This separation improves maintainability.</p>"},{"location":"architecture/#2-modularized-configuration","title":"2. Modularized Configuration","text":"<p>Configuration remains separated into domain-specific files (models, providers, agents, tools, etc.) managed by the central <code>UnifiedConfig</code>.</p>"},{"location":"architecture/#3-standardized-error-handling","title":"3. Standardized Error Handling","text":"<p>A consistent error handling approach using a hierarchy based on <code>AIFrameworkError</code> and specific exceptions like <code>AIProviderError</code>, <code>AIAuthenticationError</code>, <code>AIToolError</code>, etc., improves error visibility and debugging.</p>"},{"location":"architecture/#4-standardized-provider-interface","title":"4. Standardized Provider Interface","text":"<p>The <code>BaseProvider</code> class enforces a standard structure (<code>_prepare_request_payload</code>, <code>_make_api_request</code>, <code>_convert_response</code>) for all provider implementations (OpenAI, Anthropic, Gemini, Ollama), simplifying integration and interaction logic in core AI classes. Providers now return a standardized <code>ProviderResponse</code> object.</p>"},{"location":"architecture/#5-simplified-tool-subsystem","title":"5. Simplified Tool Subsystem","text":"<p>The tool system has been streamlined:</p> <ul> <li><code>ToolManager</code> coordinates execution via <code>ToolExecutor</code> and registration/formatting via <code>ToolRegistry</code>.</li> <li>Redundant components (<code>AIToolFinder</code>, <code>tool_prompt_builder.py</code>, <code>tool_call.py</code>) were removed.</li> <li><code>ToolEnabledAI</code> handles the tool-calling loop using the standardized provider responses and <code>ToolManager</code>.</li> </ul>"},{"location":"architecture/#6-dependency-injection-and-factories","title":"6. Dependency Injection and Factories","text":"<p>Explicit dependency injection and factories (like <code>ProviderFactory</code>) are used instead of a single container, promoting clearer dependency flows.</p>"},{"location":"architecture/#usage-example","title":"Usage Example","text":"<pre><code>from src.core import ToolEnabledAI\nfrom src.agents import Coordinator\nfrom src.config import configure\n# from your_tools_module import get_weather # Example tool function\n\n# Configure if needed (optional, uses defaults otherwise)\n# configure(model=\"gpt-4o\")\n\n# Create a ToolEnabledAI instance\nai = ToolEnabledAI(model=\"gpt-4o\") # Or any other configured model\n\n# Tools are typically registered via ToolManager or configuration,\n# but if manual registration on ToolEnabledAI's manager is needed:\n# tool_def = ToolDefinition(...) # Define your tool\n# ai._tool_manager.register_tool(\"get_weather\", tool_def)\n\n# Make a request that might use tools\n# Use process_prompt for automatic tool handling\nresponse_content = ai.process_prompt(\"What's the weather like in Paris today?\")\nprint(response_content)\n\n# Create a Coordinator instance (uses default dependencies)\ncoordinator = Coordinator()\n\n# Process a request with the coordinator\nresponse = coordinator.process_request({\n    \"prompt\": \"Translate this text to French and analyze the sentiment\",\n    \"context\": {\"source_language\": \"en\"}\n})\n\n# Print the final content from the coordinator's response\nprint(response.get(\"content\"))\n</code></pre>"},{"location":"configuration_system/","title":"Configuration System","text":""},{"location":"configuration_system/#overview","title":"Overview","text":"<p>The configuration system is designed to be modular, extensible, and easy to use. It provides a centralized way to manage configurations for models, agents, and use cases. The system uses a hybrid approach that combines default configurations with the ability to override them with external configurations.</p>"},{"location":"configuration_system/#architecture","title":"Architecture","text":"<p>The configuration system consists of the following main components:</p> <ul> <li><code>ConfigFactory</code>: A singleton class that manages the creation and access to specialized configuration managers.</li> <li><code>BaseConfigManager</code>: A base class for all configuration managers.</li> <li>Specialized managers:</li> <li><code>ModelConfigManager</code>: Manages AI model configurations.</li> <li><code>AgentConfigManager</code>: Manages agent configurations.</li> <li><code>UseCaseConfigManager</code>: Manages use case configurations.</li> </ul>"},{"location":"configuration_system/#configuration-files","title":"Configuration Files","text":"<p>The system uses two main configuration files:</p> <ol> <li><code>config.yml</code>: Contains base configurations for models and use cases.</li> <li><code>agents.yml</code>: Contains agent-specific configurations.</li> </ol> <p>These files are located in the <code>src/config</code> directory and serve as default configurations. Users can override these defaults by providing their own configuration files.</p>"},{"location":"configuration_system/#default-configurations","title":"Default Configurations","text":"<p>The system comes with default configuration files that provide a good starting point for most use cases. These files are:</p> <ul> <li><code>src/config/config.yml</code>: Contains default model and use case configurations.</li> <li><code>src/config/agents.yml</code>: Contains default agent configurations.</li> </ul>"},{"location":"configuration_system/#overriding-default-configurations","title":"Overriding Default Configurations","text":"<p>Users can override the default configurations by providing their own configuration files:</p> <pre><code>from src.config.config_factory import ConfigFactory\n\n# Initialize with custom config files\nconfig_factory = ConfigFactory.get_instance(\n    config_path=\"path/to/custom/config.yml\",\n    agent_config_path=\"path/to/custom/agents.yml\"\n)\n</code></pre> <p>If no custom configuration files are provided, the system will use the default configurations.</p>"},{"location":"configuration_system/#specialized-managers","title":"Specialized Managers","text":""},{"location":"configuration_system/#modelconfigmanager","title":"ModelConfigManager","text":"<p>The <code>ModelConfigManager</code> is responsible for managing AI model configurations. It provides methods to:</p> <ul> <li>Get available models</li> <li>Get model information</li> <li>Get model configuration</li> </ul> <p>Example usage:</p> <pre><code>from src.config.config_factory import ConfigFactory\n\n# Get the ConfigFactory instance\nconfig_factory = ConfigFactory.get_instance()\n\n# Get available models\nmodels = config_factory.model_config.get_available_models()\n\n# Get model info\nmodel_info = config_factory.model_config.get_model_info(\"gpt-4\")\n\n# Get model config\nmodel_config = config_factory.model_config.get_model_config(\"gpt-4\")\n</code></pre>"},{"location":"configuration_system/#agentconfigmanager","title":"AgentConfigManager","text":"<p>The <code>AgentConfigManager</code> is responsible for managing agent configurations. It provides methods to:</p> <ul> <li>Get available agents</li> <li>Get agent information</li> <li>Get agent configuration</li> </ul> <p>Example usage:</p> <pre><code>from src.config.config_factory import ConfigFactory\n\n# Get the ConfigFactory instance\nconfig_factory = ConfigFactory.get_instance()\n\n# Get available agents\nagents = config_factory.agent_config.get_available_agents()\n\n# Get agent info\nagent_info = config_factory.agent_config.get_agent_info(\"default\")\n\n# Get agent config\nagent_config = config_factory.agent_config.get_agent_config(\"default\")\n</code></pre>"},{"location":"configuration_system/#usecaseconfigmanager","title":"UseCaseConfigManager","text":"<p>The <code>UseCaseConfigManager</code> is responsible for managing use case configurations. It provides methods to:</p> <ul> <li>Get available use cases</li> <li>Get use case information</li> <li>Get use case configuration</li> </ul> <p>Example usage:</p> <pre><code>from src.config.config_factory import ConfigFactory\n\n# Get the ConfigFactory instance\nconfig_factory = ConfigFactory.get_instance()\n\n# Get available use cases\nusecases = config_factory.usecase_config.get_available_usecases()\n\n# Get use case info\nusecase_info = config_factory.usecase_config.get_usecase_info(\"default\")\n\n# Get use case config\nusecase_config = config_factory.usecase_config.get_usecase_config(\"default\")\n</code></pre>"},{"location":"configuration_system/#configfactory","title":"ConfigFactory","text":"<p>The <code>ConfigFactory</code> is a singleton class that manages the creation and access to specialized configuration managers. It provides methods to:</p> <ul> <li>Get the singleton instance</li> <li>Reset the singleton instance</li> <li>Get specialized configuration managers</li> </ul> <p>Example usage:</p> <pre><code>from src.config.config_factory import ConfigFactory\n\n# Get the ConfigFactory instance\nconfig_factory = ConfigFactory.get_instance()\n\n# Get specialized managers\nmodel_config = config_factory.model_config\nagent_config = config_factory.agent_config\nusecase_config = config_factory.usecase_config\n\n# Reset the singleton instance\nConfigFactory.reset()\n</code></pre>"},{"location":"configuration_system/#benefits","title":"Benefits","text":"<p>The configuration system provides several benefits:</p> <ol> <li>Separation of Concerns: Each manager is responsible for a specific type of configuration.</li> <li>Improved Testability: Each manager can be tested independently.</li> <li>Better Extensibility: New configuration managers can be added easily.</li> <li>Clearer API: Each manager provides a clear and focused API.</li> <li>Reduced Complexity: The system is easier to understand and maintain.</li> <li>Default Configurations: The system comes with default configurations that can be overridden.</li> <li>Singleton Pattern: The <code>ConfigFactory</code> uses the singleton pattern to ensure a single instance is used throughout the application.</li> </ol>"},{"location":"configuration_system/#example","title":"Example","text":"<p>See <code>examples/config_system_example.py</code> for a complete example of how to use the configuration system.</p> <p>The configuration system manages settings for various components of the AI framework. Key configuration files include:</p> <ul> <li><code>models.yml</code>: Defines AI model parameters, capabilities, and costs.</li> <li><code>agents.yml</code>: Contains agent-specific configurations.</li> <li><code>providers.yml</code>: Configures API providers (e.g., OpenAI, Anthropic).</li> <li> <p><code>use_cases.yml</code>: Defines settings for different task types (e.g., chat, coding).</p> </li> <li> <p><code>src/config/models.yml</code>: Contains base model definitions.</p> </li> <li><code>src/config/agents.yml</code>: Contains default agent configurations.</li> <li><code>src/config/providers.yml</code>: Contains API provider settings.</li> <li><code>src/config/use_cases.yml</code>: Defines default settings for different use cases.</li> </ul> <p>See <code>UserConfig</code> class and <code>UnifiedConfig._apply_user_config</code> for details.</p> <pre><code>python your_app.py --config-file path/to/user_config.yml\n# OR using environment variables:\nexport APP_MODEL=\"gpt-4o\"\nexport APP_TEMPERATURE=\"0.8\"\n# OR command line arguments:\npython your_app.py --model gpt-4o --temperature 0.8\n</code></pre>"},{"location":"error_handling/","title":"Error Handling","text":"<p>The Agentic-AI framework uses a custom exception hierarchy to provide detailed error information. All framework-specific exceptions inherit from <code>AIFrameworkError</code>.</p>"},{"location":"error_handling/#base-exception","title":"Base Exception","text":"<ul> <li><code>AIFrameworkError</code>: The base class for all custom exceptions in this framework.</li> </ul>"},{"location":"error_handling/#configuration-errors","title":"Configuration Errors","text":"<ul> <li><code>AIConfigError</code>: Base class for configuration-related issues.</li> <li><code>AIConfigLoadingError</code>: Error during loading of configuration files.</li> <li><code>AIConfigValidationError</code>: Invalid configuration values found.</li> </ul>"},{"location":"error_handling/#provider-errors","title":"Provider Errors","text":"<ul> <li><code>AIProviderError</code>: Base class for errors originating from AI model providers.</li> <li><code>AIAuthenticationError</code>: Authentication issues with the provider (e.g., invalid API key).</li> <li><code>AIRateLimitError</code>: Rate limits exceeded for the provider API.</li> <li><code>AIServiceUnavailableError</code>: Provider's service is temporarily unavailable.</li> <li><code>AIProviderInvalidRequestError</code>: The request sent to the provider was malformed or invalid.</li> <li><code>AIProviderResponseError</code>: Error parsing or handling the provider's response.</li> </ul>"},{"location":"error_handling/#core-errors","title":"Core Errors","text":"<ul> <li><code>AISetupError</code>: Errors during the setup or initialization of core components (e.g., model selection failure).</li> <li><code>AIProcessingError</code>: General errors during AI request processing.</li> </ul>"},{"location":"error_handling/#tool-errors","title":"Tool Errors","text":"<ul> <li><code>AIToolError</code>: Base class for errors related to tool handling.</li> <li><code>AIToolNotFoundError</code>: Requested tool could not be found.</li> <li><code>AIToolExecutionError</code>: An error occurred during the execution of a tool function.</li> <li><code>AIToolTimeoutError</code>: Tool execution exceeded the configured timeout.</li> </ul>"},{"location":"error_handling/#conversation-errors","title":"Conversation Errors","text":"<ul> <li><code>AIConversationError</code>: Errors related to conversation management.</li> </ul>"},{"location":"error_handling/#usage","title":"Usage","text":"<p>When interacting with the framework, you can use <code>try...except</code> blocks to catch specific errors or the base <code>AIFrameworkError</code> for general handling.</p> <pre><code>from src.core import ToolEnabledAI\nfrom src.exceptions import AIProviderError, AIToolError, AIFrameworkError\n\ntry:\n    ai = ToolEnabledAI()\n    response = ai.process_prompt(\"Use the calculator tool to find 5+5\")\n    print(response)\nexcept AIProviderError as e:\n    print(f\"AI Provider Issue: {e}\")\nexcept AIToolError as e:\n    print(f\"Tool Execution Issue: {e}\")\nexcept AIFrameworkError as e:\n    print(f\"General Framework Error: {e}\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"examples/","title":"Usage Examples","text":"<p>This page presents practical examples of using Agentic-AI in different scenarios.</p>"},{"location":"examples/#basic-ai-interaction","title":"Basic AI Interaction","text":"<pre><code>from src.config.models import Model\nfrom src.config.config_manager import ConfigManager\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.utils.logger import LoggerFactory\n\n# Set up logger\nlogger = LoggerFactory.create()\n\n# Initialize ConfigManager\nconfig_manager = ConfigManager()\n\n# Create AI instance\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    config_manager=config_manager,\n    logger=logger\n)\n\n# Send a request\nresponse = ai.request(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"examples/#creating-a-weather-assistant","title":"Creating a Weather Assistant","text":"<pre><code>from src.config.models import Model\nfrom src.core.tool_enabled_ai import ToolEnabledAI\n\n# Create a weather assistant\nai = AI(\n    model=Model.GPT_4O,\n    system_prompt=\"You are a helpful weather assistant. Your goal is to provide weather information.\"\n)\n\n# Define weather tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather for a location (mocked for example)\"\"\"\n    # In a real application, this would call a weather API\n    weather_data = {\n        \"New York\": \"Sunny, 75\u00b0F\",\n        \"London\": \"Rainy, 62\u00b0F\",\n        \"Tokyo\": \"Partly cloudy, 80\u00b0F\",\n        \"Paris\": \"Clear skies, 70\u00b0F\"\n    }\n    return weather_data.get(location, f\"Weather data not available for {location}\")\n\n# Register weather tool\nai.register_tool(\n    tool_name=\"get_weather\",\n    tool_function=get_weather,\n    description=\"Get current weather for a specific location\",\n    parameters_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"City or location name\"\n            }\n        },\n        \"required\": [\"location\"]\n    }\n)\n\n# Use the weather assistant\nresponse = ai.request(\"What's the weather like in Tokyo today?\")\nprint(response)\n</code></pre>"},{"location":"examples/#ai-with-auto-tool-finding","title":"AI with Auto Tool Finding","text":"<p>This example demonstrates using <code>AIToolFinder</code> to automatically select relevant tools.</p> <pre><code>from src.config.models import Model\nfrom src.config.config_manager import ConfigManager\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.utils.logger import LoggerFactory\n\n# Set up\nlogger = LoggerFactory.create()\nconfig_manager = ConfigManager()\n\n# Create AI with auto tool finding\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    system_prompt=\"You are a helpful assistant. Use tools when appropriate to answer user queries.\",\n    config_manager=config_manager,\n    logger=logger,\n    auto_find_tools=True  # Enable auto tool finding\n)\n\n# Register multiple tools\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    return f\"It's sunny in {location} today!\"\n\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    try:\n        result = eval(expression)\n        return f\"The result of {expression} is {result}\"\n    except:\n        return \"Sorry, I couldn't evaluate that expression.\"\n\ndef get_ticket_price(destination: str) -&gt; str:\n    \"\"\"Get ticket price for a destination.\"\"\"\n    return f\"A ticket to {destination} costs $1000 USD\"\n\n# Register all tools\nai.register_tool(\"get_weather\", get_weather, \"Get weather for a location\")\nai.register_tool(\"calculate\", calculate, \"Perform calculations\")\nai.register_tool(\"get_ticket_price\", get_ticket_price, \"Get ticket price information\")\n\n# The AI will automatically select the appropriate tool\nresponse = ai.request(\"How much does a ticket to New York cost?\")\nprint(response)\n\n# Try another query\nresponse = ai.request(\"What's 125 * 37?\")\nprint(response)\n</code></pre>"},{"location":"examples/#using-prompt-templates","title":"Using Prompt Templates","text":"<p>This example shows how to use the prompt management system.</p> <pre><code>from src.config.models import Model\nfrom src.prompts import PromptManager\nfrom src.core.tool_enabled_ai import ToolEnabledAI\n\n# Initialize prompt manager\nprompt_manager = PromptManager(storage_dir=\"data/prompts\")\n\n# Create a template for customer support\ntemplate_id = prompt_manager.create_template(\n    name=\"Customer Support\",\n    description=\"Template for answering customer support questions\",\n    template=\"You are a customer support agent for {{company}}. Answer this customer question: {{question}}\",\n    default_values={\"company\": \"Acme Corp\"}\n)\n\n# Create AI with prompt manager\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    prompt_manager=prompt_manager\n)\n\n# Use the template\nresponse = ai.request_with_template(\n    template_id=template_id,\n    variables={\n        \"question\": \"How do I reset my password?\",\n        \"company\": \"TechGiant Inc.\"\n    }\n)\nprint(response)\n\n# Create an alternative version for A/B testing\nprompt_manager.create_version(\n    template_id=template_id,\n    template_string=\"As a {{company}} support representative, please help with: {{question}}\",\n    name=\"Alternative Wording\",\n    description=\"Different wording to test effectiveness\"\n)\n\n# The A/B testing is handled automatically when user_id is provided\nresponse = ai.request_with_template(\n    template_id=template_id,\n    variables={\n        \"question\": \"How do I cancel my subscription?\",\n        \"company\": \"TechGiant Inc.\"\n    },\n    user_id=\"user-123\"  # This determines which version they get\n)\nprint(response)\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example to get started with Agentic-AI:</p> <pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.unified_config import UnifiedConfig\n\n# Initialize configuration (if needed, often handled internally)\n# config = UnifiedConfig.get_instance()\n\n# Create AI instance\nai = ToolEnabledAI(\n    model=\"claude-3-5-haiku\" # Example: Use a specific model\n)\n\n# Make a simple request\nresponse = ai.request(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"getting-started/#adding-tool-capabilities","title":"Adding Tool Capabilities","text":"<p>Register tools to allow the AI to perform actions:</p> <pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\nimport requests\nimport datetime\n\ndef get_weather(location: str):\n    # ... (function remains same) ...\n\ndef get_current_time():\n    # ... (function remains same) ...\n\n# Create tool-enabled AI\nai = ToolEnabledAI(\n    model=\"claude-3-7-sonnet\" # Example: A model good at tool use\n)\n\n# Register tools\nai.register_tool(\n    tool_name=\"get_weather\",\n    tool_function=get_weather,\n    description=\"Get the current weather for a specific location.\"\n)\nai.register_tool(\n    tool_name=\"get_current_time\",\n    tool_function=get_current_time,\n    description=\"Get the current date and time.\"\n)\n\n# The AI will now use the tool when appropriate\nresponse = ai.request(\"What's the weather like in Tokyo today?\")\nprint(f\"Weather Response: {response}\")\n\nresponse = ai.request(\"What time is it now?\")\nprint(f\"Time Response: {response}\")\n</code></pre>"},{"location":"getting-started/#automatic-tool-finding","title":"Automatic Tool Finding","text":"<p>Enable the AI to automatically select relevant tools:</p> <pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\n\n# Dummy tool functions\ndef get_weather(location: str):\n    return f\"It's always sunny in {location}!\"\n\ndef calculator_function(expression: str):\n    try: return str(eval(expression))\n    except: return \"Invalid expression\"\n\n# Create AI with auto tool finding\nai = ToolEnabledAI(\n    model=\"claude-3-7-sonnet\",\n    auto_tool_finding=True # Enable auto finding\n)\n\n# Register tools (still required for the AI to know about them)\nai.register_tool(\"get_weather\", get_weather, \"Get weather for a location\")\nai.register_tool(\"calculate\", calculator_function, \"Perform calculations\")\n\n# The AI will automatically select the appropriate tool\nresponse = ai.request(\"What's the weather like in Paris today?\")\nprint(f\"Auto Weather: {response}\")\n\nresponse = ai.request(\"Calculate 3 + 5 * 2\")\nprint(f\"Auto Calculation: {response}\")\n</code></pre> <p>See the Tool Integration section for more details.</p>"},{"location":"metrics_system/","title":"Metrics Tracking System","text":"<p>This document provides an overview of the metrics tracking system implemented in the Agentic-AI framework. The system allows tracking and analysis of agent and tool usage, performance metrics, and request processing.</p>"},{"location":"metrics_system/#overview","title":"Overview","text":"<p>The metrics tracking system consists of the following components:</p> <ol> <li>RequestMetricsService: Core service for tracking and storing metrics about requests, agents, tools, and models.</li> <li>MetricsDashboard: Visualization and reporting tool for analyzing metrics data.</li> <li>Metrics CLI: Command-line interface for querying and visualizing metrics.</li> </ol> <p>The system tracks:</p> <ul> <li>Request processing details and duration</li> <li>Agent usage and performance</li> <li>Tool usage and performance</li> <li>Model usage and token counts</li> </ul>"},{"location":"metrics_system/#installation-and-setup","title":"Installation and Setup","text":"<p>The metrics system is built into the Agentic-AI framework and does not require separate installation. All metrics are automatically stored in:</p> <pre><code>data/metrics/request_metrics.json\n</code></pre> <p>This location can be customized by providing a different path when initializing the <code>RequestMetricsService</code>.</p>"},{"location":"metrics_system/#usage","title":"Usage","text":""},{"location":"metrics_system/#1-programmatic-usage","title":"1. Programmatic Usage","text":""},{"location":"metrics_system/#tracking-request-metrics","title":"Tracking Request Metrics","text":"<pre><code>from src.metrics.request_metrics import RequestMetricsService\n\n# Initialize the metrics service\nmetrics_service = RequestMetricsService()\n\n# Start tracking a request\nrequest_id = metrics_service.start_request_tracking(\n    prompt=\"User query here\",\n    metadata={\"user_id\": \"user123\"}\n)\n\n# Later, end tracking when request is complete\nmetrics_service.end_request_tracking(\n    request_id=request_id,\n    success=True  # or False if there was an error\n)\n</code></pre>"},{"location":"metrics_system/#tracking-agent-usage","title":"Tracking Agent Usage","text":"<pre><code># Track when an agent is used\nmetrics_service.track_agent_usage(\n    request_id=request_id,\n    agent_id=\"example_agent\",\n    confidence=0.85,\n    duration_ms=152,\n    success=True\n)\n</code></pre>"},{"location":"metrics_system/#tracking-tool-usage","title":"Tracking Tool Usage","text":"<pre><code># Track when a tool is used\nmetrics_service.track_tool_usage(\n    request_id=request_id,\n    tool_id=\"example_tool\",\n    duration_ms=75,\n    success=True\n)\n</code></pre>"},{"location":"metrics_system/#tracking-model-usage","title":"Tracking Model Usage","text":"<pre><code># Track when a model is used\nmetrics_service.track_model_usage(\n    request_id=request_id,\n    model_id=\"gpt-4\",\n    tokens_in=250,\n    tokens_out=50,\n    duration_ms=1200,\n    success=True\n)\n</code></pre>"},{"location":"metrics_system/#getting-metrics","title":"Getting Metrics","text":"<pre><code># Get agent metrics\nagent_metrics = metrics_service.get_agent_metrics(\n    agent_id=\"example_agent\",  # Optional, get all if not specified\n    start_time=datetime.now() - timedelta(days=30)\n)\n\n# Get tool metrics\ntool_metrics = metrics_service.get_tool_metrics(\n    tool_id=\"example_tool\",  # Optional, get all if not specified\n    start_time=datetime.now() - timedelta(days=30)\n)\n</code></pre>"},{"location":"metrics_system/#visualization","title":"Visualization","text":"<pre><code>from src.metrics.dashboard import MetricsDashboard\n\n# Initialize the dashboard\ndashboard = MetricsDashboard()\n\n# Generate plots\ndashboard.plot_agent_usage(top_n=10, days=30)\ndashboard.plot_tool_usage(top_n=10, days=30)\n\n# Generate a performance report\nreport = dashboard.generate_performance_report(days=30)\n</code></pre>"},{"location":"metrics_system/#2-command-line-interface","title":"2. Command-Line Interface","text":"<p>The metrics system includes a command-line interface for easy analysis of metrics data.</p>"},{"location":"metrics_system/#basic-usage","title":"Basic Usage","text":"<pre><code># Show summary of metrics\npython src/metrics/metrics_cli.py summary\n\n# Show agent metrics\npython src/metrics/metrics_cli.py agents\n\n# Show metrics for a specific agent\npython src/metrics/metrics_cli.py agents --agent=\"orchestrator\"\n\n# Show tool metrics\npython src/metrics/metrics_cli.py tools\n\n# Show details for a specific request\npython src/metrics/metrics_cli.py request REQUEST_ID\n\n# Generate a performance report\npython src/metrics/metrics_cli.py report\n</code></pre>"},{"location":"metrics_system/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Generate and save a plot of agent metrics\npython src/metrics/metrics_cli.py agents --plot --output=agent_metrics.png\n\n# Generate and save a plot of tool metrics\npython src/metrics/metrics_cli.py tools --plot --output=tool_metrics.png\n\n# Generate a JSON performance report\npython src/metrics/metrics_cli.py report --output=performance_report.json\n\n# Show metrics for the last 7 days\npython src/metrics/metrics_cli.py summary --days=7\n</code></pre>"},{"location":"metrics_system/#integration-with-orchestrator","title":"Integration with Orchestrator","text":"<p>The metrics system is fully integrated with the Orchestrator agent. When the Orchestrator processes a request, it:</p> <ol> <li>Automatically generates a request ID if not provided</li> <li>Tracks the start and end of each request</li> <li>Tracks which agents and tools are used</li> <li>Records model usage and selection</li> <li>Tracks success or failure of each component</li> <li>Includes the request ID and metrics metadata in the response</li> </ol>"},{"location":"metrics_system/#integration-with-tool-registry","title":"Integration with Tool Registry","text":"<p>The metrics system is also integrated with the Tool Registry. When a tool is executed:</p> <ol> <li>The execution time is tracked</li> <li>The success or failure is recorded</li> <li>The tool usage is associated with the current request ID</li> </ol>"},{"location":"metrics_system/#extending-the-metrics-system","title":"Extending the Metrics System","text":""},{"location":"metrics_system/#adding-new-metrics","title":"Adding New Metrics","text":"<p>To add new metrics to track:</p> <ol> <li>Add new fields to the appropriate dictionaries in <code>RequestMetricsService</code></li> <li>Add methods to update and retrieve the new metrics</li> <li>Update visualization methods in <code>MetricsDashboard</code> if needed</li> </ol>"},{"location":"metrics_system/#integrating-with-custom-agents","title":"Integrating with Custom Agents","text":"<p>To integrate the metrics system with your custom agent:</p> <pre><code>from src.metrics.request_metrics import RequestMetricsService\n\nclass MyCustomAgent:\n    def process_request(self, request):\n        # Extract request_id if it exists\n        request_id = request.get(\"request_id\")\n\n        # Initialize metrics service\n        metrics_service = RequestMetricsService()\n\n        # Record start time\n        start_time = time.time()\n\n        try:\n            # Process the request\n            result = self._do_processing(request)\n\n            # Track usage\n            metrics_service.track_agent_usage(\n                request_id=request_id,\n                agent_id=self.agent_id,\n                duration_ms=int((time.time() - start_time) * 1000),\n                success=True\n            )\n\n            return result\n        except Exception as e:\n            # Track failure\n            metrics_service.track_agent_usage(\n                request_id=request_id,\n                agent_id=self.agent_id,\n                duration_ms=int((time.time() - start_time) * 1000),\n                success=False,\n                metadata={\"error\": str(e)}\n            )\n            raise\n</code></pre>"},{"location":"metrics_system/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metrics_system/#common-issues","title":"Common Issues","text":"<ol> <li>Missing metrics data: Ensure the data directory exists and is writable.</li> <li>Request ID not tracked: Make sure the request_id is passed correctly between components.</li> <li>CLI import errors: Run the CLI script from the project root directory.</li> </ol>"},{"location":"metrics_system/#debugging","title":"Debugging","text":"<p>To debug metrics tracking, you can:</p> <ol> <li>Inspect the metrics JSON file directly: <code>data/metrics/request_metrics.json</code></li> <li>Add detailed metadata to track more information</li> <li>Use the CLI's request command to inspect specific requests: <code>python src/metrics/metrics_cli.py request REQUEST_ID</code></li> </ol>"},{"location":"metrics_system/#best-practices","title":"Best Practices","text":"<ol> <li>Always include request_id when passing requests between components</li> <li>Use consistent agent_id and tool_id values for accurate tracking</li> <li>Track both successes and failures for complete analytics</li> <li>Include relevant metadata for easier debugging and analysis</li> <li>Regularly review metrics to identify performance issues and opportunities for optimization</li> </ol>"},{"location":"tool_finder_agent/","title":"ToolFinderAgent and ToolRegistry","text":""},{"location":"tool_finder_agent/#overview","title":"Overview","text":"<p>The ToolFinderAgent is a specialized agent in the Agentic-AI framework that analyzes user requests to identify relevant tools. It works with the ToolRegistry to maintain tool definitions and usage statistics, providing an intelligent way to discover and recommend tools based on user needs.</p>"},{"location":"tool_finder_agent/#architecture","title":"Architecture","text":"<p>The ToolFinderAgent architecture consists of three main components:</p> <ol> <li>ToolRegistry: Maintains tool definitions and usage statistics</li> <li>ToolFinderAgent: Analyzes user requests to find relevant tools</li> <li>ToolManager: Coordinates tool registration, discovery, and execution</li> </ol>"},{"location":"tool_finder_agent/#process-flow","title":"Process Flow","text":"<ol> <li>User Request: User sends a request to the system</li> <li>Orchestrator Analysis: The Orchestrator receives the request</li> <li>Tool Finding: The Orchestrator calls the ToolFinderAgent to identify relevant tools</li> <li>Model Response: The Orchestrator sends the request to the AI model along with information about the relevant tools</li> <li>Action Planning: The model generates a response that may include using the identified tools</li> <li>Execution: If the model's response indicates tool usage, the tools are executed</li> <li>Response: Results are returned to the user</li> </ol>"},{"location":"tool_finder_agent/#components","title":"Components","text":""},{"location":"tool_finder_agent/#toolregistry","title":"ToolRegistry","text":"<p>The ToolRegistry maintains a registry of available tools and their usage statistics. It provides methods for:</p> <ul> <li>Registering tools</li> <li>Retrieving tool definitions</li> <li>Tracking tool usage and effectiveness</li> <li>Recommending tools based on usage statistics</li> </ul> <pre><code># Example: Creating a ToolRegistry\ntool_registry = ToolRegistry(logger=logger)\n\n# Example: Registering a tool\ntool_registry.register_tool(\"get_weather\", weather_tool)\n\n# Example: Getting tool usage statistics\nstats = tool_registry.get_usage_stats(\"get_weather\")\n</code></pre>"},{"location":"tool_finder_agent/#toolfinderagent","title":"ToolFinderAgent","text":"<p>The ToolFinderAgent is a specialized agent that analyzes user requests to identify relevant tools. It:</p> <ul> <li>Uses AI to understand the user's intent</li> <li>Recommends tools based on the request</li> <li>Updates usage statistics for recommended tools</li> </ul> <pre><code># Example: Creating a ToolFinderAgent\ntool_finder_agent = ToolFinderAgent(\n    ai_instance=ai,\n    tool_registry=tool_registry,\n    config_manager=config_manager,\n    logger=logger\n)\n\n# Example: Processing a request\nresponse = tool_finder_agent.process_request(request)\nselected_tools = response.selected_tools\n</code></pre>"},{"location":"tool_finder_agent/#toolmanager","title":"ToolManager","text":"<p>The ToolManager coordinates tool operations, including:</p> <ul> <li>Tool registration</li> <li>Tool discovery using the ToolFinderAgent</li> <li>Tool execution</li> <li>Tool information retrieval</li> </ul> <pre><code># Example: Creating a ToolManager\ntool_manager = ToolManager(\n    logger=logger,\n    config_manager=config_manager,\n    tool_registry=tool_registry,\n    tool_executor=tool_executor,\n    agent_factory=agent_factory\n)\n\n# Example: Enabling agent-based tool finding\ntool_manager.enable_agent_based_tool_finding(ai_instance=ai)\n\n# Example: Finding tools for a prompt\ntools = tool_manager.find_tools(\"What's the weather like in New York?\")\n</code></pre>"},{"location":"tool_finder_agent/#integration-with-the-orchestrator","title":"Integration with the Orchestrator","text":"<p>The Orchestrator uses the ToolFinderAgent to find relevant tools for user requests. It:</p> <ol> <li>Calls the ToolFinderAgent to identify relevant tools</li> <li>Uses the identified tools to create an action plan</li> <li>Routes the request to appropriate specialized agents</li> </ol> <pre><code># Example: Setting up the Orchestrator with the ToolFinderAgent\norchestrator = Orchestrator(\n    ai_instance=ai,\n    tool_registry=tool_registry,\n    config_manager=config_manager,\n    logger=logger\n)\norchestrator.set_tool_finder_agent(tool_finder_agent)\n</code></pre>"},{"location":"tool_finder_agent/#usage-example","title":"Usage Example","text":"<p>See the <code>examples/tool_finder_agent_example.py</code> script for a complete example of using the ToolFinderAgent with the ToolRegistry.</p>"},{"location":"tool_finder_agent/#benefits","title":"Benefits","text":"<ol> <li>Intelligent Tool Selection: The ToolFinderAgent uses AI to understand the user's intent and select appropriate tools</li> <li>Usage Statistics: The ToolRegistry tracks tool usage and effectiveness, enabling better recommendations over time</li> <li>Consistent Interface: The ToolFinderAgent follows the same agent interface as other agents</li> <li>Integration with Multi-Agent System: The ToolFinderAgent can participate in the agent ecosystem</li> </ol>"},{"location":"tool_finder_agent/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Learning from Tool Usage: Enhance the ToolFinderAgent to learn from successful tool usage</li> <li>Tool Discovery: Implement automatic discovery of new tools</li> <li>Tool Composition: Enable the creation of composite tools from existing tools</li> <li>Tool Versioning: Add support for tool versioning and compatibility checking</li> </ol>"},{"location":"tool_finder_agent/#tool-execution-example","title":"Tool Execution Example","text":"<pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.tools.tool_manager import ToolManager\nfrom src.tools.basic_tools import read_file # Example tool\n\n# Initialize ToolManager and AI\ntool_manager = ToolManager()\ntool_manager.register_tool(\"read_file\", read_file)\nai = ToolEnabledAI(tool_manager=tool_manager)\n\n# Simulate AI deciding to call a tool\nresponse_from_ai = {\n    \"content\": \"Okay, I will read the file.\",\n    \"tool_calls\": [{\n        \"id\": \"call_1\",\n        \"type\": \"tool\",\n        \"name\": \"read_file\",\n        \"args\": {\"filepath\": \"README.md\"}\n    }]\n}\n\n# Process tool calls (this logic might be inside ToolEnabledAI.request)\nif response_from_ai.get(\"tool_calls\"):\n    results = []\n    for call in response_from_ai[\"tool_calls\"]:\n        tool_name = call.get(\"name\")\n        tool_args = call.get(\"args\")\n        try:\n            # Use ToolManager to execute\n            result = tool_manager.execute_tool(tool_name, tool_args)\n            results.append({\"tool_call_id\": call[\"id\"], \"result\": result})\n        except Exception as e:\n            results.append({\"tool_call_id\": call[\"id\"], \"error\": str(e)})\n\n    # Send results back to AI to get final response\n    # final_response = ai.request_with_tool_results(original_prompt, response_from_ai, results)\n    # print(final_response)\n    print(f\"Tool results: {results}\")\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides step-by-step guides for common tasks using the Agentic-AI framework.</p> <ul> <li>Creating a Custom Agent (Coming Soon)</li> <li>Adding Complex Tools (Coming Soon)</li> <li>Integrating a New Provider (Coming Soon)</li> </ul> <p>(More tutorials to be added)</p>"},{"location":"utils/","title":"Utilities","text":"<p>This page describes common utility functions and classes provided by the framework.</p> <p>(Content to be added as needed)</p>"},{"location":"agents/agent_specifications/","title":"Agent Specifications","text":"<p>This document provides detailed specifications for each agent in the multi-agent architecture.</p>"},{"location":"agents/agent_specifications/#1-request-rooter-agent","title":"1. Request Rooter Agent","text":"<p>Purpose: Analyze user requests and coordinate responses from specialized agents.</p>"},{"location":"agents/agent_specifications/#capabilities","title":"Capabilities","text":"<ul> <li>Natural language understanding to identify request intent</li> <li>Confidence scoring for different agent capabilities</li> <li>Request routing to appropriate specialized agents</li> <li>Response aggregation from multiple agents</li> <li>Conversation management and context preservation</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput","title":"Input/Output","text":"<ul> <li>Input: User request (text, potentially with metadata)</li> <li>Output: Coordinated response from one or more agents</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details","title":"Implementation Details","text":"<ul> <li>Uses AI to analyze request intent and required capabilities</li> <li>Maintains agent registry with capability information</li> <li>Routes requests based on confidence scores</li> <li>Handles sequential and parallel execution of sub-requests</li> <li>Merges responses into coherent output</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>default_model</code>: Model to use for intent analysis</li> <li><code>confidence_threshold</code>: Minimum confidence for agent selection</li> <li><code>max_parallel_agents</code>: Maximum agents to run in parallel</li> <li><code>timeout_seconds</code>: Maximum wait time for agent responses</li> </ul>"},{"location":"agents/agent_specifications/#2-action-planner-agent","title":"2. Action Planner Agent","text":"<p>Purpose: Break complex requests into well-defined subtasks with dependencies.</p>"},{"location":"agents/agent_specifications/#capabilities_1","title":"Capabilities","text":"<ul> <li>Task decomposition and sequencing</li> <li>Resource allocation planning</li> <li>Dependency management</li> <li>Critical path identification</li> <li>Error recovery planning</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput_1","title":"Input/Output","text":"<ul> <li>Input: Complex request or task</li> <li>Output: Structured action plan with subtasks and dependencies</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details_1","title":"Implementation Details","text":"<ul> <li>Uses structured reasoning to decompose tasks</li> <li>Creates DAG (Directed Acyclic Graph) of task dependencies</li> <li>Assigns appropriate agents and tools to each subtask</li> <li>Estimates resource requirements</li> <li>Identifies parallel execution opportunities</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options_1","title":"Configuration Options","text":"<ul> <li><code>max_subtasks</code>: Maximum number of subtasks to create</li> <li><code>planning_depth</code>: Maximum depth of task decomposition</li> <li><code>min_subtask_size</code>: Minimum complexity threshold for creating a subtask</li> <li><code>planning_model</code>: Specific model to use for planning</li> </ul>"},{"location":"agents/agent_specifications/#3-listener-agent","title":"3. Listener Agent","text":"<p>Purpose: Process audio input and convert to structured text with metadata.</p>"},{"location":"agents/agent_specifications/#capabilities_2","title":"Capabilities","text":"<ul> <li>Speech recognition across multiple languages</li> <li>Speaker identification</li> <li>Noise filtering</li> <li>Emotion detection</li> <li>Audio metadata extraction</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput_2","title":"Input/Output","text":"<ul> <li>Input: Audio file or stream</li> <li>Output: Transcribed text with metadata (speakers, emotions, etc.)</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details_2","title":"Implementation Details","text":"<ul> <li>Integrates with speech recognition APIs (Whisper, Google Speech, etc.)</li> <li>Implements audio preprocessing for noise reduction</li> <li>Performs speaker diarization when multiple speakers are present</li> <li>Extracts emotional tone and emphasis</li> <li>Handles multiple audio formats</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options_2","title":"Configuration Options","text":"<ul> <li><code>default_speech_model</code>: Default speech recognition model</li> <li><code>language_detection</code>: Enable/disable automatic language detection</li> <li><code>speaker_diarization</code>: Enable/disable speaker identification</li> <li><code>emotion_detection</code>: Enable/disable emotion recognition</li> <li><code>supported_formats</code>: List of supported audio formats</li> </ul>"},{"location":"agents/agent_specifications/#4-translator-agent","title":"4. Translator Agent","text":"<p>Purpose: Translate content between languages while preserving context and intent.</p>"},{"location":"agents/agent_specifications/#capabilities_3","title":"Capabilities","text":"<ul> <li>High-quality translation across languages</li> <li>Technical terminology preservation</li> <li>Context-aware translation</li> <li>Cultural adaptation</li> <li>Specialized domain knowledge (legal, medical, technical)</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput_3","title":"Input/Output","text":"<ul> <li>Input: Text content with source language</li> <li>Output: Translated content in target language</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details_3","title":"Implementation Details","text":"<ul> <li>Integrates with multiple translation engines for optimal results</li> <li>Maintains glossaries for domain-specific terminology</li> <li>Preserves formatting and structure during translation</li> <li>Provides confidence scores for translations</li> <li>Handles idiomatic expressions appropriately</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options_3","title":"Configuration Options","text":"<ul> <li><code>default_translation_engine</code>: Primary translation service to use</li> <li><code>terminology_glossaries</code>: Domain-specific terminology mappings</li> <li><code>quality_threshold</code>: Minimum quality score for acceptance</li> <li><code>supported_languages</code>: List of supported language pairs</li> <li><code>alternative_engines</code>: Backup translation services</li> </ul>"},{"location":"agents/agent_specifications/#5-website-parser-agent","title":"5. Website Parser Agent","text":"<p>Purpose: Intelligently search and extract information from websites.</p>"},{"location":"agents/agent_specifications/#capabilities_4","title":"Capabilities","text":"<ul> <li>Content extraction from web pages</li> <li>Link following based on relevance</li> <li>Information summarization</li> <li>Content filtering by relevance</li> <li>Structured data extraction</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput_4","title":"Input/Output","text":"<ul> <li>Input: Search query and/or starting URLs</li> <li>Output: Extracted and summarized information with sources</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details_4","title":"Implementation Details","text":"<ul> <li>Uses intelligent crawling with relevance scoring</li> <li>Extracts text content using DOM analysis</li> <li>Identifies and follows relevant links</li> <li>Summarizes content based on query relevance</li> <li>Extracts structured data when available</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options_4","title":"Configuration Options","text":"<ul> <li><code>max_pages</code>: Maximum pages to crawl per request</li> <li><code>max_depth</code>: Maximum link following depth</li> <li><code>relevance_threshold</code>: Minimum relevance score for inclusion</li> <li><code>timeout_seconds</code>: Maximum time for crawling</li> <li><code>respect_robots_txt</code>: Whether to honor robots.txt directives</li> </ul>"},{"location":"agents/agent_specifications/#6-mcp-searcher-agent","title":"6. MCP Searcher Agent","text":"<p>Purpose: Identify and utilize relevant MCPs (Model-Centric Processes) for tasks.</p>"},{"location":"agents/agent_specifications/#capabilities_5","title":"Capabilities","text":"<ul> <li>MCP discovery and evaluation</li> <li>Capability matching to requirements</li> <li>API integration</li> <li>Authentication handling</li> <li>Performance monitoring</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput_5","title":"Input/Output","text":"<ul> <li>Input: Task requirements and constraints</li> <li>Output: Selected MCPs with integration details</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details_5","title":"Implementation Details","text":"<ul> <li>Maintains registry of available MCPs with capabilities</li> <li>Matches task requirements to MCP capabilities</li> <li>Handles authentication and API key management</li> <li>Monitors performance and availability</li> <li>Implements fallback mechanisms</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options_5","title":"Configuration Options","text":"<ul> <li><code>mcp_registry_url</code>: URL for MCP discovery service</li> <li><code>credential_store</code>: Secure storage for API credentials</li> <li><code>matching_algorithm</code>: Algorithm for MCP selection</li> <li><code>performance_threshold</code>: Minimum performance requirements</li> <li><code>cooldown_periods</code>: Retry intervals for failed MCPs</li> </ul>"},{"location":"agents/agent_specifications/#7-content-generator-agent","title":"7. Content Generator Agent","text":"<p>Purpose: Create multimedia content based on specifications.</p>"},{"location":"agents/agent_specifications/#capabilities_6","title":"Capabilities","text":"<ul> <li>Image generation and editing</li> <li>Audio synthesis (music, voice)</li> <li>Video creation</li> <li>Diagram and chart generation</li> <li>Document formatting</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput_6","title":"Input/Output","text":"<ul> <li>Input: Content specifications and requirements</li> <li>Output: Generated content in requested formats</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details_6","title":"Implementation Details","text":"<ul> <li>Integrates with multiple generation services (DALL-E, Midjourney, etc.)</li> <li>Handles format conversion and optimization</li> <li>Implements quality control checks</li> <li>Manages content iteration based on feedback</li> <li>Supports multiple content types and formats</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options_6","title":"Configuration Options","text":"<ul> <li><code>default_providers</code>: Map of content types to default services</li> <li><code>quality_settings</code>: Quality parameters for different content types</li> <li><code>size_limits</code>: Maximum sizes for generated content</li> <li><code>iteration_limit</code>: Maximum iterations for refinement</li> <li><code>supported_formats</code>: Output formats supported for each content type</li> </ul>"},{"location":"agents/agent_specifications/#8-paralleliser-agent","title":"8. Paralleliser Agent","text":"<p>Purpose: Speed up complex tasks through parallel execution.</p>"},{"location":"agents/agent_specifications/#capabilities_7","title":"Capabilities","text":"<ul> <li>Task dependency analysis</li> <li>Parallel execution planning</li> <li>Resource allocation</li> <li>Result synchronization</li> <li>Performance optimization</li> </ul>"},{"location":"agents/agent_specifications/#inputoutput_7","title":"Input/Output","text":"<ul> <li>Input: Complex task with multiple components</li> <li>Output: Aggregated results from parallel execution</li> </ul>"},{"location":"agents/agent_specifications/#implementation-details_7","title":"Implementation Details","text":"<ul> <li>Identifies independent subtasks that can run in parallel</li> <li>Creates execution plan with optimal resource allocation</li> <li>Manages async execution and result collection</li> <li>Implements timeouts and error handling</li> <li>Optimizes resource utilization</li> </ul>"},{"location":"agents/agent_specifications/#configuration-options_7","title":"Configuration Options","text":"<ul> <li><code>max_concurrency</code>: Maximum concurrent tasks</li> <li><code>resource_limits</code>: Limits for different resource types</li> <li><code>timeout_strategy</code>: How to handle subtask timeouts</li> <li><code>error_strategy</code>: How to handle subtask failures</li> <li><code>priority_levels</code>: Task priority classification</li> </ul>"},{"location":"agents/architecture/","title":"Multi-Agent Architecture","text":""},{"location":"agents/architecture/#overview","title":"Overview","text":"<p>The multi-agent architecture extends the core Agentic-AI framework to support specialized agents that work together to handle complex user requests. Each agent is designed for a specific purpose, communicating through a centralized coordination system.</p>"},{"location":"agents/architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TD\n    User[User] --&gt;|Request| RequestRooter[Request Rooter Agent]\n\n    subgraph \"Core Agents\"\n        RequestRooter --&gt;|Route request| ActionPlanner[Action Planner]\n        RequestRooter --&gt;|Parse audio| Listener[Listener Agent]\n        RequestRooter --&gt;|Translate| Translator[Translator Agent]\n        RequestRooter --&gt;|Parallel execution| Paralleliser[Paralleliser Agent]\n    end\n\n    subgraph \"Task Agents\"\n        ActionPlanner --&gt;|Plan tasks| WebsiteParser[Website Parser]\n        ActionPlanner --&gt;|Plan tasks| ContentGenerator[Content Generator]\n        ActionPlanner --&gt;|Plan tasks| MCPSearcher[MCP Searcher]\n        ActionPlanner --&gt;|Plan tasks| CustomAgent[Custom Agent]\n    end\n\n    WebsiteParser --&gt;|Results| RequestRooter\n    ContentGenerator --&gt;|Results| RequestRooter\n    MCPSearcher --&gt;|Results| RequestRooter\n    CustomAgent --&gt;|Results| RequestRooter\n\n    RequestRooter --&gt;|Response| User\n</code></pre>"},{"location":"agents/architecture/#agent-descriptions","title":"Agent Descriptions","text":""},{"location":"agents/architecture/#core-agents","title":"Core Agents","text":"<ol> <li> <p>Orchestrator</p> </li> <li> <p>Acts as the entry point for all user requests</p> </li> <li>Analyzes requests and routes them to appropriate specialized agents</li> <li>Coordinates responses from multiple agents</li> <li>Manages the overall conversation flow</li> <li>Analyzes complex requests and breaks them down into subtasks</li> <li>Determines which agents and tools are needed for each subtask</li> <li>Creates execution plans with dependencies between tasks</li> <li>Handles task prioritization and sequencing</li> <li>Identifies tasks that can be executed in parallel</li> <li>Distributes subtasks to appropriate agents</li> <li>Manages concurrency and resource allocation</li> <li> <p>Aggregates and synchronizes results from parallel operations-</p> </li> <li> <p>Listener</p> </li> <li> <p>Processes audio input and converts it to text</p> </li> <li>Handles speech recognition and language identification</li> <li>Extracts audio metadata (speaker identification, emotion detection)</li> <li> <p>Interfaces with audio processing tools and APIs</p> </li> <li> <p>Translator</p> </li> <li> <p>Translates content between languages</p> </li> <li>Maintains context and intent across translations</li> <li>Adapts content for cultural differences</li> <li> <p>Handles specialized technical or domain-specific terminology</p> </li> <li> <p>Tool finder</p> </li> <li> <p>Identifies relevant MCPs (Model-Centric Processes) for tasks</p> </li> <li>Interfaces with MCP registries and APIs</li> <li>Evaluates MCP capabilities against task requirements</li> <li>Handles MCP authentication and integration</li> </ol>"},{"location":"agents/architecture/#task-agents","title":"Task Agents","text":"<ol> <li> <p>Website Parser</p> </li> <li> <p>Searches and extracts information from websites</p> </li> <li>Follows links intelligently to find relevant content</li> <li>Analyzes text, images, and structured data</li> <li> <p>Prioritizes content based on relevance to the user's request</p> </li> <li> <p>Content Generator</p> </li> <li>Creates multimedia content (images, videos, audio, diagrams)</li> <li>Interfaces with generation models and APIs</li> <li>Tailors content to user specifications</li> <li>Optimizes content for various formats and platforms</li> </ol>"},{"location":"agents/architecture/#implementation-approach","title":"Implementation Approach","text":""},{"location":"agents/architecture/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":"<ol> <li> <p>Agent Interface &amp; Base Classes</p> </li> <li> <p>Create <code>AgentInterface</code> protocol defining common agent methods</p> </li> <li>Implement <code>BaseAgent</code> class with shared functionality</li> <li> <p>Develop communication protocols between agents</p> </li> <li> <p>Agent Registry &amp; Factory</p> </li> <li> <p>Build registry for dynamically registering/discovering agents</p> </li> <li>Create factory for instantiating appropriate agents</li> <li> <p>Implement dependency injection for agent configuration</p> </li> <li> <p>Request Rooter Implementation</p> </li> <li>Develop the central coordination logic</li> <li>Implement request analysis capabilities</li> <li>Create routing mechanisms to specialized agents</li> </ol>"},{"location":"agents/architecture/#phase-2-core-agents","title":"Phase 2: Core Agents","text":"<ol> <li> <p>Action Planner</p> </li> <li> <p>Implement task decomposition algorithms</p> </li> <li>Create task dependency graph management</li> <li> <p>Build execution planning capabilities</p> </li> <li> <p>Paralleliser</p> </li> <li>Implement concurrent task execution framework</li> <li>Develop resource management and throttling</li> <li>Create result synchronization mechanisms</li> </ol>"},{"location":"agents/architecture/#phase-3-specialized-agents","title":"Phase 3: Specialized Agents","text":"<ol> <li> <p>Listener &amp; Translator</p> </li> <li> <p>Integrate with speech recognition services</p> </li> <li>Implement translation capabilities</li> <li> <p>Build domain-specific adaptation features</p> </li> <li> <p>Website Parser</p> </li> <li> <p>Implement intelligent web crawling</p> </li> <li>Develop relevance scoring algorithms</li> <li> <p>Create content extraction and summarization</p> </li> <li> <p>Content Generator</p> </li> <li>Integrate with generation models and APIs</li> <li>Implement format conversion and optimization</li> <li>Build quality assurance capabilities</li> </ol>"},{"location":"agents/architecture/#phase-4-integration-optimization","title":"Phase 4: Integration &amp; Optimization","text":"<ol> <li> <p>End-to-End Testing</p> </li> <li> <p>Create comprehensive test scenarios</p> </li> <li>Build automated testing framework</li> <li> <p>Implement performance benchmarking</p> </li> <li> <p>Monitoring &amp; Logging</p> </li> <li> <p>Develop agent activity logging</p> </li> <li>Implement performance metrics collection</li> <li> <p>Create visualization and analysis tools</p> </li> <li> <p>Optimization</p> </li> <li>Profile and optimize communication overhead</li> <li>Implement caching and memoization</li> <li>Refine agent selection and routing logic</li> </ol>"},{"location":"agents/architecture/#technical-considerations","title":"Technical Considerations","text":""},{"location":"agents/architecture/#agent-state-management","title":"Agent State Management","text":"<p>Agents can operate in different modes:</p> <ul> <li>Stateless: Each invocation is independent</li> <li>Conversational: Maintains context across interactions</li> <li>Persistent: Retains state across sessions</li> </ul> <p>Implementation considerations:</p> <ul> <li>Use dependency injection for state management</li> <li>Define clear state serialization/deserialization</li> <li>Implement state expiration policies</li> </ul>"},{"location":"agents/architecture/#communication-patterns","title":"Communication Patterns","text":"<p>Support multiple communication patterns:</p> <ul> <li>Request-Response: Simple synchronous interaction</li> <li>Publish-Subscribe: Event-driven asynchronous communication</li> <li>Stream Processing: Continuous data flow processing</li> </ul> <p>Implementation considerations:</p> <ul> <li>Use message queues for asynchronous communication</li> <li>Implement backpressure mechanisms</li> <li>Support both in-memory and distributed communication</li> </ul>"},{"location":"agents/architecture/#security-isolation","title":"Security &amp; Isolation","text":"<p>Ensure proper security boundaries:</p> <ul> <li>Implement agent permission system</li> <li>Verify cross-agent communication authorization</li> <li>Sandbox external tool execution</li> </ul> <p>Implementation considerations:</p> <ul> <li>Define fine-grained permission model</li> <li>Implement secure token-based authentication</li> <li>Create audit logging for all agent actions</li> </ul>"},{"location":"agents/architecture/#integration-with-existing-framework","title":"Integration with Existing Framework","text":"<p>The multi-agent architecture integrates with the existing Agentic-AI framework:</p> <ol> <li> <p>AIBase Extensions</p> </li> <li> <p>Extend AIBase to support agent-specific functionality</p> </li> <li>Maintain backward compatibility with existing code</li> <li> <p>Add agent communication capabilities</p> </li> <li> <p>Tool Integration</p> </li> <li> <p>Ensure all agents can access the tool registry</p> </li> <li>Extend tool management for agent-specific tools</li> <li> <p>Implement agent-to-tool authorization</p> </li> <li> <p>Provider Compatibility</p> </li> <li>Support different provider capabilities across agents</li> <li>Allow agent-specific provider configuration</li> <li>Implement fallback mechanisms for provider limitations</li> </ol>"},{"location":"agents/architecture/#next-steps","title":"Next Steps","text":"<ol> <li>Create detailed specifications for each agent</li> <li>Implement the agent interfaces and base classes</li> <li>Develop the Request Rooter as initial integration point</li> <li>Build agent registry and discovery mechanisms</li> <li>Implement the Action Planner for task decomposition</li> </ol>"},{"location":"agents/architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Agent Implementation Standards - Coding standards and best practices for implementing agents</li> <li>Agent Specifications - Detailed specifications for each agent type</li> <li>Implementation Plan - Phased plan for implementing the multi-agent architecture</li> </ul>"},{"location":"agents/implementation_plan/","title":"Multi-Agent Implementation Plan","text":"<p>This document outlines the detailed implementation plan for the multi-agent architecture in the Agentic-AI framework.</p>"},{"location":"agents/implementation_plan/#implementation-phases","title":"Implementation Phases","text":""},{"location":"agents/implementation_plan/#phase-1-foundation","title":"Phase 1: Foundation","text":""},{"location":"agents/implementation_plan/#11-agent-interface-design","title":"1.1 Agent Interface Design","text":"<pre><code># src/agents/interfaces.py\nfrom typing import Protocol, Dict, List, Any, Optional, Union, AsyncIterator\nfrom typing_extensions import runtime_checkable\nfrom ..core.interfaces import AIInterface\n\n@runtime_checkable\nclass AgentInterface(Protocol):\n    \"\"\"Interface for agent implementation.\"\"\"\n\n    def process_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a request and return a response.\n\n        Args:\n            request: The request object containing prompt and metadata\n\n        Returns:\n            Response object with content and metadata\n        \"\"\"\n        ...\n\n    def can_handle(self, request: Dict[str, Any]) -&gt; float:\n        \"\"\"\n        Determine if this agent can handle the request.\n\n        Args:\n            request: The request object\n\n        Returns:\n            Confidence score (0.0-1.0) indicating ability to handle\n        \"\"\"\n        ...\n</code></pre>"},{"location":"agents/implementation_plan/#12-base-agent-implementation","title":"1.2 Base Agent Implementation","text":"<pre><code># src/agents/base_agent.py\nfrom typing import Dict, Any, Optional, List\nfrom ..core.base_ai import AIBase\nfrom ..core.tool_enabled_ai import AI\nfrom ..tools.tool_manager import ToolManager\nfrom ..utils.logger import LoggerInterface\nfrom .interfaces import AgentInterface\n\nclass BaseAgent(AgentInterface):\n    \"\"\"Base implementation for all agents.\"\"\"\n\n    def __init__(self,\n                 ai_instance: Optional[AIBase] = None,\n                 tool_manager: Optional[ToolManager] = None,\n                 logger: Optional[LoggerInterface] = None):\n        \"\"\"Initialize the base agent.\"\"\"\n        self.ai = ai_instance or AI()\n        self.tool_manager = tool_manager\n        self.logger = logger\n\n    def process_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Default implementation of request processing.\"\"\"\n        response = self.ai.request(request.get(\"prompt\", \"\"))\n        return {\"content\": response}\n\n    def can_handle(self, request: Dict[str, Any]) -&gt; float:\n        \"\"\"Default implementation returns low confidence.\"\"\"\n        return 0.1\n</code></pre>"},{"location":"agents/implementation_plan/#13-agent-registry","title":"1.3 Agent Registry","text":"<pre><code># src/agents/agent_registry.py\nfrom typing import Dict, Type, List, Optional\nfrom .interfaces import AgentInterface\nfrom .base_agent import BaseAgent\nfrom ..utils.logger import LoggerInterface, LoggerFactory\n\nclass AgentRegistry:\n    \"\"\"Registry for all available agents.\"\"\"\n\n    def __init__(self, logger: Optional[LoggerInterface] = None):\n        \"\"\"Initialize the agent registry.\"\"\"\n        self.agents: Dict[str, Type[AgentInterface]] = {}\n        self.logger = logger or LoggerFactory.create(name=\"agent_registry\")\n\n    def register(self, name: str, agent_class: Type[AgentInterface]) -&gt; None:\n        \"\"\"Register an agent class.\"\"\"\n        self.agents[name] = agent_class\n        self.logger.info(f\"Registered agent: {name}\")\n\n    def get_agent_class(self, name: str) -&gt; Optional[Type[AgentInterface]]:\n        \"\"\"Get an agent class by name.\"\"\"\n        return self.agents.get(name)\n\n    def get_all_agents(self) -&gt; List[str]:\n        \"\"\"Get names of all registered agents.\"\"\"\n        return list(self.agents.keys())\n</code></pre>"},{"location":"agents/implementation_plan/#14-agent-factory","title":"1.4 Agent Factory","text":"<pre><code># src/agents/agent_factory.py\nfrom typing import Dict, Any, Optional, Type\nfrom .interfaces import AgentInterface\nfrom .agent_registry import AgentRegistry\nfrom .base_agent import BaseAgent\nfrom ..utils.logger import LoggerInterface, LoggerFactory\nfrom ..config.config_manager import ConfigManager\n\nclass AgentFactory:\n    \"\"\"Factory for creating agent instances.\"\"\"\n\n    def __init__(self,\n                 registry: Optional[AgentRegistry] = None,\n                 config_manager: Optional[ConfigManager] = None,\n                 logger: Optional[LoggerInterface] = None):\n        \"\"\"Initialize the agent factory.\"\"\"\n        self.registry = registry or AgentRegistry()\n        self.config_manager = config_manager or ConfigManager()\n        self.logger = logger or LoggerFactory.create(name=\"agent_factory\")\n\n    def create(self,\n              agent_type: str,\n              **kwargs) -&gt; AgentInterface:\n        \"\"\"\n        Create an agent instance.\n\n        Args:\n            agent_type: Type of agent to create\n            kwargs: Additional arguments for agent initialization\n\n        Returns:\n            Initialized agent instance\n        \"\"\"\n        agent_class = self.registry.get_agent_class(agent_type)\n        if not agent_class:\n            self.logger.warning(f\"Agent type not found: {agent_type}, using BaseAgent\")\n            return BaseAgent(**kwargs)\n\n        # Get agent-specific configuration if available\n        agent_config = self.config_manager.get_agent_config(agent_type)\n        if agent_config:\n            # Merge configuration with provided kwargs (kwargs take precedence)\n            agent_kwargs = {**agent_config, **kwargs}\n        else:\n            agent_kwargs = kwargs\n\n        try:\n            return agent_class(**agent_kwargs)\n        except Exception as e:\n            self.logger.error(f\"Failed to create agent {agent_type}: {str(e)}\")\n            return BaseAgent(**kwargs)\n</code></pre>"},{"location":"agents/implementation_plan/#phase-2-core-agents-implementation-3-4-weeks","title":"Phase 2: Core Agents Implementation (3-4 weeks)","text":""},{"location":"agents/implementation_plan/#21-request-rooter-agent","title":"2.1 Request Rooter Agent","text":"<pre><code># src/agents/request_rooter.py\nfrom typing import Dict, Any, List, Optional\nfrom .base_agent import BaseAgent\nfrom .agent_factory import AgentFactory\nfrom ..utils.logger import LoggerInterface\n\nclass RequestRooterAgent(BaseAgent):\n    \"\"\"\n    Agent responsible for routing requests to appropriate specialized agents.\n    \"\"\"\n\n    def __init__(self,\n                 agent_factory: Optional[AgentFactory] = None,\n                 **kwargs):\n        \"\"\"Initialize the request rooter agent.\"\"\"\n        super().__init__(**kwargs)\n        self.agent_factory = agent_factory or AgentFactory()\n\n    def process_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a request by routing it to appropriate agents.\n\n        Args:\n            request: The request object\n\n        Returns:\n            Aggregated response from all involved agents\n        \"\"\"\n        # Analyze the request to determine appropriate agents\n        agent_assignments = self._analyze_request(request)\n\n        # If no specialized agents needed, handle directly\n        if not agent_assignments:\n            return super().process_request(request)\n\n        # Process with each identified agent\n        responses = []\n        for agent_type, confidence in agent_assignments:\n            agent = self.agent_factory.create(agent_type)\n            response = agent.process_request(request)\n            responses.append({\n                \"agent\": agent_type,\n                \"confidence\": confidence,\n                \"response\": response\n            })\n\n        # Aggregate responses (simple concatenation for now)\n        return self._aggregate_responses(responses)\n\n    def _analyze_request(self, request: Dict[str, Any]) -&gt; List[tuple]:\n        \"\"\"\n        Analyze the request to determine appropriate agents.\n\n        Args:\n            request: The request object\n\n        Returns:\n            List of (agent_type, confidence) tuples\n        \"\"\"\n        # Use AI to analyze the request\n        prompt = f\"\"\"Analyze this user request and determine which specialized agents should handle it:\n        Request: {request.get('prompt', '')}\n\n        Available agents:\n        - listener: Handles audio processing and speech recognition\n        - translator: Translates between languages\n        - website_parser: Searches websites for information\n        - content_generator: Creates multimedia content\n        - action_planner: Breaks complex tasks into subtasks\n        - mcp_searcher: Finds relevant Model-Centric Processes\n        - paralleliser: Executes tasks in parallel\n\n        Return a JSON list of [agent_name, confidence] pairs, where confidence is 0.0-1.0.\n        Only include agents with confidence &gt; 0.5. If no agents are appropriate, return [].\n        \"\"\"\n\n        try:\n            response = self.ai.request(prompt)\n            # Parse response as JSON list of [agent_name, confidence] pairs\n            import json\n            agents = json.loads(response)\n            return [(agent[0], agent[1]) for agent in agents if len(agent) == 2]\n        except Exception as e:\n            self.logger.error(f\"Failed to analyze request: {str(e)}\")\n            return []\n\n    def _aggregate_responses(self, responses: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Aggregate responses from multiple agents.\n\n        Args:\n            responses: List of agent responses\n\n        Returns:\n            Aggregated response\n        \"\"\"\n        if not responses:\n            return {\"content\": \"No agents were able to process your request.\"}\n\n        # Sort by confidence\n        sorted_responses = sorted(responses, key=lambda r: r.get(\"confidence\", 0), reverse=True)\n\n        # For now, just return the highest confidence response\n        # TODO: Implement more sophisticated aggregation\n        best_response = sorted_responses[0][\"response\"]\n\n        return best_response\n</code></pre>"},{"location":"agents/implementation_plan/#22-action-planner-agent","title":"2.2 Action Planner Agent","text":"<pre><code># src/agents/action_planner.py\nfrom typing import Dict, Any, List, Optional\nfrom .base_agent import BaseAgent\n\nclass ActionPlannerAgent(BaseAgent):\n    \"\"\"\n    Agent responsible for planning complex actions by breaking them down into subtasks.\n    \"\"\"\n\n    def process_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a request by creating an action plan.\n\n        Args:\n            request: The request object\n\n        Returns:\n            Response with action plan\n        \"\"\"\n        prompt = f\"\"\"\n        Create a detailed action plan for the following request:\n        {request.get('prompt', '')}\n\n        Break it down into subtasks with the following information for each:\n        1. Task description\n        2. Tools or agents needed\n        3. Dependencies on other tasks\n        4. Expected output\n\n        Return the plan as a structured JSON object.\n        \"\"\"\n\n        response = self.ai.request(prompt)\n\n        try:\n            import json\n            action_plan = json.loads(response)\n            return {\n                \"content\": \"I've created an action plan for your request.\",\n                \"action_plan\": action_plan\n            }\n        except json.JSONDecodeError:\n            return {\n                \"content\": \"I've analyzed your request and created a plan:\\n\\n\" + response\n            }\n\n    def can_handle(self, request: Dict[str, Any]) -&gt; float:\n        \"\"\"Determine if this agent can handle the request.\"\"\"\n        # Check if request is complex and needs planning\n        prompt = f\"\"\"\n        On a scale of 0.0 to 1.0, how complex is this request and would it benefit from being broken down into subtasks?\n        Request: {request.get('prompt', '')}\n        Return only a number between 0.0 and 1.0.\n        \"\"\"\n\n        try:\n            response = self.ai.request(prompt)\n            confidence = float(response.strip())\n            return min(max(confidence, 0.0), 1.0)  # Clamp to 0.0-1.0\n        except:\n            return 0.3  # Default moderate confidence\n</code></pre>"},{"location":"agents/implementation_plan/#phase-3-specialized-agents-4-6-weeks","title":"Phase 3: Specialized Agents (4-6 weeks)","text":"<p>For each specialized agent, we'll need to:</p> <ol> <li>Create the agent class</li> <li>Implement specific functionality</li> <li>Develop integration with external services</li> <li>Write tests for the agent</li> </ol>"},{"location":"agents/implementation_plan/#phase-4-integration-testing-2-3-weeks","title":"Phase 4: Integration &amp; Testing (2-3 weeks)","text":"<p>Implement full integration testing across agents, including:</p> <ol> <li>E2E request flow</li> <li>Error handling and recovery</li> <li>Performance benchmarking</li> <li>Load testing</li> </ol>"},{"location":"agents/implementation_plan/#configuration-updates","title":"Configuration Updates","text":"<p>Add agent configuration to <code>config.yml</code>:</p> <pre><code>agents:\n  request_rooter:\n    default_model: \"gpt-4\"\n    system_prompt: \"You are an expert at analyzing user requests and routing them to specialized agents.\"\n\n  action_planner:\n    default_model: \"gpt-4\"\n    system_prompt: \"You are an expert at breaking complex tasks into well-defined subtasks.\"\n\n  website_parser:\n    default_model: \"claude-3-opus\"\n    system_prompt: \"You are an expert at finding and extracting relevant information from websites.\"\n    max_pages_per_request: 5\n    relevance_threshold: 0.7\n\n  content_generator:\n    default_model: \"claude-3-sonnet\"\n    system_prompt: \"You are an expert at creating high-quality content based on user specifications.\"\n    supported_formats: [\"image\", \"audio\", \"video\", \"diagram\"]\n    default_service_providers:\n      image: \"dalle3\"\n      audio: \"elevenlabs\"\n      video: \"runway\"\n      diagram: \"mermaid\"\n</code></pre>"},{"location":"agents/implementation_plan/#testing-strategy","title":"Testing Strategy","text":""},{"location":"agents/implementation_plan/#unit-tests","title":"Unit Tests","text":"<p>Create unit tests for each agent:</p> <pre><code># tests/agents/test_request_rooter.py\nimport unittest\nfrom unittest.mock import MagicMock, patch\nfrom src.agents.request_rooter import RequestRooterAgent\n\nclass TestRequestRooterAgent(unittest.TestCase):\n\n    def setUp(self):\n        # Create mock dependencies\n        self.mock_ai = MagicMock()\n        self.mock_agent_factory = MagicMock()\n\n        # Create agent with mocks\n        self.agent = RequestRooterAgent(\n            ai_instance=self.mock_ai,\n            agent_factory=self.mock_agent_factory\n        )\n\n    def test_analyze_request_speech(self):\n        # Test request analysis for speech-related request\n        self.mock_ai.request.return_value = '[[\"listener\", 0.9]]'\n\n        request = {\"prompt\": \"Transcribe this audio file\"}\n        result = self.agent._analyze_request(request)\n\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0][0], \"listener\")\n        self.assertAlmostEqual(result[0][1], 0.9)\n\n    def test_process_request_routing(self):\n        # Test routing functionality\n        self.mock_ai.request.return_value = '[[\"listener\", 0.9]]'\n\n        mock_listener = MagicMock()\n        mock_listener.process_request.return_value = {\"content\": \"Transcribed content\"}\n        self.mock_agent_factory.create.return_value = mock_listener\n\n        request = {\"prompt\": \"Transcribe this audio file\"}\n        result = self.agent.process_request(request)\n\n        self.mock_agent_factory.create.assert_called_once_with(\"listener\")\n        mock_listener.process_request.assert_called_once()\n        self.assertEqual(result[\"content\"], \"Transcribed content\")\n</code></pre>"},{"location":"agents/implementation_plan/#integration-tests","title":"Integration Tests","text":"<p>Create integration tests for multi-agent scenarios:</p> <pre><code># tests/agents/test_integration.py\nimport unittest\nfrom src.agents.request_rooter import RequestRooterAgent\nfrom src.agents.action_planner import ActionPlannerAgent\nfrom src.agents.agent_factory import AgentFactory\nfrom src.agents.agent_registry import AgentRegistry\n\nclass TestAgentIntegration(unittest.TestCase):\n\n    def setUp(self):\n        # Set up registry with actual agent implementations\n        self.registry = AgentRegistry()\n        self.registry.register(\"request_rooter\", RequestRooterAgent)\n        self.registry.register(\"action_planner\", ActionPlannerAgent)\n\n        # Create factory with registry\n        self.factory = AgentFactory(registry=self.registry)\n\n    def test_complex_request_flow(self):\n        # Test flow from rooter to planner\n        rooter = self.factory.create(\"request_rooter\", agent_factory=self.factory)\n\n        request = {\n            \"prompt\": \"Create a report on renewable energy, with charts and data from the top 3 research sites\"\n        }\n\n        response = rooter.process_request(request)\n\n        # Verify response has action plan\n        self.assertIn(\"action_plan\", response)\n        # Verify plan has appropriate steps\n        self.assertTrue(any(\"chart\" in str(step).lower() for step in response[\"action_plan\"]))\n</code></pre>"},{"location":"agents/implementation_plan/#deployment-plan","title":"Deployment Plan","text":"<ol> <li>Deploy first to staging environment</li> <li>Implement metrics collection for each agent</li> <li>Start with low traffic allocation (5-10%)</li> <li>Monitor performance and error rates</li> <li>Gradually increase traffic allocation</li> </ol>"},{"location":"agents/implementation_standards/","title":"Agent Implementation Standards","text":"<p>This document outlines the standard practices for implementing agents in the Agentic-AI framework. Following these guidelines ensures consistency, maintainability, and proper integration with the framework's architecture.</p>"},{"location":"agents/implementation_standards/#agent-inheritance-and-structure","title":"Agent Inheritance and Structure","text":"<p>All agents should:</p> <ol> <li>Inherit from <code>BaseAgent</code> class</li> <li>Implement the required methods from the <code>AgentInterface</code></li> <li>Follow consistent initialization patterns</li> </ol> <pre><code>from .base_agent import BaseAgent\n\nclass YourAgent(BaseAgent):\n    \"\"\"\n    Agent for performing specific functionality.\n\n    Each agent should have a clear docstring describing its purpose and capabilities.\n    \"\"\"\n\n    def __init__(self, agent_id=\"your_agent\", **kwargs):\n        \"\"\"\n        Initialize the agent.\n\n        Args:\n            agent_id: ID of the agent (optional, defaults to class name)\n            **kwargs: Additional arguments for BaseAgent\n        \"\"\"\n        super().__init__(agent_id=agent_id, **kwargs)\n\n        # Initialize agent-specific attributes\n        # ...\n</code></pre>"},{"location":"agents/implementation_standards/#configuration-management","title":"Configuration Management","text":""},{"location":"agents/implementation_standards/#use-the-configfactoryconfigmanager","title":"Use the ConfigFactory/ConfigManager","text":"<p>Always use the configuration system rather than hardcoding values or using environment variables directly:</p> <pre><code># CORRECT:\ndef __init__(self, **kwargs):\n    super().__init__(agent_id=\"your_agent\", **kwargs)\n\n    # Get agent-specific configuration\n    self.some_option = self.agent_config.get(\"some_option\", \"default_value\")\n\n    # Use ConfigFactory for framework-wide configuration\n    config_factory = kwargs.get(\"config_factory\", ConfigFactory.get_instance())\n    # Use the configuration factory for any additional config needs\n</code></pre>"},{"location":"agents/implementation_standards/#avoid-direct-environment-variable-access","title":"Avoid Direct Environment Variable Access","text":"<pre><code># INCORRECT:\napi_key = os.getenv(\"SOME_API_KEY\")  # Direct environment access\n\n# CORRECT:\nconfig_factory = kwargs.get(\"config_factory\", ConfigFactory.get_instance())\nprovider = ProviderFactory.create(\n    provider_type=\"some_provider\",\n    model_id=self.model_id,\n    config_factory=config_factory,\n    logger=self.logger\n)\n</code></pre>"},{"location":"agents/implementation_standards/#ai-and-provider-usage","title":"AI and Provider Usage","text":""},{"location":"agents/implementation_standards/#use-the-ai-instance","title":"Use the AI Instance","text":"<p>Always use the <code>self.ai</code> instance provided by <code>BaseAgent</code> for AI interactions:</p> <pre><code># CORRECT:\nresponse = self.ai.request(prompt)\n\n# INCORRECT:\nai_instance = AI()\nresponse = ai_instance.generate(prompt)\n</code></pre>"},{"location":"agents/implementation_standards/#use-providerfactory-for-provider-access","title":"Use ProviderFactory for Provider Access","text":"<p>Never instantiate provider clients directly:</p> <pre><code># INCORRECT:\nimport openai\nclient = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# CORRECT:\nprovider = ProviderFactory.create(\n    provider_type=\"openai\",\n    model_id=self.model_id,\n    config_factory=self.config_factory,\n    logger=self.logger\n)\n</code></pre>"},{"location":"agents/implementation_standards/#interface-with-providers-through-standardized-interfaces","title":"Interface with Providers Through Standardized Interfaces","text":"<p>If you need specialized provider capabilities (like multimedia), use the appropriate interface:</p> <pre><code># Check for special capabilities\nif isinstance(provider, MultimediaProviderInterface):\n    # Use the interface methods\n    transcription, metadata = provider.transcribe_audio(audio_file)\n</code></pre>"},{"location":"agents/implementation_standards/#error-handling","title":"Error Handling","text":""},{"location":"agents/implementation_standards/#use-standardized-exceptions","title":"Use Standardized Exceptions","text":"<p>Always use the framework's exception hierarchy:</p> <pre><code>from ..exceptions import AIAgentError, AIProviderError, ErrorHandler\n\n# Raise appropriate exceptions\nraise AIAgentError(\"Failed to process request\", agent_id=self.agent_id)\n</code></pre>"},{"location":"agents/implementation_standards/#use-errorhandler-for-consistent-error-handling","title":"Use ErrorHandler for Consistent Error Handling","text":"<pre><code>try:\n    # Agent operations\nexcept Exception as e:\n    error_response = ErrorHandler.handle_error(\n        AIAgentError(f\"Operation failed: {str(e)}\", agent_id=self.agent_id),\n        self.logger\n    )\n    self.logger.error(f\"Error: {error_response['message']}\")\n\n    return {\n        \"error\": error_response['message'],\n        \"agent_id\": self.agent_id,\n        \"status\": \"error\"\n    }\n</code></pre>"},{"location":"agents/implementation_standards/#proper-error-propagation","title":"Proper Error Propagation","text":"<pre><code>def _internal_method(self):\n    try:\n        # Operations\n    except Exception as e:\n        # Wrap in appropriate exception\n        raise AIAgentError(f\"Internal operation failed: {str(e)}\", agent_id=self.agent_id)\n</code></pre>"},{"location":"agents/implementation_standards/#logging","title":"Logging","text":""},{"location":"agents/implementation_standards/#use-the-logger-from-baseagent","title":"Use the Logger from BaseAgent","text":"<pre><code># CORRECT:\nself.logger.info(\"Processing request\")\nself.logger.warning(\"Potential issue: {issue}\")\nself.logger.error(f\"Error occurred: {str(error)}\")\n\n# INCORRECT:\nprint(\"Processing request\")  # Avoid print statements\n</code></pre>"},{"location":"agents/implementation_standards/#log-at-appropriate-levels","title":"Log at Appropriate Levels","text":"<ul> <li><code>debug</code>: Detailed information, typically useful only for diagnostics</li> <li><code>info</code>: Confirmation that things are working as expected</li> <li><code>warning</code>: Indication that something unexpected happened, but the application still works</li> <li><code>error</code>: Due to a more serious problem, some functionality couldn't be performed</li> <li><code>critical</code>: A serious error indicating that the program itself may be unable to continue running</li> </ul>"},{"location":"agents/implementation_standards/#response-format","title":"Response Format","text":""},{"location":"agents/implementation_standards/#standard-response-structure","title":"Standard Response Structure","text":"<p>Ensure all responses follow the standard format:</p> <pre><code>return {\n    \"content\": result,  # Main content/result\n    \"metadata\": {       # Optional metadata\n        \"key\": \"value\",\n        # Additional metadata\n    },\n    \"agent_id\": self.agent_id,\n    \"status\": \"success\"  # or \"error\"\n}\n</code></pre>"},{"location":"agents/implementation_standards/#error-response-structure","title":"Error Response Structure","text":"<pre><code>return {\n    \"error\": error_message,\n    \"agent_id\": self.agent_id,\n    \"status\": \"error\"\n}\n</code></pre>"},{"location":"agents/implementation_standards/#method-implementation","title":"Method Implementation","text":""},{"location":"agents/implementation_standards/#essential-methods","title":"Essential Methods","text":"<p>Every agent should implement:</p> <ol> <li><code>process_request(self, request)</code>: Main entry point for processing</li> <li><code>can_handle(self, request)</code>: Returns confidence score (0.0-1.0)</li> </ol>"},{"location":"agents/implementation_standards/#method-documentation","title":"Method Documentation","text":"<p>Always include clear docstrings:</p> <pre><code>def process_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a request and return a response.\n\n    Args:\n        request: Request object containing prompt and metadata\n\n    Returns:\n        Response object with content and metadata\n\n    Raises:\n        AIAgentError: If processing fails\n    \"\"\"\n</code></pre>"},{"location":"agents/implementation_standards/#testing","title":"Testing","text":""},{"location":"agents/implementation_standards/#test-all-error-paths","title":"Test All Error Paths","text":"<p>Ensure your agent handles failures gracefully:</p> <pre><code>def test_agent_handles_failure(self):\n    agent = YourAgent()\n    response = agent.process_request({\"prompt\": \"test\", \"will_fail\": True})\n    assert response[\"status\"] == \"error\"\n    assert \"error\" in response\n</code></pre>"},{"location":"agents/implementation_standards/#test-configuration-loading","title":"Test Configuration Loading","text":"<pre><code>def test_agent_loads_config(self):\n    # Create mock config\n    config = {\"some_option\": \"test_value\"}\n    config_manager = MockConfigManager(agent_configs={\"your_agent\": config})\n\n    agent = YourAgent(config_manager=config_manager)\n    assert agent.some_option == \"test_value\"\n</code></pre>"},{"location":"agents/implementation_standards/#performance-considerations","title":"Performance Considerations","text":""},{"location":"agents/implementation_standards/#lazy-loading","title":"Lazy Loading","text":"<p>Use lazy loading for heavy resources:</p> <pre><code>def _load_model(self):\n    if not hasattr(self, \"_model\"):\n        self.logger.info(\"Loading model...\")\n        self._model = SomeHeavyModel()\n    return self._model\n</code></pre>"},{"location":"agents/implementation_standards/#resource-cleanup","title":"Resource Cleanup","text":"<p>Ensure proper cleanup of resources:</p> <pre><code>def __del__(self):\n    # Clean up resources\n    if hasattr(self, \"_connection\") and self._connection:\n        self.logger.info(\"Closing connection...\")\n        self._connection.close()\n</code></pre>"},{"location":"agents/implementation_standards/#example-complete-agent-implementation","title":"Example: Complete Agent Implementation","text":"<pre><code>\"\"\"\nExample agent implementation demonstrating best practices.\n\"\"\"\nfrom typing import Dict, Any\nfrom .base_agent import BaseAgent\nfrom ..config.config_factory import ConfigFactory\nfrom ..exceptions import AIAgentError, ErrorHandler\n\nclass ExampleAgent(BaseAgent):\n    \"\"\"\n    Example agent demonstrating implementation standards.\n    \"\"\"\n\n    def __init__(self, agent_id=\"example\", **kwargs):\n        \"\"\"\n        Initialize the example agent.\n\n        Args:\n            agent_id: ID of the agent (defaults to \"example\")\n            **kwargs: Additional arguments for BaseAgent\n        \"\"\"\n        super().__init__(agent_id=agent_id, **kwargs)\n\n        # Load configuration\n        self.option1 = self.agent_config.get(\"option1\", \"default_value\")\n        self.option2 = self.agent_config.get(\"option2\", 100)\n\n        self.logger.info(f\"Initialized {self.agent_id} agent with options: {self.option1}, {self.option2}\")\n\n    def process_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a request and return a response.\n\n        Args:\n            request: Request object containing prompt and metadata\n\n        Returns:\n            Response object with content and metadata\n        \"\"\"\n        try:\n            self.logger.info(f\"Processing request: {request.get('prompt', '')[:50]}...\")\n\n            # Process the request\n            if not request.get(\"prompt\"):\n                raise AIAgentError(\"Request must include a prompt\", agent_id=self.agent_id)\n\n            # Use the AI to process\n            prompt = request[\"prompt\"]\n            response = self.ai.request(prompt)\n\n            # Return standardized response\n            return {\n                \"content\": response,\n                \"metadata\": {\n                    \"option1\": self.option1,\n                    \"length\": len(response)\n                },\n                \"agent_id\": self.agent_id,\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            error_response = ErrorHandler.handle_error(\n                AIAgentError(f\"Error processing request: {str(e)}\", agent_id=self.agent_id),\n                self.logger\n            )\n            self.logger.error(f\"Request error: {error_response['message']}\")\n\n            return {\n                \"error\": error_response['message'],\n                \"agent_id\": self.agent_id,\n                \"status\": \"error\"\n            }\n\n    def can_handle(self, request: Dict[str, Any]) -&gt; float:\n        \"\"\"\n        Determine if this agent can handle the request.\n\n        Args:\n            request: The request object\n\n        Returns:\n            Confidence score (0.0-1.0)\n        \"\"\"\n        # Check for keywords indicating this agent should handle the request\n        if not isinstance(request.get(\"prompt\"), str):\n            return 0.0\n\n        prompt = request[\"prompt\"].lower()\n        keywords = [\"example\", \"demo\", \"test\"]\n\n        for keyword in keywords:\n            if keyword in prompt:\n                return 0.8\n\n        # Default confidence\n        return 0.2\n</code></pre>"},{"location":"config/","title":"Agentic AI Configuration System","text":"<p>This document explains how to use the Agentic AI configuration system to customize your AI experience.</p>"},{"location":"config/#overview","title":"Overview","text":"<p>The configuration system provides a clean, user-facing interface for configuring the AI framework without exposing implementation details. It handles:</p> <ul> <li>Model selection and configuration</li> <li>Use case presets</li> <li>Temperature and other model parameters</li> <li>System prompts</li> <li>Debugging options</li> </ul>"},{"location":"config/#basic-usage","title":"Basic Usage","text":"<pre><code>from src.config import configure\nfrom src.core.tool_enabled_ai import ToolEnabledAI\n\n# Configure the framework\nconfigure(\n    model=\"claude-3-5-sonnet\",\n    use_case=\"solidity_coding\",\n    temperature=0.8,\n    show_thinking=True\n)\n\n# Create an AI instance - it will use the configuration applied above\nai = AI()\nresponse = ai.request(\"Write a simple ERC20 token contract\")\n</code></pre>"},{"location":"config/#configuration-options","title":"Configuration Options","text":""},{"location":"config/#model-selection","title":"Model Selection","text":"<p>Select a specific model by name:</p> <pre><code>configure(model=\"claude-3-5-sonnet\")  # Use Claude 3.5 Sonnet\nconfigure(model=\"phi4\")               # Use local Phi-4 model\n</code></pre>"},{"location":"config/#use-case-presets","title":"Use Case Presets","text":"<p>Apply a predefined configuration suitable for a specific task:</p> <pre><code>from src.config import configure, UseCasePreset\n\n# Using string\nconfigure(use_case=\"solidity_coding\")\n\n# Using enum for better IDE support\nconfigure(use_case=UseCasePreset.SOLIDITY_CODING)\n</code></pre> <p>Available use cases include:</p> <ul> <li><code>CHAT</code> - General conversational AI</li> <li><code>CODING</code> - General programming assistance</li> <li><code>SOLIDITY_CODING</code> - Ethereum smart contract development</li> <li><code>TRANSLATION</code> - Language translation</li> <li><code>CONTENT_GENERATION</code> - Creating creative content</li> <li><code>DATA_ANALYSIS</code> - Analyzing data</li> <li><code>WEB_ANALYSIS</code> - Web content analysis</li> <li><code>IMAGE_GENERATION</code> - Generating images (for compatible models)</li> </ul>"},{"location":"config/#model-parameters","title":"Model Parameters","text":"<p>Customize model behavior:</p> <pre><code>configure(\n    temperature=0.8,          # Higher values = more creative responses\n    max_tokens=2000,          # Max response length\n    system_prompt=\"You are a helpful assistant specialized in Solidity smart contract development.\"\n)\n</code></pre>"},{"location":"config/#debugging-options","title":"Debugging Options","text":"<p>Enable advanced options for development:</p> <pre><code>configure(\n    show_thinking=True        # Show AI's reasoning process\n)\n</code></pre>"},{"location":"config/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"config/#external-configuration-file","title":"External Configuration File","text":"<p>Load configuration from a YAML or JSON file:</p> <pre><code>configure(config_file=\"my_config.yml\")\n</code></pre> <p>Example YAML configuration:</p> <pre><code>model: claude-3-5-sonnet\nuse_case: solidity_coding\ntemperature: 0.8\nshow_thinking: true\nsystem_prompt: You are a helpful assistant specialized in Solidity smart contract development.\n</code></pre>"},{"location":"config/#custom-parameters","title":"Custom Parameters","text":"<p>Pass any custom parameters to the configuration system:</p> <pre><code>configure(\n    model=\"claude-3-5-sonnet\",\n    my_custom_param=\"value\",\n    another_param=123\n)\n</code></pre> <p>These custom parameters will be stored and can be accessed through the configuration API.</p>"},{"location":"config/#using-the-configuration-api","title":"Using the Configuration API","text":"<p>If you need direct access to the configuration system:</p> <pre><code>from src.config import get_config\n\nconfig = get_config()\n\n# Get the default model\ndefault_model = config.get_default_model()\n\n# Get configuration for a specific model\nmodel_config = config.get_model_config(\"claude-3-5-sonnet\")\n\n# Get all available models\nall_models = config.get_all_models()\n\n# Get system prompt (if configured)\nsystem_prompt = config.get_system_prompt()\n</code></pre>"},{"location":"config/#model-enumeration","title":"Model Enumeration","text":"<p>For type-safe model references, use the dynamically generated <code>Model</code> enum:</p> <pre><code>from src.config import Model, get_available_models, is_valid_model\n\n# Use enum member\nif Model.CLAUDE_3_5_SONNET == Model.CLAUDE_3_5_SONNET:\n    print(\"Using Claude 3.5 Sonnet\")\n\n# Check if a model is valid\nif is_valid_model(\"claude-3-5-sonnet\"):\n    print(\"Valid model\")\n\n# Get all available models\navailable_models = get_available_models()\n</code></pre>"},{"location":"config/#configuration-files","title":"Configuration Files","text":"<p>The system uses these configuration files:</p> <ul> <li><code>models.yml</code> - Model definitions and parameters</li> <li><code>providers.yml</code> - Provider configurations</li> <li><code>use_cases.yml</code> - Use case specific configurations</li> <li><code>agents.yml</code> - Agent configurations</li> <li><code>tools.yml</code> - Tool configurations</li> </ul> <p>These files are managed by system administrators and should not be modified directly by users.</p>"},{"location":"conversations/overview/","title":"Conversation Management","text":""},{"location":"conversations/overview/#overview","title":"Overview","text":"<p>The conversation management system in Agentic-AI handles:</p> <ul> <li>Maintaining conversation history</li> <li>Formatting messages for different providers</li> <li>Extracting \"thoughts\" and other metadata from responses</li> <li>Managing context and tool calls</li> </ul>"},{"location":"conversations/overview/#conversationmanager","title":"ConversationManager","text":"<p>The <code>ConversationManager</code> class tracks the conversation state:</p> <pre><code>from src.conversation.conversation_manager import ConversationManager\n\n# Create a conversation manager\nconversation = ConversationManager()\n\n# Add messages\nconversation.add_message(role=\"user\", content=\"Hello, how are you?\")\nconversation.add_message(role=\"assistant\", content=\"I'm doing well! How can I help you today?\")\n\n# Get all messages\nmessages = conversation.get_messages()\n\n# Get the latest message\nlast_message = conversation.get_last_message()\n\n# Clear the conversation\nconversation.clear()\n</code></pre>"},{"location":"conversations/overview/#working-with-thoughts","title":"Working with Thoughts","text":"<p>The system can extract AI \"thoughts\" from responses to debug reasoning:</p> <pre><code># Enable thought extraction in the conversation manager\nconversation.add_message(\n    role=\"assistant\",\n    content=\"&lt;thinking&gt;Let me consider the best approach here...&lt;/thinking&gt;The answer is 42.\",\n    extract_thoughts=True\n)\n\n# Get the processed message (without thoughts)\nprocessed_message = conversation.get_last_message()\n# Result: {\"role\": \"assistant\", \"content\": \"The answer is 42.\"}\n\n# Access the thoughts if needed\nthoughts = conversation.get_thoughts()\n# Result: [\"Let me consider the best approach here...\"]\n</code></pre>"},{"location":"conversations/overview/#handling-tool-calls","title":"Handling Tool Calls","text":"<p>Conversation manager also tracks tool calls and their results:</p> <pre><code># Add a message with tool calls\nconversation.add_message(\n    role=\"assistant\",\n    content=\"I'll check the weather for you.\",\n    tool_calls=[\n        {\n            \"name\": \"get_weather\",\n            \"arguments\": {\"location\": \"New York\"}\n        }\n    ]\n)\n\n# Add the tool response\nconversation.add_message(\n    role=\"tool\",\n    name=\"get_weather\",\n    content=\"It's sunny and 75\u00b0F in New York.\"\n)\n</code></pre>"},{"location":"conversations/overview/#response-parser","title":"Response Parser","text":"<p>The <code>ResponseParser</code> processes raw AI responses:</p> <pre><code>from src.conversation.response_parser import ResponseParser\n\nparser = ResponseParser()\n\n# Parse a response with thoughts\nresult = parser.parse_response(\n    \"&lt;thinking&gt;I should search for this information.&lt;/thinking&gt;The capital of France is Paris.\",\n    extract_thoughts=True,\n    show_thinking=False  # Hide thoughts in final output\n)\n\n# Result: {\n#   \"content\": \"The capital of France is Paris.\",\n#   \"thoughts\": \"I should search for this information.\"\n# }\n</code></pre>"},{"location":"conversations/overview/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>User     AI      ConversationManager    ResponseParser    Provider\n |       |              |                     |               |\n | Request               |                     |               |\n |------&gt;|              |                     |               |\n |       | Add user message                   |               |\n |       |-------------&gt;|                     |               |\n |       |              |                     |               |\n |       | Get messages  |                     |               |\n |       |&lt;-------------|                     |               |\n |       |              |                     |               |\n |       | Send to provider                                   |\n |       |--------------------------------------------------&gt;|\n |       |              |                     |               |\n |       | Raw response                                       |\n |       |&lt;--------------------------------------------------|\n |       |              |                     |               |\n |       | Parse response                     |               |\n |       |-----------------------------&gt;|     |               |\n |       |              |                     |               |\n |       | Parsed response                    |               |\n |       |&lt;-----------------------------|     |               |\n |       |              |                     |               |\n |       | Add assistant message              |               |\n |       |-------------&gt;|                     |               |\n |       |              |                     |               |\n | Response             |                     |               |\n |&lt;------|              |                     |               |\n</code></pre>"},{"location":"development/testing_providers/","title":"Provider Test Cases","text":"<p>This document outlines the standard test cases that should be implemented for each AI Provider implementation (e.g., OpenAI, Anthropic, Gemini, Ollama) to ensure consistent functionality, error handling, and adherence to the framework's interfaces.</p> <p>Testing Philosophy:</p> <ul> <li>Focus on unit testing the provider's specific logic (request formatting, response parsing, error mapping).</li> <li>Mock the external SDK client to avoid actual API calls and dependencies during tests.</li> <li>Use <code>pytest</code> conventions and fixtures for setup and parameterization.</li> <li>Keep mocks minimal and focused on the interaction boundary with the SDK.</li> </ul>"},{"location":"development/testing_providers/#general-test-categories-for-all-providers","title":"General Test Categories for All Providers","text":"<p>The following categories should be covered for each provider. Specific details will vary based on the provider's features and SDK.</p>"},{"location":"development/testing_providers/#1-initialization-__init__","title":"1. Initialization (<code>__init__</code>)","text":"<ul> <li> <p>Test Case 1.1: Successful Initialization (with API Key)</p> </li> <li> <p>Goal: Verify the provider initializes correctly when valid configuration (including API key) is provided.</p> </li> <li> <p>Checks:</p> <ul> <li>Provider attributes (<code>model_id</code>, <code>provider_config</code>, <code>model_config</code>) are set correctly.</li> <li>Internal parameters (<code>self.parameters</code> like temperature, max_tokens) are parsed correctly from config.</li> <li>The underlying SDK client is instantiated correctly (mocked) with the API key.</li> <li>Logger is initialized.</li> </ul> </li> <li> <p>Test Case 1.2: Initialization Fails (Missing API Key)</p> </li> <li> <p>Goal: Verify <code>AICredentialsError</code> (or appropriate error) is raised if the API key is missing in the configuration or cannot be resolved.</p> </li> <li> <p>Checks:</p> <ul> <li>Correct exception type is raised.</li> <li>Exception message is informative.</li> <li>SDK client is not instantiated.</li> </ul> </li> <li> <p>Test Case 1.3: Initialization Fails (Missing Configuration)</p> </li> <li>Goal: Verify <code>AISetupError</code> or <code>AIConfigError</code> is raised if essential provider or model configuration is missing.</li> <li>Checks:<ul> <li>Correct exception type is raised.</li> <li>Exception message indicates the missing configuration.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#2-prepare-request-payload-_prepare_request_payload","title":"2. Prepare Request Payload (<code>_prepare_request_payload</code>)","text":"<p>(Note: This method might be implicitly tested via <code>_make_api_request</code> or <code>request</code> tests, but explicit tests can be useful if the logic is complex)</p> <ul> <li> <p>Test Case 2.1: Basic Payload Formatting</p> </li> <li> <p>Goal: Verify the provider correctly formats the input messages and base parameters into the structure expected by the SDK's API call.</p> </li> <li> <p>Checks:</p> <ul> <li><code>model</code> ID is included correctly.</li> <li><code>messages</code> are formatted according to the provider's requirements (roles, content structure).</li> <li>Base parameters (<code>temperature</code>, <code>max_tokens</code>, etc.) from <code>self.parameters</code> are included.</li> </ul> </li> <li> <p>Test Case 2.2: Payload Formatting with Options Override</p> </li> <li>Goal: Verify that parameters passed via <code>**options</code> in the <code>request</code> method override the default <code>self.parameters</code>.</li> <li>Checks:<ul> <li>Payload includes the overridden values for parameters like <code>temperature</code>.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#3-make-api-request-_make_api_request","title":"3. Make API Request (<code>_make_api_request</code>)","text":"<ul> <li> <p>Test Case 3.1: Successful API Call</p> </li> <li> <p>Goal: Verify the method correctly calls the mocked SDK client's relevant function with the prepared payload and returns the raw SDK response.</p> </li> <li> <p>Checks:</p> <ul> <li>Mock SDK client's method (e.g., <code>chat.completions.create</code>, <code>messages.create</code>) is called once.</li> <li>The call is made with the expected payload dictionary.</li> <li>The raw mock response object from the SDK client is returned.</li> </ul> </li> <li> <p>Test Case 3.2: SDK Error Mapping</p> </li> <li>Goal: Verify that various errors raised by the (mocked) SDK are caught and mapped to the framework's specific exceptions (<code>AIAuthenticationError</code>, <code>AIRateLimitError</code>, <code>InvalidRequestError</code>, <code>ModelNotFoundError</code>, <code>ContentModerationError</code>, <code>AIProviderError</code>).</li> <li>Setup: Use <code>pytest.mark.parametrize</code> to test different SDK error types.</li> <li>Checks:<ul> <li>For each simulated SDK error, the correct custom framework exception is raised.</li> <li>The exception message is informative.</li> <li>Relevant details (like status code or error code) are potentially preserved in the custom exception.</li> <li>Logger is called appropriately upon error.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#4-convert-response-_convert_response","title":"4. Convert Response (<code>_convert_response</code>)","text":"<ul> <li> <p>Test Case 4.1: Convert Response (Text Only)</p> </li> <li> <p>Goal: Verify the provider correctly parses a simple text response from the (mocked) SDK response object into the standardized <code>ProviderResponse</code> model.</p> </li> <li> <p>Checks:</p> <ul> <li><code>ProviderResponse.content</code> contains the correct text.</li> <li><code>ProviderResponse.tool_calls</code> is <code>None</code> or empty.</li> <li><code>ProviderResponse.stop_reason</code> is mapped correctly.</li> <li><code>ProviderResponse.usage</code> (tokens) is extracted correctly.</li> <li><code>ProviderResponse.model</code> ID is extracted correctly.</li> <li><code>ProviderResponse.error</code> is <code>None</code>.</li> </ul> </li> <li> <p>Test Case 4.2: Convert Response (Content Moderation / Stop Reason)</p> </li> <li>Goal: Verify specific stop reasons (like content filters) are correctly identified and mapped in the <code>ProviderResponse</code>.</li> <li>Checks:<ul> <li><code>ProviderResponse.stop_reason</code> reflects the moderation or specific stop condition.</li> <li><code>ProviderResponse.content</code> might be empty or contain a specific marker.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#5-tool-handling-if-provider-supports-tools","title":"5. Tool Handling (If Provider Supports Tools)","text":"<ul> <li> <p>Test Case 5.1: Prepare Payload with Tools</p> </li> <li> <p>Goal: Verify <code>_prepare_request_payload</code> (or equivalent logic) correctly formats the <code>tools</code> and <code>tool_choice</code> parameters according to the provider's specification when tools are provided.</p> </li> <li>Setup: Provide a list of <code>ToolDefinition</code> mocks.</li> <li> <p>Checks:</p> <ul> <li>The <code>tools</code> parameter in the payload matches the SDK's expected format.</li> <li><code>tool_choice</code> is included if specified.</li> </ul> </li> <li> <p>Test Case 5.2: Convert Response with Tool Calls</p> </li> <li> <p>Goal: Verify <code>_convert_response</code> correctly parses tool call requests from the SDK response into a list of <code>ToolCall</code> objects within the <code>ProviderResponse</code>.</p> </li> <li>Setup: Mock an SDK response indicating tool calls.</li> <li> <p>Checks:</p> <ul> <li><code>ProviderResponse.tool_calls</code> is a list of <code>ToolCall</code> objects.</li> <li>Each <code>ToolCall</code> has the correct <code>id</code>, <code>name</code>, and parsed <code>arguments</code> (as a dictionary).</li> <li><code>ProviderResponse.content</code> may or may not be present, depending on the SDK response.</li> <li><code>ProviderResponse.stop_reason</code> indicates tool use (if applicable).</li> </ul> </li> <li> <p>Test Case 5.3: Convert Response with Invalid Tool Arguments</p> </li> <li> <p>Goal: Verify how <code>_convert_response</code> handles cases where the SDK returns tool call arguments that are not valid JSON (if applicable to the provider).</p> </li> <li>Setup: Mock an SDK response with a tool call where <code>arguments</code> is an invalid JSON string.</li> <li> <p>Checks:</p> <ul> <li><code>ProviderResponse.tool_calls</code> contains a <code>ToolCall</code> object.</li> <li>The <code>arguments</code> field in the <code>ToolCall</code> should ideally contain the raw, unparsed string or a representation indicating the parsing failure (e.g., <code>{\"_raw_args\": \"...\"}</code>). It should not raise an unhandled exception.</li> </ul> </li> <li> <p>Test Case 5.4: Add Tool Message Formatting (<code>_add_tool_message</code>)</p> </li> <li>Goal: Verify the provider correctly formats a <code>ToolResult</code> into the message structure expected by the provider to be sent back in the next turn.</li> <li>Setup: Provide a <code>tool_call_id</code>, <code>tool_name</code>, and <code>content</code> (result string).</li> <li>Checks:<ul> <li>The returned message list/dictionary matches the provider's required format for tool result messages (e.g., role='tool', specific keys for ID/name/content).</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#next-steps","title":"Next Steps","text":"<p>With these general cases defined, the next step is to write the specific <code>pytest</code> files for each provider, starting with one (e.g., <code>OpenAIProvider</code>), implementing these tests using mocks for the <code>openai</code> SDK.</p>"},{"location":"development/testing_strategy/","title":"Comprehensive Testing Strategy","text":"<p>This document outlines the general testing strategy for the Agentic-AI framework, covering unit, integration, and potentially end-to-end testing approaches. It serves as the primary guide for writing tests.</p> <p>Note:</p> <ul> <li>Provider-specific testing details are found in <code>docs/development/testing_providers.md</code>.</li> <li>For other complex modules (e.g., specific agents, core AI classes), detailed testing guidance may be included as a \"Testing\" subsection within their primary documentation page.</li> </ul>"},{"location":"development/testing_strategy/#i-overarching-principles","title":"I. Overarching Principles","text":"<ol> <li>Testing Pyramid: Adhere to the testing pyramid principle:<ul> <li>Base: Comprehensive Unit Tests for individual components. Focus on testing logic in isolation.</li> <li>Middle: Focused Integration Tests for interactions between key components. Verify contracts and collaborations.</li> <li>Top: Minimal End-to-End (E2E) Tests for critical user flows (if applicable, especially involving the UI). Ensure the system works from user input to final output.</li> </ul> </li> <li>Test Granularity: Write tests at the appropriate level. Unit tests should mock dependencies outside the unit (e.g., external APIs, filesystem, other classes), while integration tests should involve real instances of collaborating components (mocking only external systems like actual LLM APIs or databases not under test).</li> <li>Framework: Use <code>pytest</code> as the primary testing framework. Leverage fixtures (<code>tests/conftest.py</code>) for reusable setup (mocks, configurations, test data, temporary files/directories).</li> <li>Mocking: Use <code>unittest.mock</code> (<code>Mock</code>, <code>MagicMock</code>, <code>patch</code>) for mocking dependencies in unit tests. Keep mocks focused on the interaction boundary \u2013 mock what a dependency returns or that it was called correctly, not its internal implementation.</li> <li>Coverage: Aim for high unit test coverage on core logic, complex algorithms, and critical components. Use coverage reports (<code>pytest-cov</code>) as a guide but prioritize testing essential behaviors, edge cases, and error conditions over blindly chasing 100% line coverage.</li> <li>Location: Maintain a clear and consistent test structure mirroring the <code>src/</code> directory:<ul> <li><code>tests/unit/&lt;component&gt;/</code>: For unit tests corresponding to <code>src/&lt;component&gt;/</code>.</li> <li><code>tests/integration/&lt;component_or_workflow&gt;/</code>: For integration tests focusing on interactions between specific components or end-to-end workflows within the backend.</li> <li><code>tests/e2e/</code>: For true end-to-end tests involving external interfaces like a UI or API gateway (if applicable).</li> </ul> </li> <li>Verify, don't assume: When creating tests for any class, verify the constructor's arguments, class methods, and their arguments. Don't assume you know them, as the consumer class may overlook changes made in the class it uses after some refactoring.</li> </ol>"},{"location":"development/testing_strategy/#ii-component-specific-strategies","title":"II. Component-Specific Strategies","text":"<p>The following outlines the recommended testing approach for each major component area found in <code>src/</code>.</p> <ol> <li><code>config/</code> (<code>UnifiedConfig</code>, Loaders, etc.):<ul> <li>Unit Tests: High priority. Test loading from default paths, specified paths, environment variables. Verify correct merging logic (e.g., file overrides defaults, user overrides base). Test access methods (<code>get_provider_config</code>, <code>get_model_config</code>, <code>get_api_key</code>, etc.). Test error handling for missing files, malformed content (e.g., invalid YAML/JSON), and missing required keys. Mock filesystem operations (<code>pathlib.Path</code>, <code>open</code>, <code>os.environ</code>) extensively using fixtures like <code>tmp_path</code>.</li> </ul> </li> <li><code>core/</code>:<ul> <li><code>providers/</code>: Follow the detailed strategy in <code>docs/development/testing_providers.md</code>. Focus on unit tests mocking the specific provider SDKs or HTTP clients (<code>openai</code>, <code>anthropic</code>, <code>google.generativeai</code>, <code>ollama</code>, <code>requests</code>).</li> <li><code>interfaces/</code>: No explicit tests needed (abstract definitions).</li> <li><code>models/</code>: Unit tests for any custom validation logic, methods, or complex default factories within Pydantic/dataclass models. Basic validation is implicitly tested during usage in other components.</li> <li><code>provider_factory.py</code>: Unit tests covering provider registration (including duplicates/errors), creation logic (correct class instantiation, parameter passing from config), retrieval of registered providers, and error handling for unknown providers or configuration issues. Mock <code>UnifiedConfig</code>.</li> <li><code>base_ai.py</code> / <code>tool_enabled_ai.py</code> (Main AI Classes):</li> <li>Unit Tests: Test core logic like prompt assembly (if complex), basic provider interaction (using a mock <code>ProviderInterface</code>), handling simple provider responses (<code>ProviderResponse</code>), parsing tool calls, invoking the tool manager, and formatting tool results for the provider. Mock <code>ProviderFactory</code> or <code>ProviderInterface</code>, <code>ToolManager</code>, <code>ConversationManager</code>.</li> <li>Integration Tests: Test interaction with a real <code>ProviderFactory</code> (using mock providers). Test the full request-&gt;tool_call-&gt;tool_result-&gt;request loop using mock providers that simulate tool calls and responses, interacting with a real <code>ConversationManager</code> and <code>ToolManager</code> (using mock tools/executor). Test error handling during the loop (provider errors, tool errors).</li> <li><code>model_selector.py</code>: Unit tests covering model selection logic based on use case, quality, speed, cost constraints. Test filtering, cost calculation, best model selection, and enum mapping. Mock <code>UnifiedConfig</code> to provide controlled model/use-case data.</li> </ul> </li> <li><code>tools/</code>:<ul> <li><code>tool_registry.py</code>: Unit tests for registering tools (checking for duplicates, validation), formatting tools for different provider types (OpenAI, Anthropic, etc.), retrieving tool definitions.</li> <li><code>tool_executor.py</code>: Unit tests for executing tool functions successfully, handling execution errors (capturing exceptions in <code>ToolResult</code>), enforcing timeouts (patching <code>signal</code> or using appropriate async constructs if refactored), implementing retries. Mock the actual tool functions being executed.</li> <li><code>tool_manager.py</code>: Unit tests for coordinating registration (via <code>ToolRegistry</code>), execution (via <code>ToolExecutor</code>), and potentially formatting. Mock <code>ToolRegistry</code> and <code>ToolExecutor</code>.</li> <li>Specific Tools (functions/classes defined): Unit test the logic of each individual tool function/class itself, mocking any external dependencies (APIs, libraries) they might use.</li> </ul> </li> <li><code>agents/</code>:<ul> <li><code>base_agent.py</code>: Unit tests for any common setup, helper methods, or abstract logic defined in the base class.</li> <li><code>agent_registry.py</code>: Unit tests covering class registration (mocking <code>issubclass</code>), handling duplicates (overwriting), retrieving classes, and handling invalid types. Mock internal <code>_register_agents</code> call during init for isolation.</li> <li><code>agent_factory.py</code>: Unit tests covering agent class registration, creation logic based on type and configuration, dependency injection (if applicable), retrieval of registered types, and error handling for unknown types. Mock <code>AgentRegistry</code> and the agent classes being instantiated.</li> <li><code>agent_registrar.py</code>: Unit tests verifying that the correct agent classes are registered with the provided registry mock.</li> <li>Specific Agents (<code>Coordinator</code>, <code>RequestAnalyzer</code>, etc.):</li> <li>Unit Tests: Test the agent's specific logic, decision-making processes, state management, and interaction with its direct dependencies. Mock dependencies like other agents, <code>ToolEnabledAI</code> (or its interface), <code>ToolManager</code>, <code>PromptTemplate</code>, etc. Test different input scenarios and expected outputs or state changes.</li> <li>Integration Tests: Test interactions between collaborating agents (e.g., <code>Coordinator</code> -&gt; <code>RequestAnalyzer</code> -&gt; <code>SpecificTaskAgent</code>). Test agents interacting with a real (but potentially configured with mock providers) <code>ToolEnabledAI</code> instance to verify the flow of data and control.</li> </ul> </li> <li><code>prompts/</code>:<ul> <li>Unit Tests: Test template loading (from file or string), rendering with various valid/invalid inputs (including missing variables), handling different template formats if supported, and potential error conditions during rendering.</li> </ul> </li> <li><code>conversation/</code>:<ul> <li>Unit Tests: Test conversation history management: adding user/assistant/tool messages, retrieving history (full or truncated), enforcing length limits (token count or message count), formatting history for different provider needs, serialization/deserialization if applicable.</li> </ul> </li> <li><code>utils/</code>:<ul> <li>Unit Tests: Test each utility function or class independently. Ensure pure functions are tested with various inputs and edge cases. Mock external dependencies for utilities that interact with I/O, network, etc. (e.g., file system wrappers, HTTP clients, logger backends).</li> </ul> </li> <li><code>metrics/</code>:<ul> <li>Unit Tests: Test metric collection logic (incrementing counters, recording timings), aggregation methods, and formatting/reporting logic. Mock the underlying storage or reporting mechanism (e.g., logging, database, external monitoring service).</li> </ul> </li> <li><code>exceptions.py</code>: No explicit tests needed (definitions only). Their correct usage and propagation are tested in other components.</li> <li><code>ui/</code>: (If a UI component exists)<ul> <li>Unit Tests: For any backend API handlers, data processing logic, or state management associated specifically with the UI. Mock interactions with other backend components (<code>agents</code>, <code>core</code>, etc.).</li> <li>E2E Tests: Use browser automation tools (e.g., Playwright, Selenium) to simulate user interactions and verify end-to-end flows through the UI and backend. Focus on critical paths.</li> </ul> </li> </ol>"},{"location":"development/testing_strategy/#iii-integration-test-priorities","title":"III. Integration Test Priorities","text":"<p>Start with integration tests that cover fundamental interactions:</p> <ol> <li>Config loading -&gt; Provider Factory -&gt; Provider Creation (using mock SDKs).</li> <li>Core AI (<code>ToolEnabledAI</code>) -&gt; Provider Interaction (using mock <code>ProviderInterface</code> simulating success, errors, tool calls).</li> <li>Core AI (<code>ToolEnabledAI</code>) -&gt; Tool Manager -&gt; Tool Executor (using mock tool functions).</li> <li>Full Tool Loop: Core AI -&gt; Mock Provider (returns tool call) -&gt; Tool Manager -&gt; Mock Tool -&gt; Core AI -&gt; Mock Provider (processes tool result).</li> <li>Agent (<code>Coordinator</code>) -&gt; Core AI (<code>ToolEnabledAI</code>) interaction (basic request/response).</li> <li>Agent (<code>Coordinator</code>) -&gt; Core AI (<code>ToolEnabledAI</code>) -&gt; Tool loop execution.</li> </ol>"},{"location":"development/testing_strategy/#iv-test-execution-and-ci","title":"IV. Test Execution and CI","text":"<ul> <li>Tests should be easily runnable locally via <code>pytest</code>.</li> <li>Integrate test execution into the Continuous Integration (CI) pipeline (e.g., GitHub Actions).</li> <li>Run linters (e.g., Ruff, MyPy) and formatters (e.g., Black, isort) alongside tests in CI.</li> <li>Consider running tests automatically on pull requests.</li> <li>Track test coverage and identify significant gaps in critical areas.</li> </ul>"},{"location":"development/testing_strategy/#v-prioritization","title":"V. Prioritization","text":"<ol> <li>Unit Tests - Foundational: <code>config</code>, <code>core/providers</code> (as per <code>testing_providers.md</code>), <code>tools</code> (core classes), <code>prompts</code>, <code>conversation</code>, <code>utils</code>.</li> <li>Unit Tests - Core Logic: <code>core/ai_core</code>, <code>agents</code> (base and specific).</li> <li>Integration Tests: Start with the priority list in Section III.</li> <li>Unit Tests - Others: <code>metrics</code>, <code>core/models</code> (if complex).</li> <li>E2E Tests: If UI exists.</li> </ol>"},{"location":"examples/configuration_example/","title":"Configuration Usage Examples","text":"<p>This document provides examples of how to use the new configuration system in various scenarios.</p>"},{"location":"examples/configuration_example/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.unified_config import UnifiedConfig\n\n# Get the configuration instance\nconfig = UnifiedConfig.get_instance()\n\n# Create an AI instance using the default configuration\nai = ToolEnabledAI()\nresponse = ai.request(\"Hello, who are you?\")\nprint(f\"Default AI Response: {response}\")\n</code></pre>"},{"location":"examples/configuration_example/#example-2-using-configuration-file","title":"Example 2: Using Configuration File","text":"<p>Create a <code>user_config.yml</code> file:</p> <pre><code># user_config.yml\nmodel: gpt-4o-mini\ntemperature: 0.8\nshow_thinking: true\n</code></pre> <p>Then load it:</p> <pre><code>import sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.unified_config import UnifiedConfig\nfrom src.config.user_config import UserConfig\n\n# Load user configuration from file\nuser_cfg = UserConfig(config_file=\"user_config.yml\")\nconfig = UnifiedConfig.get_instance(user_config=user_cfg)\n\n# Create an AI instance with the configured model\nai = ToolEnabledAI()\nresponse = ai.request(\"Explain the concept of recursion\")\nprint(f\"Configured AI Response: {response}\")\n</code></pre>"},{"location":"examples/configuration_example/#example-3-selecting-model-via-use-case","title":"Example 3: Selecting Model via Use Case","text":"<p>Assuming <code>use_cases.yml</code> defines a <code>solidity_coding</code> use case and potentially associates preferred models.</p> <pre><code>import sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.unified_config import UnifiedConfig\nfrom src.core.model_selector import ModelSelector, UseCase\n\nconfig = UnifiedConfig.get_instance()\nselector = ModelSelector(config=config)\n\n# Select model based on use case\nselected_model = selector.select_model(UseCase.SOLIDITY_CODING)\n\n# Create an AI instance optimized for Solidity coding\nai = ToolEnabledAI(model=selected_model.value if selected_model else None)\nresponse = ai.request(\"Write a simple Solidity smart contract for storing a value.\")\nprint(f\"Solidity AI Response: {response}\")\n</code></pre>"},{"location":"examples/configuration_example/#custom-configuration-with-multiple-options","title":"Custom Configuration with Multiple Options","text":"<pre><code>from src.config import configure\nfrom src.core.tool_enabled_ai import AI\n\n# Configure with multiple options\nconfigure(\n    model=\"claude-3-5-sonnet\",\n    temperature=0.8,\n    system_prompt=\"You are a helpful assistant specialized in Solidity smart contract development.\",\n    show_thinking=True,\n    max_tokens=2000\n)\n\n# Create an AI instance with the custom configuration\nai = AI()\nresponse = ai.request(\"What is the best way to implement a token vesting mechanism?\")\nprint(response)\n</code></pre>"},{"location":"examples/configuration_example/#loading-configuration-from-a-file","title":"Loading Configuration from a File","text":"<p>Create a file named <code>solidity_config.yml</code>:</p> <pre><code>model: claude-3-5-sonnet\nuse_case: solidity_coding\ntemperature: 0.8\nshow_thinking: true\nsystem_prompt: You are a helpful assistant specialized in Solidity smart contract development.\n</code></pre> <p>Then use it in your code:</p> <pre><code>from src.config import configure\nfrom src.core.tool_enabled_ai import AI\n\n# Load configuration from a file\nconfigure(config_file=\"solidity_config.yml\")\n\n# Create an AI instance using the loaded configuration\nai = AI()\nresponse = ai.request(\"Explain gas optimization techniques\")\nprint(response)\n</code></pre>"},{"location":"examples/configuration_example/#accessing-the-configuration-api-directly","title":"Accessing the Configuration API Directly","text":"<pre><code>from src.config import get_config, configure, get_available_models\nfrom src.core.tool_enabled_ai import AI\n\n# Configure the system\nconfigure(model=\"claude-3-5-sonnet\")\n\n# Get the configuration instance\nconfig = get_config()\n\n# Get all available models\navailable_models = get_available_models()\nprint(\"Available models:\", available_models)\n\n# Get the current default model\ndefault_model = config.get_default_model()\nprint(\"Default model:\", default_model)\n\n# Get the model configuration\nmodel_config = config.get_model_config(default_model)\nprint(\"Model name:\", model_config[\"name\"])\nprint(\"Provider:\", model_config[\"provider\"])\nprint(\"Quality:\", model_config[\"quality\"])\nprint(\"Speed:\", model_config[\"speed\"])\n\n# Create an AI instance\nai = AI()\nresponse = ai.request(\"Hello!\")\nprint(response)\n</code></pre>"},{"location":"examples/configuration_example/#using-with-the-orchestrator","title":"Using with the Orchestrator","text":"<pre><code>from src.config import configure, UseCasePreset\nfrom src.agents.orchestrator import Orchestrator\n\n# Configure for data analysis use case\nconfigure(\n    model=\"claude-3-5-sonnet\",\n    use_case=UseCasePreset.DATA_ANALYSIS,\n    show_thinking=True\n)\n\n# Create an orchestrator (it will use the configured settings)\norchestrator = Orchestrator()\n\n# Process a request\nresponse = orchestrator.process_request({\n    \"prompt\": \"Analyze the trends in this data: [1, 3, 6, 10, 15, 21, 28]\",\n    \"conversation_history\": []\n})\n\nprint(response[\"content\"])\n</code></pre>"},{"location":"prompts/management/","title":"Prompt Management","text":""},{"location":"prompts/management/#overview","title":"Overview","text":"<p>The prompt management system in Agentic-AI allows you to:</p> <ul> <li>Create reusable prompt templates with variables</li> <li>Version prompts to test different variations</li> <li>Track performance metrics for prompts</li> <li>A/B test different prompt versions</li> </ul>"},{"location":"prompts/management/#creating-templates","title":"Creating Templates","text":"<p>Prompt templates are parameterized prompts with placeholders for variables:</p> <pre><code>from src.prompts import PromptManager\n\n# Initialize prompt manager\nprompt_manager = PromptManager(storage_dir=\"data/prompts\")\n\n# Create a template\ntemplate_id = prompt_manager.create_template(\n    name=\"Question Template\",\n    description=\"Template for asking questions about topics\",\n    template=\"Answer this question about {{topic}}: {{question}}\",\n    default_values={\"topic\": \"general knowledge\"}\n)\n\n# Use the template with the AI\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    config_manager=config_manager,\n    prompt_manager=prompt_manager\n)\n\nresponse = ai.request_with_template(\n    template_id=template_id,\n    variables={\n        \"topic\": \"history\",\n        \"question\": \"When was the Declaration of Independence signed?\"\n    }\n)\n</code></pre>"},{"location":"prompts/management/#versioning-prompts","title":"Versioning Prompts","text":"<p>Create and test multiple versions of a prompt template:</p> <pre><code># Create an alternative version\nversion_id = prompt_manager.create_version(\n    template_id=template_id,\n    template_string=\"I need information about {{topic}}. Please answer: {{question}}\",\n    name=\"Alternative Wording\",\n    description=\"Different wording to test effectiveness\"\n)\n\n# Set a version as active\nprompt_manager.set_active_version(template_id, version_id)\n\n# Or create and set as active in one step\nversion_id = prompt_manager.create_version(\n    template_id=template_id,\n    template_string=\"New template text with {{variables}}\",\n    set_active=True\n)\n</code></pre>"},{"location":"prompts/management/#tracking-metrics","title":"Tracking Metrics","text":"<p>The prompt management system automatically tracks usage and performance metrics:</p> <pre><code># Get metrics for a template\nmetrics = prompt_manager.get_template_metrics(template_id)\n\nprint(f\"Template used {metrics['usage_count']} times\")\nfor metric_name, values in metrics[\"metrics\"].items():\n    print(f\"{metric_name}: avg={values['avg']}, min={values['min']}, max={values['max']}\")\n\n# You can also record custom metrics\nprompt_manager.record_prompt_performance(\n    usage_id=\"some-usage-id\",\n    metrics={\n        \"accuracy\": 0.95,\n        \"relevance\": 0.87\n    }\n)\n</code></pre>"},{"location":"prompts/management/#ab-testing","title":"A/B Testing","text":"<p>Perform A/B testing by providing a user ID when using templates:</p> <pre><code># Different users will get different versions based on consistent hashing\nresponse1 = ai.request_with_template(\n    template_id=template_id,\n    variables={\"key\": \"value\"},\n    user_id=\"user-123\"\n)\n\nresponse2 = ai.request_with_template(\n    template_id=template_id,\n    variables={\"key\": \"value\"},\n    user_id=\"user-456\"\n)\n\n# View metrics by version\nmetrics_by_version = prompt_manager.get_version_metrics(template_id)\nfor version_id, metrics in metrics_by_version.items():\n    print(f\"Version {version_id}: {metrics}\")\n</code></pre>"},{"location":"prompts/management/#example-usage","title":"Example Usage","text":"<pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.prompts.prompt_template import PromptTemplate\n\n# Initialize template service (loads templates from default directory)\ntemplate_service = PromptTemplate()\n\n# Create AI instance, passing the template service\nai = ToolEnabledAI(\n    prompt_template=template_service\n    # ... other AI config ...\n)\n\n# Example: Use a template for a request\nvariables = {\"topic\": \"renewable energy\"}\nresponse = ai.request_with_template(\n    template_id=\"explain_concept\",\n    variables=variables,\n    version=\"v1\" # Optional: specify version\n)\nprint(response)\n\n# Example: Track performance\n# Assuming 'request_with_template' returns usage_id along with response\n# (or modify AI base to store last usage_id)\n# usage_id = ...\n# metrics = {\"tokens_used\": 500, \"success\": True}\n# template_service.record_prompt_performance(usage_id, metrics)\n</code></pre>"},{"location":"providers/overview/","title":"AI Providers &amp; Models","text":""},{"location":"providers/overview/#supported-providers","title":"Supported Providers","text":"<p>Agentic-AI supports multiple AI model providers:</p> <ul> <li>Anthropic: Claude models (3.5 Haiku, 3.7 Sonnet, etc.)</li> <li>OpenAI: GPT models (4o, o3-mini, etc.)</li> <li>Google: Gemini models (2.5 Pro, 1.5 Pro, etc.)</li> <li>Ollama: Open-source models for local deployment</li> </ul> <p>Providers are supported via official SDKs.</p>"},{"location":"providers/overview/#models-configuration","title":"Models Configuration","text":"<p>Models are configured in the <code>config.yml</code> file:</p> <pre><code>models:\n  claude-3-7-sonnet:\n    name: \"Claude 3.7 Sonnet\"\n    model_id: claude-3-7-sonnet-20250219\n    provider: anthropic\n    privacy: EXTERNAL\n    quality: MAXIMUM\n    speed: STANDARD\n    parameters: 350000000000 # 350B\n    input_limit: 1000000\n    output_limit: 200000\n    temperature: 0.7\n    cost:\n      input_tokens: 3 # $3 per 1M input tokens\n      output_tokens: 15 # $15 per 1M output tokens\n      minimum_cost: 0.0001 # Minimum cost per request\n  ollama:\n    api_url: http://localhost:11434\n    # Ollama-specific settings\n</code></pre>"},{"location":"providers/overview/#model-selection","title":"Model Selection","text":"<p>Models are defined in the <code>models.py</code> file as an enumeration:</p> <pre><code>from enum import Enum\n\nclass Model(Enum):\n    # Anthropic models\n    CLAUDE_3_5_SONNET = \"claude-3-5-sonnet-20240620\"\n    CLAUDE_3_7_SONNET = \"claude-3-7-sonnet-20250219\"\n    CLAUDE_3_OPUS = \"claude-3-opus-20240229\"\n\n    # OpenAI models\n    GPT_4O = \"gpt-4o\"\n    GPT_4O_MINI = \"gpt-4o-mini\"\n    GPT_35_TURBO = \"gpt-3.5-turbo\"\n\n    # Google models\n    GEMINI_PRO = \"gemini-pro\"\n    GEMINI_2_5_PRO = \"gemini-2.5-pro\"\n\n    # Ollama models\n    LLAMA3_8B = \"llama3:8b\"\n    LLAMA3_70B = \"llama3:70b\"\n</code></pre> <p>And mapped to providers in the configuration:</p> <pre><code>models:\n  claude-3-5-sonnet-20240620:\n    provider: anthropic\n    model_id: claude-3-5-sonnet-20240620\n    ...\n  gpt-4o:\n    provider: openai\n    model_id: gpt-4o\n</code></pre>"},{"location":"providers/overview/#using-models","title":"Using Models","text":"<p>Create an AI instance with your chosen model:</p> <pre><code>from src.config.models import Model\nfrom src.core.tool_enabled_ai import ToolEnabledAI\n\n# Using an Anthropic model\nai = AI(model=Model.CLAUDE_3_7_SONNET)\n\n# Using an OpenAI model\nai = AI(model=Model.GPT_4O)\n\n# Using a Google model\nai = AI(model=Model.GEMINI_2_5_PRO)\n\n# Using an Ollama model\nai = AI(model=Model.GEMMA3-27B)\n</code></pre>"},{"location":"providers/overview/#model-selection-helper","title":"Model Selection Helper","text":"<p>The ModelSelector helps choose the appropriate model for different use cases:</p> <pre><code>from src.core.model_selector import ModelSelector, UseCase\n\n# Get a model recommendation\nrecommended_model = ModelSelector.select_model(\n    use_case=UseCase.CREATIVE_WRITING,\n    quality_preference=Quality.HIGH,\n    speed_preference=Speed.BALANCED\n)\n\n# Create AI with the recommended model\nai = AI(model=recommended_model)\n</code></pre>"},{"location":"tools/auto_tool_finding/","title":"Automatic Tool Finding","text":"<p>Agentic-AI includes a powerful feature called <code>AIToolFinder</code> that enables AI to automatically discover and select relevant tools based on user input.</p>"},{"location":"tools/auto_tool_finding/#overview","title":"Overview","text":"<p>The <code>AIToolFinder</code> component uses an AI model to analyze:</p> <ol> <li>The user's request</li> <li>Available tools and their descriptions</li> <li>Conversation context (optional)</li> </ol> <p>Based on this analysis, it selects the most appropriate tools for handling the request.</p>"},{"location":"tools/auto_tool_finding/#how-it-works","title":"How It Works","text":""},{"location":"tools/auto_tool_finding/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>User Query\n    |\n    v\nAIToolFinder\n    |\n    +-----&gt; Available Tools Catalog\n    |           |\n    |&lt;-----------\n    |\n    +-----&gt; Tool Selection AI Model\n    |           |\n    |&lt;-----------\n    |\n    v\nSelected Tools\n    |\n    v\nTool Executor\n    |\n    v\nFinal Response\n</code></pre>"},{"location":"tools/auto_tool_finding/#behind-the-scenes","title":"Behind the Scenes","text":"<ol> <li> <p>When a user submits a query, the <code>AIToolFinder</code> prepares a prompt containing:</p> </li> <li> <p>The user's query</p> </li> <li>A list of all available tools with descriptions</li> <li> <p>Optional conversation history for context</p> </li> <li> <p>This prompt is sent to a configured AI model that specializes in tool selection</p> </li> <li> <p>The AI analyzes the query and tool descriptions, then returns a JSON response listing the relevant tools:</p> </li> </ol> <pre><code>{\n  \"tools\": [\"tool_name_1\", \"tool_name_2\"]\n}\n</code></pre> <ol> <li> <p><code>AIToolFinder</code> verifies the selected tools against the available tools catalog</p> </li> <li> <p>The validated list of tool names is returned to the calling component (usually <code>ToolManager</code>)</p> </li> </ol>"},{"location":"tools/auto_tool_finding/#implementation","title":"Implementation","text":"<p>The core of <code>AIToolFinder</code> is in <code>src/tools/ai_tool_finder.py</code>:</p> <pre><code>class AIToolFinder:\n    \"\"\"\n    Uses an AI model to find relevant tools based on user prompts and tool descriptions.\n    \"\"\"\n\n    def __init__(self,\n                 model_id: str,\n                 config_manager: ConfigManager,\n                 logger: LoggerInterface):\n        \"\"\"\n        Initialize the AIToolFinder.\n\n        Args:\n            model_id: The ID of the AI model to use for tool finding.\n            config_manager: Configuration manager instance.\n            logger: Logger instance.\n        \"\"\"\n        # ... initialization ...\n\n    def find_tools(self, user_prompt: str, conversation_history: Optional[List[str]] = None) -&gt; Set[str]:\n        \"\"\"\n        Analyzes the user's prompt and returns a set of relevant tool names.\n        \"\"\"\n        # ... implementation ...\n</code></pre>"},{"location":"tools/auto_tool_finding/#usage","title":"Usage","text":""},{"location":"tools/auto_tool_finding/#enabling-auto-tool-finding","title":"Enabling Auto Tool Finding","text":"<p>Enable automatic tool finding by setting <code>auto_find_tools=True</code> when creating an AI instance:</p> <pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.models import Model\n\n# Create AI with auto tool finding\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    auto_find_tools=True\n)\n\n# Register tools\nai.register_tool(\"tool1\", tool1_function, \"Tool 1 description\")\nai.register_tool(\"tool2\", tool2_function, \"Tool 2 description\")\n\n# When user requests come in, AIToolFinder will automatically select tools\nresponse = ai.request(\"Can you tell me the weather in New York?\")\n</code></pre>"},{"location":"tools/auto_tool_finding/#manual-tool-finding-via-toolmanager","title":"Manual Tool Finding via ToolManager","text":"<p>You can also use the <code>ToolManager</code> directly:</p> <pre><code>from src.tools.tool_manager import ToolManager\nfrom src.config.models import Model\n\n# Create tool manager with auto tool finding\ntool_manager = ToolManager(\n    config_manager=config_manager,\n    logger=logger\n)\n\n# Enable auto tool finding with a specific model\ntool_manager.enable_auto_tool_finding(\n    enabled=True,\n    tool_finder_model_id=Model.GPT_4O_MINI\n)\n\n# Register tools\ntool_manager.register_tool(\"tool1\", tool1_function, \"Tool 1 description\")\ntool_manager.register_tool(\"tool2\", tool2_function, \"Tool 2 description\")\n\n# Find and execute tools for a user prompt\nresults = tool_manager.process_with_tools(\"What's the weather like today?\")\n</code></pre>"},{"location":"tools/auto_tool_finding/#customizing-the-tool-finding-model","title":"Customizing the Tool Finding Model","text":"<p>You can specify which AI model should be used for tool finding:</p> <pre><code>from src.tools.tool_manager import ToolManager\nfrom src.config.models import Model\n\n# Create tool manager\ntool_manager = ToolManager()\n\n# Enable auto tool finding with a specific model\ntool_manager.enable_auto_tool_finding(\n    enabled=True,\n    tool_finder_model_id=Model.GPT_4O_MINI  # Choose a smaller/faster model for tool finding\n)\n</code></pre>"},{"location":"tools/auto_tool_finding/#best-practices","title":"Best Practices","text":"<ol> <li>Tool Descriptions: Provide clear, concise descriptions for each tool</li> <li>Tool Naming: Use descriptive names that indicate the tool's function</li> <li>Model Selection: For tool finding, smaller/faster models are often sufficient</li> <li>Performance Monitoring: Track which tools are selected and adjust descriptions if tools are being incorrectly selected</li> <li>Conversation Context: Include conversation history when relevant to provide better context for tool selection</li> </ol>"},{"location":"tools/overview/","title":"Tool Integration","text":""},{"location":"tools/overview/#overview","title":"Overview","text":"<p>Tools allow the AI to perform actions and retrieve information beyond its training data. The Agentic-AI framework includes a sophisticated tool management system that enables:</p> <ol> <li>Registering Python functions as tools</li> <li>Automatic discovery of relevant tools for user requests</li> <li>Systematic execution of tool calls with proper error handling</li> </ol>"},{"location":"tools/overview/#tool-components","title":"Tool Components","text":"<ul> <li>ToolManager: Central service that coordinates tool registration (via <code>ToolRegistry</code>), execution (via <code>ToolExecutor</code>), and retrieval of tool definitions.</li> <li>ToolRegistry: Stores tool definitions (<code>ToolDefinition</code>), handles provider-specific formatting, and tracks usage statistics.</li> <li>ToolExecutor: Executes the actual tool functions safely with timeout and retry logic.</li> <li>Models (<code>src/tools/models.py</code>): Defines core data structures like <code>ToolDefinition</code>, <code>ToolCall</code>, and <code>ToolResult</code> using Pydantic.</li> </ul>"},{"location":"tools/overview/#creating-and-registering-tools","title":"Creating and Registering Tools","text":"<p>Tools are Python functions that can be made available to the AI. The core element is the <code>ToolDefinition</code> model, which includes:</p> <ul> <li><code>name</code>: A unique name for the tool.</li> <li><code>description</code>: A clear description of what the tool does (used by the LLM).</li> <li><code>parameters_schema</code>: A JSON schema defining the expected input parameters.</li> <li><code>function</code>: The actual Python callable that implements the tool's logic.</li> </ul> <p>Tools are registered with the <code>ToolManager</code> (often during application setup or via configuration loading), which uses the <code>ToolRegistry</code> internally.</p> <pre><code>from src.tools import ToolManager, ToolDefinition\n# Assume get_weather is defined elsewhere\n# from my_tools import get_weather\n\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # Implementation...\n    return f\"Weather data for {location}\"\n\n# Create a ToolDefinition\nweather_tool_def = ToolDefinition(\n    name=\"get_weather\",\n    description=\"Get the current weather for a specific city or location.\",\n    parameters_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city or location (e.g., 'Paris, France')\"\n            }\n        },\n        \"required\": [\"location\"]\n    },\n    function=get_weather\n)\n\n# Get a ToolManager instance (e.g., from dependency injection or create default)\ntool_manager = ToolManager()\n\n# Register the tool definition\ntool_manager.register_tool(weather_tool_def.name, weather_tool_def)\n</code></pre>"},{"location":"tools/overview/#tool-execution-flow","title":"Tool Execution Flow","text":"<p>When <code>ToolEnabledAI.process_prompt</code> is called:</p> <ol> <li><code>ToolEnabledAI</code> retrieves available tool definitions from <code>ToolManager</code>.</li> <li>It passes these definitions (formatted by <code>ToolRegistry</code> via the provider) to the underlying AI Provider (e.g., OpenAI, Anthropic) along with the user prompt and conversation history.</li> <li>The LLM decides if a tool needs to be called. If so, the provider returns a <code>ProviderResponse</code> containing one or more <code>ToolCall</code> objects (with name, arguments, ID).</li> <li><code>ToolEnabledAI</code> receives the response.</li> <li>If <code>ToolCall</code> objects are present, <code>ToolEnabledAI</code> iterates through them.</li> <li>For each <code>ToolCall</code>, it invokes its internal <code>_execute_tool_call(tool_call)</code> method, which in turn uses <code>ToolManager.execute_tool(tool_name=..., **arguments)</code>.</li> <li><code>ToolManager</code> uses <code>ToolExecutor</code> to run the actual tool function associated with the <code>tool_name</code>.</li> <li><code>ToolExecutor</code> returns a <code>ToolResult</code> (success/failure, result/error) to <code>ToolManager</code>, which passes it back to <code>ToolEnabledAI</code> via <code>_execute_tool_call</code>.</li> <li><code>ToolEnabledAI</code> formats the <code>ToolResult</code> into the appropriate message format(s) for the specific provider (using the provider's <code>_add_tool_message</code> method) and adds these messages to the conversation history.</li> <li><code>ToolEnabledAI</code> calls the provider again with the updated history (including tool results).</li> <li>The loop repeats if the provider requests more tool calls (up to a maximum iteration limit).</li> <li>Once the provider returns a final response without tool calls, <code>ToolEnabledAI</code> returns the text content.</li> </ol> <p>This loop allows the AI to use tools iteratively to accomplish tasks.</p>"},{"location":"ui/gradio_interface/","title":"Gradio Chat Interface","text":"<p>The Agentic-AI framework includes a user-friendly chat interface built with Gradio. This interface integrates with the multi-agent architecture to provide a seamless user experience.</p>"},{"location":"ui/gradio_interface/#features","title":"Features","text":"<ul> <li>Text-based chat interface</li> <li>Audio input support (microphone)</li> <li>Integration with multiple agents</li> <li>Conversation history management</li> <li>System status messages</li> </ul>"},{"location":"ui/gradio_interface/#architecture","title":"Architecture","text":"<p>The UI is built around the <code>AgenticChatUI</code> class, which:</p> <ol> <li>Initializes the necessary agents (Request Rooter, Listener)</li> <li>Sets up the Gradio interface components</li> <li>Handles message routing between the UI and the agent system</li> <li>Processes both text and audio inputs</li> </ol>"},{"location":"ui/gradio_interface/#agent-integration","title":"Agent Integration","text":"<p>The chat interface works with the multi-agent architecture:</p> <ul> <li>Request Rooter Agent: Analyzes user text requests and routes them to appropriate agents</li> <li>Listener Agent: Processes audio input and converts it to text</li> </ul>"},{"location":"ui/gradio_interface/#example-usage","title":"Example Usage","text":"<pre><code>from src.ui.gradio_chat import AgenticChatUI\nfrom src.config.config_manager import ConfigManager\nfrom src.agents import AgentFactory, AgentRegistry\n\n# Initialize components\nconfig_manager = ConfigManager()\nregistry = AgentRegistry()\nagent_factory = AgentFactory(registry=registry)\n\n# Register agent types\nfrom src.agents.request_rooter import RequestRooterAgent\nfrom src.agents.listener_agent import ListenerAgent\nagent_factory.register_agent(\"request_rooter\", RequestRooterAgent)\nagent_factory.register_agent(\"listener\", ListenerAgent)\n\n# Create the UI\nchat_ui = AgenticChatUI(\n    config_manager=config_manager,\n    agent_factory=agent_factory,\n    enable_audio=True\n)\n\n# Launch the interface\nchat_ui.launch(share=True)\n</code></pre>"},{"location":"ui/gradio_interface/#running-the-ui","title":"Running the UI","text":"<p>You can run the UI directly from the examples directory:</p> <pre><code>python examples/run_chat_ui.py\n</code></pre> <p>Command line options:</p> <pre><code>--share          Create a public share link\n--no-audio       Disable audio input\n--debug          Enable debug mode (shows thinking)\n--port PORT      Port to run the server on (default: 7860)\n--config CONFIG  Path to custom config file\n</code></pre>"},{"location":"ui/gradio_interface/#customization","title":"Customization","text":"<p>The UI can be customized in several ways:</p> <ol> <li>CSS Styling: Modify the <code>_get_custom_css</code> method in the <code>AgenticChatUI</code> class</li> <li>Component Layout: Customize the Gradio layout in the <code>build_ui</code> method</li> <li>Model Selection: Choose the AI model for generation.</li> <li>Agent Configuration: Update agent parameters in the <code>agents.yml</code> file</li> <li>API Keys: Ensure necessary API keys are set as environment variables (e.g., <code>OPENAI_API_KEY</code>).</li> </ol>"},{"location":"ui/gradio_interface/#adding-new-agent-types","title":"Adding New Agent Types","text":"<p>To add support for new agent types in the UI:</p> <ol> <li>Register the agent class with the registry:</li> </ol> <pre><code>agent_factory.register_agent(\"new_agent_type\", NewAgentClass)\n</code></pre> <ol> <li> <p>Update the request rooter agent to recognize and route to the new agent</p> </li> <li> <p>If needed, add specific UI components to handle the agent's input/output requirements</p> </li> </ol>"},{"location":"ui/gradio_interface/#future-enhancements","title":"Future Enhancements","text":"<p>Planned enhancements for the UI include:</p> <ol> <li>File upload/download support</li> <li>Image/video display capabilities</li> <li>Custom visualization components for specialized agents</li> <li>Persistent conversation history</li> <li>User authentication and profiles</li> </ol>"}]}