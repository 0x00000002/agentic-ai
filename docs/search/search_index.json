{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agentic-AI","text":"<p>A modular framework for building AI applications with tool integration capabilities.</p>"},{"location":"#overview","title":"Overview","text":"<p>Agentic-AI is a Python library designed to create AI-powered applications that can:</p> <ul> <li>Use multiple AI model providers (OpenAI, Anthropic, Google, Ollama)</li> <li>Dynamically discover and call tools based on user input</li> <li>Manage conversations and maintain context</li> <li>Template and version prompts with metrics tracking</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multiple Provider Support: Use models from OpenAI, Anthropic, Google, and Ollama seamlessly</li> <li>Tool Integration: Register Python functions as tools the AI can use</li> <li>Automatic Tool Discovery: AI-powered selection of relevant tools based on user queries</li> <li>Prompt Management: Create, version, and track performance of prompt templates</li> <li>Conversation Management: Maintain context across multiple interactions</li> </ul>"},{"location":"agents/","title":"Agent System","text":""},{"location":"agents/#overview","title":"Overview","text":"<p>The Agentic AI framework utilizes a flexible agent system centered around a <code>Coordinator</code> agent. This system is designed to analyze incoming user requests, determine the appropriate course of action, and delegate tasks to specialized agents. All agents inherit common functionality from a <code>BaseAgent</code> class and are managed through an <code>AgentRegistry</code> and instantiated by an <code>AgentFactory</code>.</p>"},{"location":"agents/#core-components","title":"Core Components","text":"<ul> <li> <p><code>BaseAgent</code> (<code>src/agents/base_agent.py</code>):</p> </li> <li> <p>The foundation for all agents in the system.</p> </li> <li>Provides common initialization logic, including loading agent-specific configuration from <code>agents.yml</code> via <code>UnifiedConfig</code>.</li> <li>Handles basic request processing, including applying model and system prompt overrides specified in the request.</li> <li>Includes a default <code>can_handle()</code> method (returning low confidence) that specialized agents should override.</li> <li> <p>Provides basic response enrichment with agent ID and status.</p> </li> <li> <p><code>Coordinator</code> (<code>src/agents/coordinator.py</code>):</p> </li> <li> <p>The central orchestrator of the agent system.</p> </li> <li>Acts as the primary entry point for user requests directed at the agent layer.</li> <li>Uses <code>RequestAnalyzer</code> to classify the intent of the incoming request (e.g., META, AUDIO_TRANSCRIPTION, QUESTION, TASK).</li> <li>Routes requests based on the classified intent:<ul> <li>META: Handles directly by providing system information (agents, tools).</li> <li>AUDIO_TRANSCRIPTION: Delegates to a specialized agent (e.g., <code>ListenerAgent</code>).</li> <li>QUESTION: Delegates simple queries to a configured default agent (e.g., <code>ChatAgent</code>).</li> <li>TASK: Intended for complex requests potentially requiring multiple agents. The current implementation might delegate to a default handler or requires further development for multi-step/multi-agent planning and execution.</li> <li>UNKNOWN: Falls back to the default handler agent.</li> </ul> </li> <li>Utilizes <code>AgentFactory</code> to create instances of the required agents for delegation.</li> <li> <p>May use <code>ResponseAggregator</code> (though TASK handling is currently simplified) to combine results from multiple agents if implemented.</p> </li> <li> <p><code>AgentFactory</code> (<code>src/agents/agent_factory.py</code>):</p> </li> <li> <p>Responsible for creating instances of specific agent types.</p> </li> <li>Uses <code>AgentRegistry</code> to know which agent classes correspond to agent IDs.</li> <li> <p>Injects necessary dependencies like <code>UnifiedConfig</code>, <code>ToolManager</code>, and loggers into the created agent instances.</p> </li> <li> <p><code>AgentRegistry</code> (<code>src/agents/agent_registry.py</code>):</p> </li> <li> <p>Maintains a mapping between agent IDs (strings used in configuration and code) and their corresponding agent classes.</p> </li> <li>Allows for dynamic registration and discovery of available agents.</li> <li>Used by <code>AgentFactory</code> to find the correct class to instantiate.</li> <li> <p>Note: <code>agent_registrar.py</code> might contain related or older registration logic.</p> </li> <li> <p><code>RequestAnalyzer</code> (<code>src/agents/request_analyzer.py</code>):</p> </li> <li> <p>Analyzes incoming requests to determine user intent (META, QUESTION, TASK, etc.).</p> </li> <li> <p>May also be used (or intended to be used) to identify which specific agents or tools are best suited for a TASK request, potentially involving an LLM call with context about available agents and tools.</p> </li> <li> <p><code>ResponseAggregator</code> (<code>src/agents/response_aggregator.py</code>):</p> </li> <li>Designed to combine responses from multiple agents (primarily for complex TASK requests).</li> <li>Takes the original request and a list of individual agent responses.</li> <li>Synthesizes a final, coherent response for the user.</li> <li>Its usage might be limited depending on the current implementation status of multi-agent TASK handling in the <code>Coordinator</code>.</li> </ul>"},{"location":"agents/#specialized-agents","title":"Specialized Agents","text":"<p>These agents inherit from <code>BaseAgent</code> and perform specific functions:</p> <ul> <li> <p><code>ListenerAgent</code> (<code>src/agents/listener_agent.py</code>):</p> </li> <li> <p>Handles audio input, likely performing transcription using a specified speech-to-text model or service.</p> </li> <li> <p>Triggered by the <code>Coordinator</code> for requests identified as <code>AUDIO_TRANSCRIPTION</code>.</p> </li> <li> <p><code>ChatAgent</code> (<code>src/agents/chat_agent.py</code>):</p> </li> <li> <p>A general-purpose agent for handling conversational queries or simple questions.</p> </li> <li> <p>Often configured as the default handler in the <code>Coordinator</code> for <code>QUESTION</code> intent or fallbacks.</p> </li> <li> <p><code>CodingAssistantAgent</code> (<code>src/agents/coding_assistant_agent.py</code>):</p> </li> <li> <p>Specialized for code-related tasks (generation, explanation, debugging).</p> </li> <li> <p>Likely uses specific system prompts or configurations defined in <code>agents.yml</code>.</p> </li> <li> <p><code>ToolFinderAgent</code> (<code>src/agents/tool_finder_agent.py</code>):</p> </li> <li> <p>Responsible for identifying the most relevant tools from the <code>ToolManager</code> to fulfill a user's request.</p> </li> <li> <p>May involve an LLM call analyzing the request against the descriptions of available tools.</p> </li> <li> <p>Other Potential Agents: The system may include other specialized agents not explicitly listed here (check <code>src/agents</code> and <code>src/config/agents.yml</code>).</p> </li> </ul>"},{"location":"agents/#request-flow-simplified","title":"Request Flow (Simplified)","text":"<ol> <li>User request (potentially initiated via UI or API) reaches the agent system.</li> <li>The <code>Coordinator</code> receives the request.</li> <li><code>Coordinator</code> uses <code>RequestAnalyzer</code> to classify intent.</li> <li>If META: <code>Coordinator</code> gathers info and responds directly.</li> <li>If AUDIO: <code>Coordinator</code> uses <code>AgentFactory</code> to get <code>ListenerAgent</code>, delegates, and returns the response.</li> <li>If QUESTION/UNKNOWN: <code>Coordinator</code> uses <code>AgentFactory</code> to get the default agent (e.g., <code>ChatAgent</code>), delegates, and returns the response.</li> <li>If TASK: <code>Coordinator</code> (currently) might delegate to the default handler or execute more complex logic (potentially involving <code>ToolFinderAgent</code>, other specialized agents, and <code>ResponseAggregator</code> - requires verification of current implementation).</li> <li>The chosen agent(s) process the request, potentially using their injected <code>AIBase</code> instance (LLM) and <code>ToolManager</code>.</li> <li>The final response (potentially aggregated) is returned.</li> </ol>"},{"location":"agents/#architecture-diagram-prompt","title":"Architecture Diagram Prompt","text":"<pre><code>sequenceDiagram\n    actor User\n    participant Coord as Coordinator Agent\n    participant RA as Request Analyzer\n    participant AF as Agent Factory\n    participant AR as Agent Registry\n    participant RAgg as Response Aggregator\n    participant UC as UnifiedConfig\n    participant LA as Listener Agent\n    participant CA as Chat Agent\n    participant CAA as Coding Assistant Agent\n    participant TFA as Tool Finder Agent\n    participant TM as Tool Manager\n    participant AI as AIBase (LLM Interface)\n\n    User-&gt;&gt;Coord: Send Request\n    Coord-&gt;&gt;RA: Analyze Request\n    RA-&gt;&gt;AR: Query Available Agents\n    AR--&gt;&gt;RA: Return Agent Classes\n    RA--&gt;&gt;Coord: Return Intent\n\n    Coord-&gt;&gt;AF: Create Agent Based on Intent\n    AF-&gt;&gt;AR: Get Agent Class\n    AR--&gt;&gt;AF: Return Agent Class\n    AF-&gt;&gt;UC: Get Configuration\n    UC--&gt;&gt;AF: Return Configuration\n\n    alt Listener Intent\n        AF--&gt;&gt;LA: Instantiate\n        LA-&gt;&gt;AI: Process with LLM\n        AI--&gt;&gt;LA: Return Results\n        LA--&gt;&gt;Coord: Return Response\n    else Coding Intent\n        AF--&gt;&gt;CAA: Instantiate\n        CAA-&gt;&gt;AI: Process with LLM\n        AI--&gt;&gt;CAA: Return Results\n        CAA--&gt;&gt;Coord: Return Response\n    else Tool Finding Intent\n        AF--&gt;&gt;TFA: Instantiate\n        TFA-&gt;&gt;TM: Query Available Tools\n        TM-&gt;&gt;UC: Get Tool Configuration\n        UC--&gt;&gt;TM: Return Tool Configuration\n        TM--&gt;&gt;TFA: Return Available Tools\n        TFA-&gt;&gt;AI: Process with LLM\n        AI--&gt;&gt;TFA: Return Results\n        TFA--&gt;&gt;Coord: Return Response\n    else Default Intent\n        AF--&gt;&gt;CA: Instantiate\n        CA-&gt;&gt;AI: Process with LLM\n        AI--&gt;&gt;CA: Return Results\n        CA--&gt;&gt;Coord: Return Response\n    end\n\n    Coord-&gt;&gt;RAgg: Aggregate Responses (for TASKS)\n    RAgg--&gt;&gt;Coord: Return Aggregated Response\n\n    Coord--&gt;&gt;User: Return Final Response</code></pre> <p>(Note: This diagram represents the logical flow and dependencies based on the code review. Dashed lines indicate inheritance or potential usage.)</p>"},{"location":"architecture/","title":"Agentic-AI Architecture","text":"<p>This document describes the architecture of the Agentic-AI framework, focusing on the core design principles and component relationships.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>Agentic-AI is a modular framework for building AI applications with integrated tool usage and agent-based processing capabilities. The architecture emphasizes:</p> <ul> <li>Separation of Concerns: Each component has a clear, focused responsibility (e.g., configuration, core AI logic, tool execution, agent coordination).</li> <li>Interface-Based Design: Components interact through well-defined interfaces (<code>AIInterface</code>, <code>ProviderInterface</code>, <code>AgentInterface</code>, etc.) where applicable.</li> <li>Configuration Management: A centralized <code>UnifiedConfig</code> system loads settings from modular YAML files, allowing overrides via code, files, or environment variables.</li> <li>Dependency Injection/Access: Key services like configuration (<code>UnifiedConfig</code>), logging (<code>LoggerFactory</code>), and potentially <code>ToolManager</code> or <code>PromptTemplate</code> are often accessed via singletons or passed during initialization.</li> <li>Standardized Error Handling: A hierarchy of exceptions (<code>AIFrameworkError</code>, <code>AIConfigError</code>, <code>AIProviderError</code>, etc.) and an <code>ErrorHandler</code> facilitate consistent error management.</li> </ul>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ul> <li> <p>Configuration (<code>src/config</code>): Manages all settings.</p> </li> <li> <p><code>UnifiedConfig</code>: Singleton providing access to merged configuration from YAML files (<code>models.yml</code>, <code>providers.yml</code>, <code>agents.yml</code>, <code>tools.yml</code>, <code>use_cases.yml</code>) and overrides.</p> </li> <li><code>UserConfig</code>: Represents user-provided overrides.</li> <li> <p><code>configure()</code>: Function to apply user overrides.</p> </li> <li> <p>Core AI (<code>src/core</code>): Handles direct interaction with LLMs.</p> </li> <li> <p><code>AIBase</code>: Foundational class managing provider selection (via <code>ProviderFactory</code>), basic request/response handling, and conversation history (<code>ConversationManager</code>).</p> </li> <li><code>ToolEnabledAI</code>: Extends <code>AIBase</code> to manage the tool-calling loop, interacting with <code>ToolManager</code> and the provider.</li> <li><code>ProviderFactory</code>: Creates instances of specific provider clients.</li> <li><code>providers/</code>: Contains implementations for different AI providers (OpenAI, Anthropic, etc.), inheriting from <code>BaseProvider</code> and implementing <code>ProviderInterface</code>.<ul> <li><code>ProviderToolHandler</code>: Helper class within providers for formatting tool definitions and results.</li> </ul> </li> <li> <p><code>ConversationManager</code>: Tracks message history for AI interactions.</p> </li> <li> <p>Tools (<code>src/tools</code>): Manages function/API calling capabilities.</p> </li> <li> <p><code>ToolManager</code>: Central service coordinating tool execution (<code>execute_tool</code>) using <code>ToolExecutor</code> and retrieving tool definitions from <code>ToolRegistry</code>. Does not handle tool finding.</p> </li> <li><code>ToolRegistry</code>: Loads <code>ToolDefinition</code> objects from configuration, stores them, and provides them. Handles provider-specific formatting logic.</li> <li><code>ToolExecutor</code>: Executes the actual tool functions with timeout/retry logic.</li> <li> <p><code>models</code>: Defines <code>ToolDefinition</code>, <code>ToolCall</code>, <code>ToolResult</code>.</p> </li> <li> <p>Tools (<code>src/tools</code>): Manages function calling (internal tools) and external API interactions (MCP tools).</p> </li> <li> <p><code>ToolManager</code>: Central service coordinating tool discovery, formatting, and asynchronous execution (<code>async def execute_tool</code>). Loads definitions from <code>ToolRegistry</code> (internal) and <code>MCPClientManager</code> (MCP). Dispatches execution calls to <code>ToolExecutor</code> or <code>MCPClientManager</code> based on tool <code>source</code>.</p> </li> <li><code>ToolRegistry</code>: Loads, validates, and stores internal tool definitions (<code>source='internal'</code>) from <code>tools.yml</code>. Provides access to these definitions.</li> <li><code>MCPClientManager</code>: Loads MCP server configurations and declared MCP tool definitions (<code>source='mcp'</code>) from <code>mcp.yml</code>. Manages <code>ClientSession</code> connections to external MCP servers and handles MCP <code>call_tool</code> requests.</li> <li><code>ToolExecutor</code>: Executes only internal Python tool functions (<code>source='internal'</code>) with asyncio-based timeout/retry logic.</li> <li><code>ToolStatsManager</code>: Tracks usage statistics for all tools.</li> <li> <p><code>models</code>: Defines <code>ToolDefinition</code>, <code>ToolCall</code>, <code>ToolResult</code>.</p> </li> <li> <p>Agents (<code>src/agents</code>): Enables specialized processing and workflows.</p> </li> <li> <p><code>Coordinator</code>: Central agent orchestrating request handling based on intent analysis (<code>RequestAnalyzer</code>), often delegating to specialized agents.</p> </li> <li><code>BaseAgent</code>: Abstract base class for all agents.</li> <li><code>AgentFactory</code>: Creates agent instances using <code>AgentRegistry</code>.</li> <li><code>AgentRegistry</code>: Maps agent IDs to agent classes.</li> <li> <p>Specialized Agents (e.g., <code>ListenerAgent</code>, <code>ChatAgent</code>, <code>ToolFinderAgent</code>): Implement specific functionalities.</p> </li> <li> <p>Prompts (<code>src/prompts</code>): Manages reusable prompt templates.</p> </li> <li> <p><code>PromptTemplate</code>: Service loading templates from YAML files, handling versioning and variable substitution.</p> </li> <li> <p>Utilities (<code>src/utils</code>): Provides common functionalities.</p> </li> <li> <p><code>LoggerFactory</code>: Creates configured logger instances.</p> </li> <li> <p>Error Handling (<code>src/exceptions.py</code>): Defines custom exceptions and the <code>ErrorHandler</code>.</p> </li> <li> <p>UI (<code>src/ui</code>): Provides user interfaces.</p> </li> <li><code>SimpleChatUI</code>: A Gradio-based chat interface interacting with the <code>Coordinator</code>.</li> </ul>"},{"location":"architecture/#component-relationships-diagrams","title":"Component Relationships Diagrams","text":"<p>These diagrams illustrate the key relationships and dependencies between the major components.</p>"},{"location":"architecture/#1-configuration-system","title":"1. Configuration System","text":"<pre><code>graph TD\n    subgraph Configuration\n        UnifiedConfig[UnifiedConfig Singleton]\n        YAMLFiles[YAML Config Files]\n        ConfigureFunc[\"configure()\"]\n        UserConfig[UserConfig]\n        UnifiedConfig -- Loads --&gt; YAMLFiles\n        ConfigureFunc -- Applies --&gt; UserConfig\n        ConfigureFunc -- Updates --&gt; UnifiedConfig\n    end\n\n    subgraph Components Using Config\n        AIBase\n        ToolEnabledAI\n        Coordinator\n        ToolManager\n        ProviderImplementations\n        BaseAgent\n    end\n\n    AIBase --&gt; UnifiedConfig\n    ToolEnabledAI --&gt; UnifiedConfig\n    Coordinator --&gt; UnifiedConfig\n    ToolManager --&gt; UnifiedConfig\n    ProviderImplementations --&gt; UnifiedConfig\n    BaseAgent --&gt; UnifiedConfig</code></pre>"},{"location":"architecture/#2-core-ai-system","title":"2. Core AI System","text":"<pre><code>graph TD\n    subgraph Core AI\n        ToolEnabledAI[ToolEnabledAI]\n        AIBase[AIBase]\n        ProviderFactory[ProviderFactory]\n        ProviderImplementations[Provider Implementations]\n        ConversationManager[ConversationManager]\n        PromptTemplateSvc[PromptTemplate Service]\n\n        ToolEnabledAI -- Inherits --&gt; AIBase\n        AIBase --&gt; ProviderFactory\n        AIBase --&gt; ConversationManager\n        AIBase --&gt; PromptTemplateSvc\n        ProviderFactory --&gt; ProviderImplementations\n    end\n\n    subgraph Dependencies\n       UnifiedConfig[UnifiedConfig Singleton]\n       ToolManager[ToolManager]\n       LoggerFactory[LoggerFactory Singleton]\n    end\n\n    ToolEnabledAI --&gt; UnifiedConfig\n    ToolEnabledAI --&gt; ToolManager\n    ToolEnabledAI --&gt; LoggerFactory\n    AIBase --&gt; UnifiedConfig\n    AIBase --&gt; LoggerFactory\n    ProviderImplementations --&gt; UnifiedConfig\n    PromptTemplateSvc --&gt; LoggerFactory</code></pre>"},{"location":"architecture/#3-tool-subsystem","title":"3. Tool Subsystem","text":"<pre><code>graph TD\n    subgraph \"Users Of Tools\"\n        ToolEnabledAI -- \"Calls Execute (await)\" --&gt; ToolManager\n        BaseAgent -- \"Calls Execute (await)\" --&gt; ToolManager\n    end\n\n    subgraph \"Tools Subsystem\"\n        ToolManager -- \"Gets All Definitions\" --&gt; ToolRegistry[\"ToolRegistry (Internal Tools)\"]\n        ToolManager -- \"Gets All Definitions\" --&gt; MCPClientManager[\"MCPClientManager (MCP Tools)\"]\n\n        ToolManager -- \"Dispatches\" --&gt; ToolExecutor[\"ToolExecutor (Internal)\"]\n        ToolManager -- \"Dispatches\" --&gt; MCPClientManager\n\n        ToolManager -- \"Records Stats\" --&gt; ToolStatsManager\n\n        ToolExecutor -- \"Returns (awaitable)\" --&gt; ToolResult[\"ToolResult Model\"]\n        MCPClientManager -- \"Returns MCP Response\" --&gt; ToolManager\n\n        ToolRegistry -- \"Stores/Provides\" --&gt; InternalToolDef[\"ToolDefinition (source='internal')\"]\n        MCPClientManager -- \"Stores/Provides\" --&gt; MCPToolDef[\"ToolDefinition (source='mcp')\"]\n\n        ToolStatsManager -- \"Persists\" --&gt; StatsFile[\"Tool Stats JSON\"]\n\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; ToolRegistry\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; MCPClientManager\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; ToolExecutor\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; ToolStatsManager\n\n        ToolsYAML[\"tools.yml\"] -- \"Read By\" --&gt; UnifiedConfigRef\n        MCPYAML[\"mcp.yml\"] -- \"Read By\" --&gt; UnifiedConfigRef\n    end\n\n    subgraph \"Dependencies\"\n       ToolManager -- \"Uses\" --&gt; LoggerFactory[\"LoggerFactory\"]\n       ToolRegistry -- \"Uses\" --&gt; LoggerFactory\n       MCPClientManager -- \"Uses\" --&gt; LoggerFactory\n       ToolExecutor -- \"Uses\" --&gt; LoggerFactory\n       ToolStatsManager -- \"Uses\" --&gt; LoggerFactory\n       ToolManager -- \"Uses\" --&gt; UnifiedConfigRef\n    end\n\n    ToolEnabledAI -- \"Gets Formatted Tools\" --&gt; ToolManager\n\n    style ToolManager fill:#f9f,stroke:#333,stroke-width:2px\n    style ToolExecutor fill:#fdf,stroke:#333,stroke-width:1px\n    style MCPClientManager fill:#fdf,stroke:#333,stroke-width:1px\n    style ToolEnabledAI fill:#ccf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"architecture/#4-agent-system","title":"4. Agent System","text":"<pre><code>graph TD\n    subgraph \"Agents\"\n        Coordinator[\"Coordinator\"]\n        AgentFactory[\"AgentFactory\"]\n        AgentRegistry[\"AgentRegistry\"]\n        BaseAgent[\"BaseAgent\"]\n        ListenerAgent[\"ListenerAgent\"]\n        ChatAgent[\"ChatAgent\"]\n        ToolFinderAgent[\"ToolFinderAgent\"]\n        OtherAgents[\"...\"]\n        RequestAnalyzer[\"Request Analyzer\"]\n\n        Coordinator -- \"Inherits\" --&gt; BaseAgent\n        ListenerAgent -- \"Inherits\" --&gt; BaseAgent\n        ChatAgent -- \"Inherits\" --&gt; BaseAgent\n        ToolFinderAgent -- \"Inherits\" --&gt; BaseAgent\n        OtherAgents -- \"Inherits\" --&gt; BaseAgent\n\n        Coordinator -- \"Uses\" --&gt; AgentFactory\n        Coordinator -- \"Uses\" --&gt; RequestAnalyzer\n        AgentFactory -- \"Uses\" --&gt; AgentRegistry\n        AgentFactory -- \"Creates\" --&gt; BaseAgent\n        RequestAnalyzer -- \"Uses\" --&gt; AgentRegistry\n    end\n\n    subgraph \"Dependencies\"\n       UnifiedConfig[\"UnifiedConfig Singleton\"]\n       ToolManager[\"ToolManager\"]\n       LoggerFactory[\"LoggerFactory Singleton\"]\n       AIBase[\"AIBase\"]\n    end\n\n    BaseAgent --&gt; UnifiedConfig\n    BaseAgent --&gt; ToolManager\n    BaseAgent --&gt; LoggerFactory\n    BaseAgent --&gt; AIBase\n    Coordinator -- \"Coordinator might directly use\" --&gt; ToolManager</code></pre>"},{"location":"architecture/#5-high-level-interactions","title":"5. High-Level Interactions","text":"<pre><code>graph LR\n    subgraph User Input\n        UI[UI / API]\n    end\n\n    subgraph Core Processing\n        Coordinator[Agent System (Coordinator)]\n        ToolEnabledAI[Core AI (ToolEnabledAI)]\n    end\n\n    subgraph Supporting Systems\n        Config[Configuration (UnifiedConfig)]\n        Tools[Tool Subsystem (ToolManager)]\n        Prompts[Prompt Templates]\n        Logging[Logging]\n        Providers[Providers]\n    end\n\n    UI --&gt; Coordinator\n    UI --&gt; ToolEnabledAI # Direct AI calls possible\n\n    Coordinator -- Delegates / Uses --&gt; ToolEnabledAI\n    Coordinator --&gt; Tools\n    ToolEnabledAI --&gt; Tools\n    ToolEnabledAI --&gt; Providers\n\n    Coordinator --&gt; Config\n    ToolEnabledAI --&gt; Config\n    Tools --&gt; Config\n    Providers --&gt; Config\n    Prompts --&gt; Config\n\n    Coordinator --&gt; Prompts\n    ToolEnabledAI --&gt; Prompts\n\n    Coordinator --&gt; Logging\n    ToolEnabledAI --&gt; Logging\n    Tools --&gt; Logging\n    Providers --&gt; Logging</code></pre> <p>(Note: These diagrams illustrate major dependencies and interactions. Not all minor connections are shown for clarity.)</p>"},{"location":"architecture/#key-architectural-improvements","title":"Key Architectural Improvements","text":"<ul> <li>Unified Configuration: Replaced disparate config managers with a single <code>UnifiedConfig</code> singleton accessing modular YAML files, simplifying configuration access.</li> <li>Standardized Provider Interface: <code>BaseProvider</code> and <code>ProviderInterface</code> enforce a standard structure for provider implementations, returning a standardized <code>ProviderResponse</code> object to simplify core AI logic.</li> <li>Refined Agent System: Centralized orchestration logic in the <code>Coordinator</code> agent, supported by <code>AgentFactory</code>, <code>AgentRegistry</code>, and <code>RequestAnalyzer</code>, providing a clearer structure than earlier multi-agent concepts.</li> <li>Focused Tool Subsystem: <code>ToolManager</code>, <code>ToolRegistry</code>, <code>ToolExecutor</code>, and <code>ToolStatsManager</code> provide clear responsibilities for tool definition, formatting, execution, and usage statistics tracking, integrated seamlessly with <code>ToolEnabledAI</code>.</li> <li>Simplified Core AI: <code>AIBase</code> handles fundamental LLM interaction, while <code>ToolEnabledAI</code> specifically layers tool-calling orchestration on top.</li> <li>YAML-based Prompt Templates: Standardized on the <code>PromptTemplate</code> service loading versioned templates from YAML files as the primary prompt management method.</li> <li>Consistent Error Handling: Implemented a clear exception hierarchy and <code>ErrorHandler</code> for uniform error management.</li> </ul>"},{"location":"architecture/#usage-example","title":"Usage Example","text":"<pre><code>from src.core import ToolEnabledAI\nfrom src.agents import Coordinator\nfrom src.config import configure, UseCasePreset\nfrom src.tools import ToolDefinition, ToolManager # Assuming definition exists\n\n# --- Configuration (Optional) ---\n# Configure framework settings if defaults aren't suitable\n# configure(model=\"gpt-4o\", use_case=UseCasePreset.CODING)\n\n# --- Direct AI Usage with Tools ---\nprint(\"\\n--- Using ToolEnabledAI Directly ---\")\n# Create a ToolEnabledAI instance (uses configured model)\nai = ToolEnabledAI()\n\n# Tools are typically loaded via configuration (tools.yml)\n# If manual registration is needed (less common):\n# tool_manager = ToolManager() # Get or create manager\n# weather_tool_def = ToolDefinition(...) # Define get_weather\n# tool_manager.register_tool(\"get_weather\", weather_tool_def)\n# ai = ToolEnabledAI(tool_manager=tool_manager)\n\n# Make a request - process_prompt handles the tool loop\nresponse_ai = ai.process_prompt(\"What is the weather like in London?\")\nprint(f\"AI Response: {response_ai}\")\n\n# --- Agent-Based Usage via Coordinator ---\nprint(\"\\n--- Using Coordinator Agent ---\")\n# Create a Coordinator instance (uses default dependencies &amp; config)\ncoordinator = Coordinator()\n\n# Prepare a request for the coordinator\nrequest_data = {\n    \"prompt\": \"Summarize the main points of the latest Agentic AI documentation updates.\",\n    # Add other relevant info like user_id, session_id if needed by agents\n}\n\n# Process the request through the coordinator\nresponse_coord = coordinator.process_request(request_data)\n\n# Print the final content from the coordinator's response\nprint(f\"Coordinator Response: {response_coord.get('content')}\")\n</code></pre>"},{"location":"configuration_system/","title":"Configuration System","text":""},{"location":"configuration_system/#overview","title":"Overview","text":"<p>The Agentic AI configuration system provides a unified and flexible way to manage settings for the entire framework. It handles model selection, provider settings, agent behaviors, tool configurations, and use-case-specific parameters. The system is designed to be user-friendly while allowing for deep customization. It combines default configurations stored in YAML files with multiple override mechanisms.</p>"},{"location":"configuration_system/#core-components","title":"Core Components","text":"<ul> <li><code>UnifiedConfig</code>: A singleton class (<code>src/config/unified_config.py</code>) that acts as the central hub for all configuration data. It loads base configurations from YAML files and merges overrides from various sources.</li> <li><code>UserConfig</code>: A data class (<code>src/config/user_config.py</code>) representing user-provided configuration settings. Used by the <code>configure</code> function to pass overrides to <code>UnifiedConfig</code>.</li> <li><code>configure()</code> function: The primary user-facing function (<code>src/config/__init__.py</code>) to apply configuration settings and overrides.</li> <li><code>get_config()</code> function: A function (<code>src/config/__init__.py</code>) to retrieve the singleton <code>UnifiedConfig</code> instance for accessing detailed configuration data.</li> </ul>"},{"location":"configuration_system/#configuration-files","title":"Configuration Files","text":"<p>The base configurations are stored in several YAML files located in the <code>src/config</code> directory:</p> <ul> <li><code>models.yml</code>: Defines AI model parameters, capabilities, costs, and provider information.</li> <li><code>providers.yml</code>: Configures API providers (e.g., OpenAI, Anthropic, Ollama) including base URLs and potentially API key environment variable names.</li> <li><code>agents.yml</code>: Contains configurations specific to different agent types or roles.</li> <li><code>use_cases.yml</code>: Defines settings presets for different task types (e.g., chat, coding, solidity_coding), often specifying default models or parameters.</li> <li><code>tools.yml</code>: Configures available internal tools (Python functions within the project) that agents can use. See below for structure.</li> <li><code>mcp.yml</code>: Configures connections to external Model-Centric Protocol (MCP) servers and declares the tools those servers are expected to provide. See below for structure.</li> </ul> <p>These files provide the default settings for the framework.</p>"},{"location":"configuration_system/#toolsyml-structure","title":"<code>tools.yml</code> Structure","text":"<p>This file defines tools that are implemented as Python functions within this framework.</p> <pre><code># src/config/tools.yml\ntools:\n  - name: \"tool_name\" # Unique identifier\n    description: \"Description for LLM\" # How the AI understands the tool\n    module: \"src.path.to.module\" # Python module containing the function\n    function: \"function_name\" # Name of the sync or async Python function\n    parameters_schema: { ... } # JSON schema for arguments\n    category: \"grouping_category\" # Optional: for organization\n    source: \"internal\" # Must be \"internal\" for this file\n    speed: \"fast\" # Optional: fast, medium, slow (default: medium)\n    safety: \"native\" # Optional: native, sandboxed, external (default: native)\n\n# Tool execution settings (optional)\nexecution:\n  timeout: 30 # Default timeout in seconds\n  max_retries: 3\n\n# Tool statistics settings (optional)\nstats:\n  storage_path: \"data/tool_stats.json\"\n  track_usage: true\n</code></pre> <p>The <code>ToolRegistry</code> component is responsible for loading and managing these internal tool definitions.</p>"},{"location":"configuration_system/#mcpyml-structure","title":"<code>mcp.yml</code> Structure","text":"<p>This file defines connections to external MCP servers and declares the tools they provide. This declaration helps the LLM know about the tool, but the actual available tools might differ when connecting to the server.</p> <p>The MCP server itself must be running independently and accessible at the specified network address.</p> <pre><code># src/config/mcp.yml\nmcp_servers:\n  unique_server_name_1: # Identifier for this MCP server connection\n    description: \"Description of server\" # Optional\n    url: \"http://localhost:8001\" # REQUIRED: Network endpoint (http, https, ws, wss)\n    auth: # Optional: Authentication details\n      type: \"bearer\" # Currently only \"bearer\" is supported\n      token_env_var: \"MCP_SERVER_1_TOKEN\" # Env var containing the Bearer token\n      # NOTE: Headers (including auth) are NOT currently sent for ws/wss connections\n      # due to limitations in the underlying mcp library (v1.6.0).\n    provides_tools: # List of tools *declared* by this server\n      - name: \"mcp_tool_name_a\" # Unique name across ALL tools (internal &amp; MCP)\n        description: \"Description for LLM\"\n        inputSchema: { ... } # JSON schema for arguments (or parameters_schema)\n        speed: \"medium\" # Optional: (default: medium)\n        safety: \"external\" # Optional: (default: external)\n        # 'source' and 'mcp_server_name' are added automatically\n\n  unique_server_name_2:\n    description: \"Another server\"\n    url: \"wss://secure.example.com:8002\"\n    # No auth section means no authentication required or sent\n    provides_tools:\n      - name: \"mcp_tool_name_b\"\n        # ... tool definition ...\n</code></pre> <p>The <code>MCPClientManager</code> component is responsible for loading these configurations, establishing connections to the servers using the appropriate network client (<code>sse_client</code> for http/https, <code>websocket_client</code> for ws/wss), and providing the declared tool definitions.</p>"},{"location":"configuration_system/#usage","title":"Usage","text":"<p>The most common way to interact with the configuration system is through the <code>configure()</code> function, typically called once at the beginning of an application.</p>"},{"location":"configuration_system/#basic-configuration","title":"Basic Configuration","text":"<p>Apply basic settings like model, use case, and temperature directly:</p> <pre><code>from src.config import configure\nfrom src.core.tool_enabled_ai import ToolEnabledAI # Or your main AI class\n\n# Configure the framework\nconfigure(\n    model=\"claude-3-5-sonnet\",\n    use_case=\"solidity_coding\",\n    temperature=0.8,\n    show_thinking=True # Example debug/verbosity flag\n)\n\n# AI instances created after this will use the applied configuration\n# ai = ToolEnabledAI()\n# response = ai.request(\"Write a simple ERC20 token contract\")\n</code></pre>"},{"location":"configuration_system/#using-use-case-presets","title":"Using Use Case Presets","text":"<p>Apply predefined configurations suitable for specific tasks using the <code>UseCasePreset</code> enum or strings:</p> <pre><code>from src.config import configure, UseCasePreset\n\n# Using string\nconfigure(use_case=\"solidity_coding\")\n\n# Using enum for better IDE support and type safety\nconfigure(use_case=UseCasePreset.SOLIDITY_CODING)\n</code></pre> <p>Check <code>src/config/user_config.py</code> or <code>src/config/use_cases.yml</code> for available presets.</p>"},{"location":"configuration_system/#overriding-configuration","title":"Overriding Configuration","text":"<p>The system supports multiple ways to override the default configurations:</p> <ol> <li><code>configure()</code> function arguments: Settings passed directly to <code>configure()</code> take precedence over defaults (as shown in the basic example).</li> <li> <p>External Configuration File: Specify a YAML or JSON file containing overrides.</p> <pre><code># In your Python script\nconfigure(config_file=\"path/to/your_config.yml\")\n</code></pre> <pre><code># path/to/your_config.yml\nmodel: claude-3-5-sonnet\ntemperature: 0.9\nsystem_prompt: \"You are an expert Solidity auditor.\"\nshow_thinking: false\n</code></pre> </li> <li> <p>Environment Variables: The system uses <code>python-dotenv</code> to load variables from a <code>.env</code> file in the project root. Standard environment variables can also be used. Naming conventions might apply (e.g., provider API keys like <code>OPENAI_API_KEY</code>). Check <code>src/config/providers.yml</code> and <code>src/config/unified_config.py</code> for details on environment variable usage.</p> </li> </ol> <p>Order of Precedence (Highest to Lowest):</p> <ol> <li>Arguments passed to <code>configure()</code>.</li> <li>Settings loaded from the <code>config_file</code> specified in <code>configure()</code>.</li> <li>Environment variables.</li> <li>Default values from the base YAML files (<code>models.yml</code>, etc.).</li> </ol>"},{"location":"configuration_system/#accessing-configuration-details","title":"Accessing Configuration Details","text":"<p>For advanced use cases or framework development, you can access the detailed configuration values stored in the <code>UnifiedConfig</code> instance:</p> <pre><code>from src.config import get_config\n\nconfig = get_config() # Retrieve the singleton instance\n\n# Get the configured default model\ndefault_model_id = config.default_model # Property access\n# OR\ndefault_model_id = config.get_default_model() # Method access\n\n# Get configuration for a specific model\nmodel_config = config.get_model_config(\"claude-3-5-sonnet\")\nprint(f\"Input limit for Claude 3.5 Sonnet: {model_config.get('input_limit')}\")\n\n# Get all available model names\nall_model_names = config.get_model_names()\n\n# Get the effective system prompt (after overrides)\nsystem_prompt = config.get_system_prompt()\n\n# Get provider configuration\nopenai_config = config.get_provider_config(\"openai\")\n\n# Check debugging flags\nif config.show_thinking:\n    print(\"Showing AI thinking process...\")\n</code></pre> <p>Refer to the <code>UnifiedConfig</code> class (<code>src/config/unified_config.py</code>) for all available methods and properties.</p>"},{"location":"configuration_system/#dynamic-model-enum","title":"Dynamic Model Enum","text":"<p>For convenience and type safety, a dynamic <code>Model</code> enum is generated based on the models defined in <code>models.yml</code>.</p> <pre><code>from src.config import Model, get_available_models, is_valid_model\n\n# Use enum member for comparisons or assignments\nif selected_model == Model.CLAUDE_3_5_SONNET:\n    print(\"Using Claude 3.5 Sonnet\")\n\n# Check if a model string identifier is valid\nif is_valid_model(\"gpt-4o\"):\n    print(\"GPT-4o is a configured model.\")\n\n# Get all available model enum members\navailable_models_enum = get_available_models() # Returns list of Model enum members\n</code></pre>"},{"location":"configuration_system/#benefits","title":"Benefits","text":"<ul> <li>Unified Interface: Single point of configuration via <code>configure()</code> and access via <code>get_config()</code>.</li> <li>Modularity: Base configurations are split into logical YAML files.</li> <li>Flexibility: Multiple override mechanisms (function args, file, env vars).</li> <li>Clarity: <code>UserConfig</code> provides a clear structure for overrides.</li> <li>Discoverability: Methods like <code>get_model_names()</code>, <code>get_available_providers()</code> aid exploration.</li> <li>Testability: The singleton instance can be reset (<code>UnifiedConfig.reset_instance()</code>) for isolated testing.</li> </ul>"},{"location":"core/","title":"Core AI Components","text":"<p>This section describes the fundamental classes responsible for AI interaction and capabilities.</p>"},{"location":"core/#aibase-srccorebase_aipy","title":"<code>AIBase</code> (<code>src/core/base_ai.py</code>)","text":"<p><code>AIBase</code> serves as the foundational class for all AI interactions. It implements the basic <code>AIInterface</code> and handles core responsibilities:</p> <ul> <li>Initialization: Takes a model identifier (<code>model</code>), optional <code>system_prompt</code>, <code>logger</code>, <code>request_id</code>, and <code>PromptTemplate</code> service.</li> <li>Configuration: Fetches model and provider configurations from <code>UnifiedConfig</code>.</li> <li>Provider Instantiation: Uses <code>ProviderFactory</code> to create the appropriate <code>ProviderInterface</code> instance (e.g., <code>OpenAIProvider</code>, <code>AnthropicProvider</code>) based on the model's configuration.</li> <li>Conversation Management: Initializes and maintains a <code>ConversationManager</code> instance to track message history.</li> <li>System Prompt: Sets the initial system prompt in the conversation history, using the provided <code>system_prompt</code>, a configured default, or a basic fallback.</li> <li>Basic Request Handling:</li> <li><code>request(prompt, **options)</code>: Adds the user prompt to the conversation, sends the full message history to the provider, receives the response, adds the assistant's response to history, and returns the response content string.</li> <li><code>stream(prompt, **options)</code>: Similar to <code>request</code> but uses the provider's streaming capabilities.</li> <li>State Management: Provides methods like <code>reset_conversation()</code>, <code>get_conversation()</code>, <code>get_system_prompt()</code>, <code>set_system_prompt()</code>, and <code>get_model_info()</code>.</li> </ul> <p><code>AIBase</code> itself does not handle tool calls.</p>"},{"location":"core/#toolenabledai-srccoretool_enabled_aipy","title":"<code>ToolEnabledAI</code> (<code>src/core/tool_enabled_ai.py</code>)","text":"<p><code>ToolEnabledAI</code> inherits from <code>AIBase</code> and adds the capability to use tools (functions or external APIs) during request processing.</p> <ul> <li>Initialization: Takes the same arguments as <code>AIBase</code>, plus an optional <code>ToolManager</code> instance. If no <code>ToolManager</code> is provided, it creates a default one.</li> <li>Tool Support Check: Determines if the underlying provider (obtained via <code>AIBase</code> initialization) supports tool calling based on its interface or attributes.</li> <li>Tool Management: Interacts with the <code>ToolManager</code> to get definitions of available tools.</li> <li>Tool Calling Loop (<code>process_prompt</code>): This is the primary method for handling requests that might involve tools.</li> <li>Adds the user prompt to history.</li> <li>Enters a loop (limited by <code>max_tool_iterations</code>).</li> <li>Gets available tool definitions from <code>ToolManager</code>.</li> <li>Calls the provider with the current messages and tool definitions.</li> <li>Receives the provider's response (which might include text content and/or tool call requests).</li> <li>Adds the assistant's message (content and requested tool calls) to history.</li> <li>If tool calls are requested: Executes each tool call using <code>ToolManager.execute_tool()</code>. Adds the tool results back into the conversation history using the provider-specific format.</li> <li>If no tool calls are requested: Exits the loop and returns the final text content from the assistant.</li> <li>Continues the loop if tools were called and the iteration limit is not reached.</li> <li>Basic Request (No Tool Loop):</li> <li><code>request_basic(prompt, **options)</code>: Sends the prompt and history to the provider but does not automatically execute any tool calls requested in the response. It returns the raw <code>ProviderResponse</code> object, allowing the caller to inspect potential tool calls.</li> <li>Tool Information: Provides <code>get_tool_history()</code> and <code>get_available_tools()</code>.</li> </ul> <p>For most standard interactions where you want the AI to be able to use tools automatically, you should instantiate and use <code>ToolEnabledAI</code> and call its <code>process_prompt</code> method.</p>"},{"location":"error_handling/","title":"Error Handling","text":"<p>The Agentic-AI framework uses a custom exception hierarchy to provide detailed error information. All framework-specific exceptions inherit from <code>AIFrameworkError</code>.</p>"},{"location":"error_handling/#base-exception","title":"Base Exception","text":"<ul> <li><code>AIFrameworkError</code>: The base class for all custom exceptions in this framework.</li> </ul>"},{"location":"error_handling/#configuration-errors","title":"Configuration Errors","text":"<ul> <li><code>AIConfigError</code>: Base class for configuration-related issues.</li> <li><code>AIConfigLoadingError</code>: Error during loading of configuration files.</li> <li><code>AIConfigValidationError</code>: Invalid configuration values found.</li> </ul>"},{"location":"error_handling/#provider-errors","title":"Provider Errors","text":"<ul> <li><code>AIProviderError</code>: Base class for errors originating from AI model providers.</li> <li><code>AIAuthenticationError</code>: Authentication issues with the provider (e.g., invalid API key).</li> <li><code>AIRateLimitError</code>: Rate limits exceeded for the provider API.</li> <li><code>AIServiceUnavailableError</code>: Provider's service is temporarily unavailable.</li> <li><code>AIProviderInvalidRequestError</code>: The request sent to the provider was malformed or invalid.</li> <li><code>AIProviderResponseError</code>: Error parsing or handling the provider's response.</li> </ul>"},{"location":"error_handling/#core-errors","title":"Core Errors","text":"<ul> <li><code>AISetupError</code>: Errors during the setup or initialization of core components (e.g., model selection failure).</li> <li><code>AIProcessingError</code>: General errors during AI request processing.</li> </ul>"},{"location":"error_handling/#tool-errors","title":"Tool Errors","text":"<ul> <li><code>AIToolError</code>: Base class for errors related to tool handling.</li> <li><code>AIToolNotFoundError</code>: Requested tool could not be found.</li> <li><code>AIToolExecutionError</code>: An error occurred during the execution of a tool function.</li> <li><code>AIToolTimeoutError</code>: Tool execution exceeded the configured timeout.</li> </ul>"},{"location":"error_handling/#conversation-errors","title":"Conversation Errors","text":"<ul> <li><code>AIConversationError</code>: Errors related to conversation management.</li> </ul>"},{"location":"error_handling/#usage","title":"Usage","text":"<p>When interacting with the framework, you can use <code>try...except</code> blocks to catch specific errors or the base <code>AIFrameworkError</code> for general handling.</p> <pre><code>from src.core import ToolEnabledAI\nfrom src.exceptions import AIProviderError, AIToolError, AIFrameworkError\n\ntry:\n    ai = ToolEnabledAI()\n    response = ai.process_prompt(\"Use the calculator tool to find 5+5\")\n    print(response)\nexcept AIProviderError as e:\n    print(f\"AI Provider Issue: {e}\")\nexcept AIToolError as e:\n    print(f\"Tool Execution Issue: {e}\")\nexcept AIFrameworkError as e:\n    print(f\"General Framework Error: {e}\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"examples/","title":"Usage Examples","text":"<p>This page presents practical examples of using Agentic-AI in different scenarios.</p>"},{"location":"examples/#basic-ai-interaction","title":"Basic AI Interaction","text":"<pre><code>from src.config.models import Model\nfrom src.config.config_manager import ConfigManager\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.utils.logger import LoggerFactory\n\n# Set up logger\nlogger = LoggerFactory.create()\n\n# Initialize ConfigManager\nconfig_manager = ConfigManager()\n\n# Create AI instance\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    config_manager=config_manager,\n    logger=logger\n)\n\n# Send a request\nresponse = ai.request(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"examples/#creating-a-weather-assistant","title":"Creating a Weather Assistant","text":"<pre><code>from src.config.models import Model\nfrom src.core.tool_enabled_ai import ToolEnabledAI\n\n# Create a weather assistant\nai = AI(\n    model=Model.GPT_4O,\n    system_prompt=\"You are a helpful weather assistant. Your goal is to provide weather information.\"\n)\n\n# Define weather tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather for a location (mocked for example)\"\"\"\n    # In a real application, this would call a weather API\n    weather_data = {\n        \"New York\": \"Sunny, 75\u00b0F\",\n        \"London\": \"Rainy, 62\u00b0F\",\n        \"Tokyo\": \"Partly cloudy, 80\u00b0F\",\n        \"Paris\": \"Clear skies, 70\u00b0F\"\n    }\n    return weather_data.get(location, f\"Weather data not available for {location}\")\n\n# Register weather tool\nai.register_tool(\n    tool_name=\"get_weather\",\n    tool_function=get_weather,\n    description=\"Get current weather for a specific location\",\n    parameters_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"City or location name\"\n            }\n        },\n        \"required\": [\"location\"]\n    }\n)\n\n# Use the weather assistant\nresponse = ai.request(\"What's the weather like in Tokyo today?\")\nprint(response)\n</code></pre>"},{"location":"examples/#ai-with-auto-tool-finding","title":"AI with Auto Tool Finding","text":"<p>This example demonstrates using <code>AIToolFinder</code> to automatically select relevant tools.</p> <pre><code>from src.config.models import Model\nfrom src.config.config_manager import ConfigManager\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.utils.logger import LoggerFactory\n\n# Set up\nlogger = LoggerFactory.create()\nconfig_manager = ConfigManager()\n\n# Create AI with auto tool finding\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    system_prompt=\"You are a helpful assistant. Use tools when appropriate to answer user queries.\",\n    config_manager=config_manager,\n    logger=logger,\n    auto_find_tools=True  # Enable auto tool finding\n)\n\n# Register multiple tools\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    return f\"It's sunny in {location} today!\"\n\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    try:\n        result = eval(expression)\n        return f\"The result of {expression} is {result}\"\n    except:\n        return \"Sorry, I couldn't evaluate that expression.\"\n\ndef get_ticket_price(destination: str) -&gt; str:\n    \"\"\"Get ticket price for a destination.\"\"\"\n    return f\"A ticket to {destination} costs $1000 USD\"\n\n# Register all tools\nai.register_tool(\"get_weather\", get_weather, \"Get weather for a location\")\nai.register_tool(\"calculate\", calculate, \"Perform calculations\")\nai.register_tool(\"get_ticket_price\", get_ticket_price, \"Get ticket price information\")\n\n# The AI will automatically select the appropriate tool\nresponse = ai.request(\"How much does a ticket to New York cost?\")\nprint(response)\n\n# Try another query\nresponse = ai.request(\"What's 125 * 37?\")\nprint(response)\n</code></pre>"},{"location":"examples/#using-prompt-templates","title":"Using Prompt Templates","text":"<p>This example shows how to use the prompt management system.</p> <pre><code>from src.config.models import Model\nfrom src.prompts import PromptManager\nfrom src.core.tool_enabled_ai import ToolEnabledAI\n\n# Initialize prompt manager\nprompt_manager = PromptManager(storage_dir=\"data/prompts\")\n\n# Create a template for customer support\ntemplate_id = prompt_manager.create_template(\n    name=\"Customer Support\",\n    description=\"Template for answering customer support questions\",\n    template=\"You are a customer support agent for {{company}}. Answer this customer question: {{question}}\",\n    default_values={\"company\": \"Acme Corp\"}\n)\n\n# Create AI with prompt manager\nai = AI(\n    model=Model.CLAUDE_3_7_SONNET,\n    prompt_manager=prompt_manager\n)\n\n# Use the template\nresponse = ai.request_with_template(\n    template_id=template_id,\n    variables={\n        \"question\": \"How do I reset my password?\",\n        \"company\": \"TechGiant Inc.\"\n    }\n)\nprint(response)\n\n# Create an alternative version for A/B testing\nprompt_manager.create_version(\n    template_id=template_id,\n    template_string=\"As a {{company}} support representative, please help with: {{question}}\",\n    name=\"Alternative Wording\",\n    description=\"Different wording to test effectiveness\"\n)\n\n# The A/B testing is handled automatically when user_id is provided\nresponse = ai.request_with_template(\n    template_id=template_id,\n    variables={\n        \"question\": \"How do I cancel my subscription?\",\n        \"company\": \"TechGiant Inc.\"\n    },\n    user_id=\"user-123\"  # This determines which version they get\n)\nprint(response)\n</code></pre>"},{"location":"examples/#using-the-tool-statistics-manager","title":"Using the Tool Statistics Manager","text":"<p>This example demonstrates how to track and analyze tool usage with the <code>ToolStatsManager</code>.</p> <pre><code>from src.tools.tool_stats_manager import ToolStatsManager\nfrom src.tools.tool_manager import ToolManager\n\n# Option 1: Using ToolStatsManager directly\nstats_manager = ToolStatsManager()\n\n# Record tool usage\nstats_manager.update_stats(\n    tool_name=\"search_tool\",\n    success=True,\n    duration_ms=250\n)\n\n# Retrieve statistics\nsearch_stats = stats_manager.get_stats(\"search_tool\")\nprint(f\"Search tool success rate: {search_stats['successes']/search_stats['uses']*100:.1f}%\")\n\n# Save statistics for persistence\nstats_manager.save_stats()\n\n# Option 2: Using through ToolManager (recommended approach)\ntool_manager = ToolManager()\n\n# Execute a tool - stats tracking happens automatically\nresult = tool_manager.execute_tool(\"weather_tool\", location=\"New York\")\n\n# Save statistics\ntool_manager.save_usage_stats()\n\n# Get tool statistics\ntool_info = tool_manager.get_tool_info(\"weather_tool\")\nif tool_info and 'usage_stats' in tool_info:\n    usage_stats = tool_info['usage_stats']\n    print(f\"Weather tool used {usage_stats['uses']} times with {usage_stats['successes']} successes\")\n</code></pre> <p>For more detailed examples, see Tool Statistics Example.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example to get started with Agentic-AI:</p> <pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config import configure, get_config\n\n# Configure the framework (optional - uses defaults otherwise)\n# Example: Use a specific model\n# configure(model=\"claude-3-haiku\")\n\n# Create AI instance (uses the configured model or default)\nai = ToolEnabledAI()\n\n# Make a simple request (uses the underlying provider)\n# Note: For tool usage, use process_prompt() instead of request()\nresponse = ai.request(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"getting-started/#using-tools","title":"Using Tools","text":"<p>Tools allow the AI to perform actions. They are typically defined in <code>src/config/tools.yml</code> and loaded automatically by the <code>ToolManager</code>.</p> <pre><code>from src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config import configure, UseCasePreset\n\n# Configure the framework, maybe selecting a model good at tool use\nconfigure(\n    model=\"claude-3-5-sonnet\",\n    use_case=UseCasePreset.CHAT # Or another relevant use case\n)\n\n# Create AI instance. It automatically gets a ToolManager\n# which loads tools defined in tools.yml.\nai = ToolEnabledAI()\n\n# The AI will use tools loaded by ToolManager when appropriate.\n# Use process_prompt() to enable the tool-calling loop.\nresponse = ai.process_prompt(\"What's the weather like in Tokyo today?\")\nprint(f\"Weather Response: {response}\")\n\nresponse = ai.process_prompt(\"What time is it now?\")\nprint(f\"Time Response: {response}\")\n</code></pre> <p>See the Tool Integration section for more details on defining and managing tools.</p>"},{"location":"getting-started/#using-agents-coordinator","title":"Using Agents (Coordinator)","text":"<p>For more complex interactions involving specific workflows or routing, you often use the <code>Coordinator</code> agent:</p> <pre><code>from src.agents import Coordinator\nfrom src.config import configure, UseCasePreset\n\n# Configure the framework (e.g., model, use case)\n# configure(model=\"claude-3-haiku\", use_case=UseCasePreset.CHAT)\n\n# Create a Coordinator instance (uses default dependencies &amp; config)\ncoordinator = Coordinator()\n\n# Prepare a request for the coordinator\nrequest_data = {\n    \"prompt\": \"Tell me a joke and then tell me the weather in London.\",\n}\n\n# Process the request through the coordinator\nresponse_coord = coordinator.process_request(request_data)\n\n# Print the final content from the coordinator's response\nprint(f\"Coordinator Response: {response_coord.get('content')}\")\n</code></pre> <p>Refer to the Agent System documentation for more on agents.</p>"},{"location":"metrics_system/","title":"Metrics Tracking System","text":"<p>This document provides an overview of the metrics tracking system implemented in the Agentic-AI framework. The system allows tracking and analysis of agent and tool usage, performance metrics, and request processing.</p>"},{"location":"metrics_system/#overview","title":"Overview","text":"<p>The metrics tracking system consists of the following components:</p> <ol> <li>RequestMetricsService: Core service for tracking and storing metrics about requests, agents, tools, and models.</li> <li>MetricsDashboard: Visualization and reporting tool for analyzing metrics data.</li> <li>Metrics CLI: Command-line interface for querying and visualizing metrics.</li> </ol> <p>The system tracks:</p> <ul> <li>Request processing details and duration</li> <li>Agent usage and performance</li> <li>Tool usage and performance</li> <li>Model usage and token counts</li> </ul>"},{"location":"metrics_system/#installation-and-setup","title":"Installation and Setup","text":"<p>The metrics system is built into the Agentic-AI framework and does not require separate installation. All metrics are automatically stored in:</p> <pre><code>data/metrics/request_metrics.json\n</code></pre> <p>This location can be customized by providing a different path when initializing the <code>RequestMetricsService</code>.</p>"},{"location":"metrics_system/#usage","title":"Usage","text":""},{"location":"metrics_system/#1-programmatic-usage","title":"1. Programmatic Usage","text":""},{"location":"metrics_system/#tracking-request-metrics","title":"Tracking Request Metrics","text":"<pre><code>from src.metrics.request_metrics import RequestMetricsService\n\n# Initialize the metrics service\nmetrics_service = RequestMetricsService()\n\n# Start tracking a request\nrequest_id = metrics_service.start_request_tracking(\n    prompt=\"User query here\",\n    metadata={\"user_id\": \"user123\"}\n)\n\n# Later, end tracking when request is complete\nmetrics_service.end_request_tracking(\n    request_id=request_id,\n    success=True  # or False if there was an error\n)\n</code></pre>"},{"location":"metrics_system/#tracking-agent-usage","title":"Tracking Agent Usage","text":"<pre><code># Track when an agent is used\nmetrics_service.track_agent_usage(\n    request_id=request_id,\n    agent_id=\"example_agent\",\n    confidence=0.85,\n    duration_ms=152,\n    success=True\n)\n</code></pre>"},{"location":"metrics_system/#tracking-tool-usage","title":"Tracking Tool Usage","text":"<pre><code># Track when a tool is used\nmetrics_service.track_tool_usage(\n    request_id=request_id,\n    tool_id=\"example_tool\",\n    duration_ms=75,\n    success=True\n)\n</code></pre>"},{"location":"metrics_system/#tracking-model-usage","title":"Tracking Model Usage","text":"<pre><code># Track when a model is used\nmetrics_service.track_model_usage(\n    request_id=request_id,\n    model_id=\"gpt-4\",\n    tokens_in=250,\n    tokens_out=50,\n    duration_ms=1200,\n    success=True\n)\n</code></pre>"},{"location":"metrics_system/#getting-metrics","title":"Getting Metrics","text":"<pre><code># Get agent metrics\nagent_metrics = metrics_service.get_agent_metrics(\n    agent_id=\"example_agent\",  # Optional, get all if not specified\n    start_time=datetime.now() - timedelta(days=30)\n)\n\n# Get tool metrics\ntool_metrics = metrics_service.get_tool_metrics(\n    tool_id=\"example_tool\",  # Optional, get all if not specified\n    start_time=datetime.now() - timedelta(days=30)\n)\n</code></pre>"},{"location":"metrics_system/#visualization","title":"Visualization","text":"<pre><code>from src.metrics.dashboard import MetricsDashboard\n\n# Initialize the dashboard\ndashboard = MetricsDashboard()\n\n# Generate plots\ndashboard.plot_agent_usage(top_n=10, days=30)\ndashboard.plot_tool_usage(top_n=10, days=30)\n\n# Generate a performance report\nreport = dashboard.generate_performance_report(days=30)\n</code></pre>"},{"location":"metrics_system/#2-command-line-interface","title":"2. Command-Line Interface","text":"<p>The metrics system includes a command-line interface for easy analysis of metrics data.</p>"},{"location":"metrics_system/#basic-usage","title":"Basic Usage","text":"<pre><code># Show summary of metrics\npython src/metrics/metrics_cli.py summary\n\n# Show agent metrics\npython src/metrics/metrics_cli.py agents\n\n# Show metrics for a specific agent\npython src/metrics/metrics_cli.py agents --agent=\"orchestrator\"\n\n# Show tool metrics\npython src/metrics/metrics_cli.py tools\n\n# Show details for a specific request\npython src/metrics/metrics_cli.py request REQUEST_ID\n\n# Generate a performance report\npython src/metrics/metrics_cli.py report\n</code></pre>"},{"location":"metrics_system/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Generate and save a plot of agent metrics\npython src/metrics/metrics_cli.py agents --plot --output=agent_metrics.png\n\n# Generate and save a plot of tool metrics\npython src/metrics/metrics_cli.py tools --plot --output=tool_metrics.png\n\n# Generate a JSON performance report\npython src/metrics/metrics_cli.py report --output=performance_report.json\n\n# Show metrics for the last 7 days\npython src/metrics/metrics_cli.py summary --days=7\n</code></pre>"},{"location":"metrics_system/#integration-with-orchestrator","title":"Integration with Orchestrator","text":"<p>The metrics system is fully integrated with the Orchestrator agent. When the Orchestrator processes a request, it:</p> <ol> <li>Automatically generates a request ID if not provided</li> <li>Tracks the start and end of each request</li> <li>Tracks which agents and tools are used</li> <li>Records model usage and selection</li> <li>Tracks success or failure of each component</li> <li>Includes the request ID and metrics metadata in the response</li> </ol>"},{"location":"metrics_system/#integration-with-tool-registry","title":"Integration with Tool Registry","text":"<p>The metrics system is also integrated with the Tool Registry. When a tool is executed:</p> <ol> <li>The execution time is tracked</li> <li>The success or failure is recorded</li> <li>The tool usage is associated with the current request ID</li> </ol>"},{"location":"metrics_system/#tool-specific-statistics-with-toolstatsmanager","title":"Tool-Specific Statistics with ToolStatsManager","text":"<p>The framework includes a dedicated <code>ToolStatsManager</code> component that provides detailed per-tool usage statistics, complementing the broader metrics system:</p>"},{"location":"metrics_system/#key-differences-from-requestmetricsservice","title":"Key Differences from RequestMetricsService","text":"<ul> <li>Tool-Focused: Specializes in tool usage statistics rather than the entire system</li> <li>Performance Metrics: Tracks detailed performance metrics like average execution time</li> <li>Stateless Operation: Works independently of request tracking (no request IDs needed)</li> <li>Simplicity: Lightweight design for simple tool usage tracking</li> <li>Durability: Persistent JSON storage with automatic save/load operations</li> </ul>"},{"location":"metrics_system/#example-usage","title":"Example Usage","text":"<pre><code>from src.tools.tool_stats_manager import ToolStatsManager\n\n# Get the ToolStatsManager instance\nstats_manager = ToolStatsManager()\n\n# Record a tool usage\nstats_manager.update_stats(\n    tool_name=\"weather_tool\",\n    success=True,\n    duration_ms=250\n)\n\n# Get statistics for analysis\nweather_stats = stats_manager.get_stats(\"weather_tool\")\nprint(f\"Success rate: {weather_stats['successes']/weather_stats['uses']*100:.1f}%\")\nprint(f\"Average duration: {weather_stats['avg_duration_ms']:.2f}ms\")\n</code></pre> <p>For more detailed information about ToolStatsManager, see the Tool Usage Statistics section in the Tools documentation.</p>"},{"location":"metrics_system/#extending-the-metrics-system","title":"Extending the Metrics System","text":""},{"location":"metrics_system/#adding-new-metrics","title":"Adding New Metrics","text":"<p>To add new metrics to track:</p> <ol> <li>Add new fields to the appropriate dictionaries in <code>RequestMetricsService</code></li> <li>Add methods to update and retrieve the new metrics</li> <li>Update visualization methods in <code>MetricsDashboard</code> if needed</li> </ol>"},{"location":"metrics_system/#integrating-with-custom-agents","title":"Integrating with Custom Agents","text":"<p>To integrate the metrics system with your custom agent:</p> <pre><code>from src.metrics.request_metrics import RequestMetricsService\n\nclass MyCustomAgent:\n    def process_request(self, request):\n        # Extract request_id if it exists\n        request_id = request.get(\"request_id\")\n\n        # Initialize metrics service\n        metrics_service = RequestMetricsService()\n\n        # Record start time\n        start_time = time.time()\n\n        try:\n            # Process the request\n            result = self._do_processing(request)\n\n            # Track usage\n            metrics_service.track_agent_usage(\n                request_id=request_id,\n                agent_id=self.agent_id,\n                duration_ms=int((time.time() - start_time) * 1000),\n                success=True\n            )\n\n            return result\n        except Exception as e:\n            # Track failure\n            metrics_service.track_agent_usage(\n                request_id=request_id,\n                agent_id=self.agent_id,\n                duration_ms=int((time.time() - start_time) * 1000),\n                success=False,\n                metadata={\"error\": str(e)}\n            )\n            raise\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Missing metrics data**: Ensure the data directory exists and is writable.\n2. **Request ID not tracked**: Make sure the request_id is passed correctly between components.\n3. **CLI import errors**: Run the CLI script from the project root directory.\n\n### Debugging\n\nTo debug metrics tracking, you can:\n\n1. Inspect the metrics JSON file directly: `data/metrics/request_metrics.json`\n2. Add detailed metadata to track more information\n3. Use the CLI's request command to inspect specific requests: `python src/metrics/metrics_cli.py request REQUEST_ID`\n\n## Best Practices\n\n1. Always include request_id when passing requests between components\n2. Use consistent agent_id and tool_id values for accurate tracking\n3. Track both successes and failures for complete analytics\n4. Include relevant metadata for easier debugging and analysis\n5. Regularly review metrics to identify performance issues and opportunities for optimization\n</code></pre> <pre><code>\n</code></pre>"},{"location":"utils/","title":"Utilities","text":"<p>This section covers utility components used throughout the Agentic AI framework.</p>"},{"location":"utils/#logging-srcutilsloggerpy","title":"Logging (<code>src/utils/logger.py</code>)","text":"<p>The framework utilizes a flexible logging system based on a <code>LoggerFactory</code>.</p> <ul> <li><code>LoggerFactory</code>: A factory class used to create logger instances.</li> <li>Purpose: Provides a consistent way to obtain configured loggers across different modules.</li> <li>Usage:</li> </ul> <pre><code>from src.utils.logger import LoggerFactory\n\n# Get a logger instance (typically within a class __init__)\nlogger = LoggerFactory.create(name=\"my_module_or_class_name\")\n\n# Use the logger\nlogger.info(\"This is an informational message.\")\nlogger.debug(\"This is a debug message.\")\nlogger.warning(\"This is a warning.\")\nlogger.error(\"This is an error message.\")\n</code></pre> <ul> <li>Configuration: Logging levels and output handlers (e.g., console, file) are typically configured globally, potentially influenced by environment variables or application setup.</li> </ul>"},{"location":"conversations/overview/","title":"Conversation Management","text":""},{"location":"conversations/overview/#overview","title":"Overview","text":"<p>The conversation management system in Agentic-AI handles:</p> <ul> <li>Maintaining conversation history</li> <li>Formatting messages for different providers</li> <li>Extracting \"thoughts\" and other metadata from responses</li> <li>Managing context and tool calls</li> </ul>"},{"location":"conversations/overview/#conversationmanager","title":"ConversationManager","text":"<p>The <code>ConversationManager</code> class tracks the conversation state:</p> <pre><code>from src.conversation.conversation_manager import ConversationManager\n\n# Create a conversation manager\nconversation = ConversationManager()\n\n# Add messages\nconversation.add_message(role=\"user\", content=\"Hello, how are you?\")\nconversation.add_message(role=\"assistant\", content=\"I'm doing well! How can I help you today?\")\n\n# Get all messages\nmessages = conversation.get_messages()\n\n# Get the latest message\nlast_message = conversation.get_last_message()\n\n# Clear the conversation\nconversation.clear()\n</code></pre>"},{"location":"conversations/overview/#working-with-thoughts","title":"Working with Thoughts","text":"<p>The system can extract AI \"thoughts\" (typically enclosed in <code>&lt;thinking&gt;</code> tags) from responses to aid in debugging the AI's reasoning process. This extraction happens when adding a message if the <code>extract_thoughts</code> flag is set.</p> <pre><code># Assume 'conversation' is an initialized ConversationManager instance\nraw_response_content = \"&lt;thinking&gt;Let me consider the best approach here...&lt;/thinking&gt;The answer is 42.\"\n\n# Add the assistant message, enabling thought extraction\nconversation.add_message(\n    role=\"assistant\",\n    content=raw_response_content,\n    extract_thoughts=True\n)\n\n# Get the last message added\nlast_message = conversation.get_last_message()\n\n# The 'content' key contains the response without the thinking tags\nprint(f\"Processed Content: {last_message['content']}\")\n# Output: Processed Content: The answer is 42.\n\n# The extracted thoughts are stored in the 'thoughts' key within the message dictionary\nif 'thoughts' in last_message:\n    print(f\"Extracted Thoughts: {last_message['thoughts']}\")\n    # Output: Extracted Thoughts: Let me consider the best approach here...\n# Note: If multiple thinking blocks exist, they might be concatenated or handled based on ResponseParser logic.\n\n# There is no separate get_thoughts() method on ConversationManager itself.\n# Thoughts are accessed directly from the message dictionary after extraction.\n</code></pre>"},{"location":"conversations/overview/#handling-tool-calls","title":"Handling Tool Calls","text":"<p>Conversation manager also tracks tool calls and their results:</p> <pre><code># Add a message with tool calls\nconversation.add_message(\n    role=\"assistant\",\n    content=\"I'll check the weather for you.\",\n    tool_calls=[\n        {\n            \"name\": \"get_weather\",\n            \"arguments\": {\"location\": \"New York\"}\n        }\n    ]\n)\n\n# Add the tool response\nconversation.add_message(\n    role=\"tool\",\n    name=\"get_weather\",\n    content=\"It's sunny and 75\u00b0F in New York.\"\n)\n</code></pre>"},{"location":"conversations/overview/#response-parser","title":"Response Parser","text":"<p>The <code>ResponseParser</code> processes raw AI responses:</p> <pre><code>from src.conversation.response_parser import ResponseParser\n\nparser = ResponseParser()\n\n# Parse a response with thoughts\nresult = parser.parse_response(\n    \"&lt;thinking&gt;I should search for this information.&lt;/thinking&gt;The capital of France is Paris.\",\n    extract_thoughts=True,\n    show_thinking=False  # Hide thoughts in final output\n)\n\n# Result: {\n#   \"content\": \"The capital of France is Paris.\",\n#   \"thoughts\": \"I should search for this information.\"\n# }\n</code></pre>"},{"location":"conversations/overview/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>User     AI      ConversationManager    ResponseParser    Provider\n |       |              |                     |               |\n | Request               |                     |               |\n |------&gt;|              |                     |               |\n |       | Add user message                   |               |\n |       |-------------&gt;|                     |               |\n |       |              |                     |               |\n |       | Get messages  |                     |               |\n |       |&lt;-------------|                     |               |\n |       |              |                     |               |\n |       | Send to provider                                   |\n |       |--------------------------------------------------&gt;|\n |       |              |                     |               |\n |       | Raw response                                       |\n |       |&lt;--------------------------------------------------|\n |       |              |                     |               |\n |       | Parse response                     |               |\n |       |-----------------------------&gt;|     |               |\n |       |              |                     |               |\n |       | Parsed response                    |               |\n |       |&lt;-----------------------------|     |               |\n |       |              |                     |               |\n |       | Add assistant message              |               |\n |       |-------------&gt;|                     |               |\n |       |              |                     |               |\n | Response             |                     |               |\n |&lt;------|              |                     |               |\n</code></pre>"},{"location":"development/testing_providers/","title":"Provider Test Cases","text":"<p>This document outlines the standard test cases that should be implemented for each AI Provider implementation (e.g., OpenAI, Anthropic, Gemini, Ollama) to ensure consistent functionality, error handling, and adherence to the framework's interfaces.</p> <p>Testing Philosophy:</p> <ul> <li>Focus on unit testing the provider's specific logic (request formatting, response parsing, error mapping).</li> <li>Mock the external SDK client to avoid actual API calls and dependencies during tests.</li> <li>Use <code>pytest</code> conventions and fixtures for setup and parameterization.</li> <li>Keep mocks minimal and focused on the interaction boundary with the SDK.</li> </ul>"},{"location":"development/testing_providers/#general-test-categories-for-all-providers","title":"General Test Categories for All Providers","text":"<p>The following categories should be covered for each provider. Specific details will vary based on the provider's features and SDK.</p>"},{"location":"development/testing_providers/#1-initialization-__init__","title":"1. Initialization (<code>__init__</code>)","text":"<ul> <li> <p>Test Case 1.1: Successful Initialization (with API Key)</p> </li> <li> <p>Goal: Verify the provider initializes correctly when valid configuration (including API key) is provided.</p> </li> <li> <p>Checks:</p> <ul> <li>Provider attributes (<code>model_id</code>, <code>provider_config</code>, <code>model_config</code>) are set correctly.</li> <li>Internal parameters (<code>self.parameters</code> like temperature, max_tokens) are parsed correctly from config.</li> <li>The underlying SDK client is instantiated correctly (mocked) with the API key.</li> <li>Logger is initialized.</li> </ul> </li> <li> <p>Test Case 1.2: Initialization Fails (Missing API Key)</p> </li> <li> <p>Goal: Verify <code>AICredentialsError</code> (or appropriate error) is raised if the API key is missing in the configuration or cannot be resolved.</p> </li> <li> <p>Checks:</p> <ul> <li>Correct exception type is raised.</li> <li>Exception message is informative.</li> <li>SDK client is not instantiated.</li> </ul> </li> <li> <p>Test Case 1.3: Initialization Fails (Missing Configuration)</p> </li> <li>Goal: Verify <code>AISetupError</code> or <code>AIConfigError</code> is raised if essential provider or model configuration is missing.</li> <li>Checks:<ul> <li>Correct exception type is raised.</li> <li>Exception message indicates the missing configuration.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#2-prepare-request-payload-_prepare_request_payload","title":"2. Prepare Request Payload (<code>_prepare_request_payload</code>)","text":"<p>(Note: This method might be implicitly tested via <code>_make_api_request</code> or <code>request</code> tests, but explicit tests can be useful if the logic is complex)</p> <ul> <li> <p>Test Case 2.1: Basic Payload Formatting</p> </li> <li> <p>Goal: Verify the provider correctly formats the input messages and base parameters into the structure expected by the SDK's API call.</p> </li> <li> <p>Checks:</p> <ul> <li><code>model</code> ID is included correctly.</li> <li><code>messages</code> are formatted according to the provider's requirements (roles, content structure).</li> <li>Base parameters (<code>temperature</code>, <code>max_tokens</code>, etc.) from <code>self.parameters</code> are included.</li> </ul> </li> <li> <p>Test Case 2.2: Payload Formatting with Options Override</p> </li> <li>Goal: Verify that parameters passed via <code>**options</code> in the <code>request</code> method override the default <code>self.parameters</code>.</li> <li>Checks:<ul> <li>Payload includes the overridden values for parameters like <code>temperature</code>.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#3-make-api-request-_make_api_request","title":"3. Make API Request (<code>_make_api_request</code>)","text":"<ul> <li> <p>Test Case 3.1: Successful API Call</p> </li> <li> <p>Goal: Verify the method correctly calls the mocked SDK client's relevant function with the prepared payload and returns the raw SDK response.</p> </li> <li> <p>Checks:</p> <ul> <li>Mock SDK client's method (e.g., <code>chat.completions.create</code>, <code>messages.create</code>) is called once.</li> <li>The call is made with the expected payload dictionary.</li> <li>The raw mock response object from the SDK client is returned.</li> </ul> </li> <li> <p>Test Case 3.2: SDK Error Mapping</p> </li> <li>Goal: Verify that various errors raised by the (mocked) SDK are caught and mapped to the framework's specific exceptions (<code>AIAuthenticationError</code>, <code>AIRateLimitError</code>, <code>InvalidRequestError</code>, <code>ModelNotFoundError</code>, <code>ContentModerationError</code>, <code>AIProviderError</code>).</li> <li>Setup: Use <code>pytest.mark.parametrize</code> to test different SDK error types.</li> <li>Checks:<ul> <li>For each simulated SDK error, the correct custom framework exception is raised.</li> <li>The exception message is informative.</li> <li>Relevant details (like status code or error code) are potentially preserved in the custom exception.</li> <li>Logger is called appropriately upon error.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#4-convert-response-_convert_response","title":"4. Convert Response (<code>_convert_response</code>)","text":"<ul> <li> <p>Test Case 4.1: Convert Response (Text Only)</p> </li> <li> <p>Goal: Verify the provider correctly parses a simple text response from the (mocked) SDK response object into the standardized <code>ProviderResponse</code> model.</p> </li> <li> <p>Checks:</p> <ul> <li><code>ProviderResponse.content</code> contains the correct text.</li> <li><code>ProviderResponse.tool_calls</code> is <code>None</code> or empty.</li> <li><code>ProviderResponse.stop_reason</code> is mapped correctly.</li> <li><code>ProviderResponse.usage</code> (tokens) is extracted correctly.</li> <li><code>ProviderResponse.model</code> ID is extracted correctly.</li> <li><code>ProviderResponse.error</code> is <code>None</code>.</li> </ul> </li> <li> <p>Test Case 4.2: Convert Response (Content Moderation / Stop Reason)</p> </li> <li>Goal: Verify specific stop reasons (like content filters) are correctly identified and mapped in the <code>ProviderResponse</code>.</li> <li>Checks:<ul> <li><code>ProviderResponse.stop_reason</code> reflects the moderation or specific stop condition.</li> <li><code>ProviderResponse.content</code> might be empty or contain a specific marker.</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#5-tool-handling-if-provider-supports-tools","title":"5. Tool Handling (If Provider Supports Tools)","text":"<ul> <li> <p>Test Case 5.1: Prepare Payload with Tools</p> </li> <li> <p>Goal: Verify <code>_prepare_request_payload</code> (or equivalent logic) correctly formats the <code>tools</code> and <code>tool_choice</code> parameters according to the provider's specification when tools are provided.</p> </li> <li>Setup: Provide a list of <code>ToolDefinition</code> mocks.</li> <li> <p>Checks:</p> <ul> <li>The <code>tools</code> parameter in the payload matches the SDK's expected format.</li> <li><code>tool_choice</code> is included if specified.</li> </ul> </li> <li> <p>Test Case 5.2: Convert Response with Tool Calls</p> </li> <li> <p>Goal: Verify <code>_convert_response</code> correctly parses tool call requests from the SDK response into a list of <code>ToolCall</code> objects within the <code>ProviderResponse</code>.</p> </li> <li>Setup: Mock an SDK response indicating tool calls.</li> <li> <p>Checks:</p> <ul> <li><code>ProviderResponse.tool_calls</code> is a list of <code>ToolCall</code> objects.</li> <li>Each <code>ToolCall</code> has the correct <code>id</code>, <code>name</code>, and parsed <code>arguments</code> (as a dictionary).</li> <li><code>ProviderResponse.content</code> may or may not be present, depending on the SDK response.</li> <li><code>ProviderResponse.stop_reason</code> indicates tool use (if applicable).</li> </ul> </li> <li> <p>Test Case 5.3: Convert Response with Invalid Tool Arguments</p> </li> <li> <p>Goal: Verify how <code>_convert_response</code> handles cases where the SDK returns tool call arguments that are not valid JSON (if applicable to the provider).</p> </li> <li>Setup: Mock an SDK response with a tool call where <code>arguments</code> is an invalid JSON string.</li> <li> <p>Checks:</p> <ul> <li><code>ProviderResponse.tool_calls</code> contains a <code>ToolCall</code> object.</li> <li>The <code>arguments</code> field in the <code>ToolCall</code> should ideally contain the raw, unparsed string or a representation indicating the parsing failure (e.g., <code>{\"_raw_args\": \"...\"}</code>). It should not raise an unhandled exception.</li> </ul> </li> <li> <p>Test Case 5.4: Add Tool Message Formatting (<code>_add_tool_message</code>)</p> </li> <li>Goal: Verify the provider correctly formats a <code>ToolResult</code> into the message structure expected by the provider to be sent back in the next turn.</li> <li>Setup: Provide a <code>tool_call_id</code>, <code>tool_name</code>, and <code>content</code> (result string).</li> <li>Checks:<ul> <li>The returned message list/dictionary matches the provider's required format for tool result messages (e.g., role='tool', specific keys for ID/name/content).</li> </ul> </li> </ul>"},{"location":"development/testing_providers/#next-steps","title":"Next Steps","text":"<p>With these general cases defined, the next step is to write the specific <code>pytest</code> files for each provider, starting with one (e.g., <code>OpenAIProvider</code>), implementing these tests using mocks for the <code>openai</code> SDK.</p>"},{"location":"development/testing_strategy/","title":"Comprehensive Testing Strategy","text":"<p>This document outlines the general testing strategy for the Agentic-AI framework, covering unit, integration, and potentially end-to-end testing approaches. It serves as the primary guide for writing tests.</p> <p>Note:</p> <ul> <li>Provider-specific testing details are found in <code>docs/development/testing_providers.md</code>.</li> <li>For other complex modules (e.g., specific agents, core AI classes), detailed testing guidance may be included as a \"Testing\" subsection within their primary documentation page.</li> </ul>"},{"location":"development/testing_strategy/#i-overarching-principles","title":"I. Overarching Principles","text":"<ol> <li>Testing Pyramid: Adhere to the testing pyramid principle:<ul> <li>Base: Comprehensive Unit Tests for individual components. Focus on testing logic in isolation.</li> <li>Middle: Focused Integration Tests for interactions between key components. Verify contracts and collaborations.</li> <li>Top: Minimal End-to-End (E2E) Tests for critical user flows (if applicable, especially involving the UI). Ensure the system works from user input to final output.</li> </ul> </li> <li>Test Granularity: Write tests at the appropriate level. Unit tests should mock dependencies outside the unit (e.g., external APIs, filesystem, other classes), while integration tests should involve real instances of collaborating components (mocking only external systems like actual LLM APIs or databases not under test).</li> <li>Framework: Use <code>pytest</code> as the primary testing framework. Leverage fixtures (<code>tests/conftest.py</code>) for reusable setup (mocks, configurations, test data, temporary files/directories).</li> <li>Mocking: Use <code>unittest.mock</code> (<code>Mock</code>, <code>MagicMock</code>, <code>patch</code>) for mocking dependencies in unit tests. Keep mocks focused on the interaction boundary \u2013 mock what a dependency returns or that it was called correctly, not its internal implementation.</li> <li>Coverage: Aim for high unit test coverage on core logic, complex algorithms, and critical components. Use coverage reports (<code>pytest-cov</code>) as a guide but prioritize testing essential behaviors, edge cases, and error conditions over blindly chasing 100% line coverage.</li> <li>Location: Maintain a clear and consistent test structure mirroring the <code>src/</code> directory:<ul> <li><code>tests/unit/&lt;component&gt;/</code>: For unit tests corresponding to <code>src/&lt;component&gt;/</code>.</li> <li><code>tests/integration/&lt;component_or_workflow&gt;/</code>: For integration tests focusing on interactions between specific components or end-to-end workflows within the backend.</li> <li><code>tests/e2e/</code>: For true end-to-end tests involving external interfaces like a UI or API gateway (if applicable).</li> </ul> </li> <li>Verify, don't assume: When creating tests for any class, verify the constructor's arguments, class methods, and their arguments. Don't assume you know them, as the consumer class may overlook changes made in the class it uses after some refactoring.</li> </ol>"},{"location":"development/testing_strategy/#ii-component-specific-strategies","title":"II. Component-Specific Strategies","text":"<p>The following outlines the recommended testing approach for each major component area found in <code>src/</code>.</p> <ol> <li><code>config/</code> (<code>UnifiedConfig</code>, Loaders, etc.):<ul> <li>Unit Tests: High priority. Test loading from default paths, specified paths, environment variables. Verify correct merging logic (e.g., file overrides defaults, user overrides base). Test access methods (<code>get_provider_config</code>, <code>get_model_config</code>, <code>get_api_key</code>, etc.). Test error handling for missing files, malformed content (e.g., invalid YAML/JSON), and missing required keys. Mock filesystem operations (<code>pathlib.Path</code>, <code>open</code>, <code>os.environ</code>) extensively using fixtures like <code>tmp_path</code>.</li> </ul> </li> <li><code>core/</code>:<ul> <li><code>providers/</code>: Follow the detailed strategy in <code>docs/development/testing_providers.md</code>. Focus on unit tests mocking the specific provider SDKs or HTTP clients (<code>openai</code>, <code>anthropic</code>, <code>google.generativeai</code>, <code>ollama</code>, <code>requests</code>).</li> <li><code>interfaces/</code>: No explicit tests needed (abstract definitions).</li> <li><code>models/</code>: Unit tests for any custom validation logic, methods, or complex default factories within Pydantic/dataclass models. Basic validation is implicitly tested during usage in other components.</li> <li><code>provider_factory.py</code>: Unit tests covering provider registration (including duplicates/errors), creation logic (correct class instantiation, parameter passing from config), retrieval of registered providers, and error handling for unknown providers or configuration issues. Mock <code>UnifiedConfig</code>.</li> <li><code>base_ai.py</code> / <code>tool_enabled_ai.py</code> (Main AI Classes):</li> <li>Unit Tests: Test core logic like prompt assembly (if complex), basic provider interaction (using a mock <code>ProviderInterface</code>), handling simple provider responses (<code>ProviderResponse</code>), parsing tool calls, invoking the tool manager, and formatting tool results for the provider. Mock <code>ProviderFactory</code> or <code>ProviderInterface</code>, <code>ToolManager</code>, <code>ConversationManager</code>.</li> <li>Integration Tests: Test interaction with a real <code>ProviderFactory</code> (using mock providers). Test the full request-&gt;tool_call-&gt;tool_result-&gt;request loop using mock providers that simulate tool calls and responses, interacting with a real <code>ConversationManager</code> and <code>ToolManager</code> (using mock tools/executor). Test error handling during the loop (provider errors, tool errors).</li> <li><code>model_selector.py</code>: Unit tests covering model selection logic based on use case, quality, speed, cost constraints. Test filtering, cost calculation, best model selection, and enum mapping. Mock <code>UnifiedConfig</code> to provide controlled model/use-case data.</li> </ul> </li> <li><code>tools/</code>:<ul> <li><code>tool_registry.py</code>: Unit tests for registering tools (checking for duplicates, validation), formatting tools for different provider types (OpenAI, Anthropic, etc.), retrieving tool definitions.</li> <li><code>tool_executor.py</code>: Unit tests for executing tool functions successfully, handling execution errors (capturing exceptions in <code>ToolResult</code>), enforcing timeouts (patching <code>signal</code> or using appropriate async constructs if refactored), implementing retries. Mock the actual tool functions being executed.</li> <li><code>tool_manager.py</code>: Unit tests for coordinating registration (via <code>ToolRegistry</code>), execution (via <code>ToolExecutor</code>), and potentially formatting. Mock <code>ToolRegistry</code> and <code>ToolExecutor</code>.</li> <li>Specific Tools (functions/classes defined): Unit test the logic of each individual tool function/class itself, mocking any external dependencies (APIs, libraries) they might use.</li> </ul> </li> <li><code>agents/</code>:<ul> <li><code>base_agent.py</code>: Unit tests for any common setup, helper methods, or abstract logic defined in the base class.</li> <li><code>agent_registry.py</code>: Unit tests covering class registration (mocking <code>issubclass</code>), handling duplicates (overwriting), retrieving classes, and handling invalid types. Mock internal <code>_register_agents</code> call during init for isolation.</li> <li><code>agent_factory.py</code>: Unit tests covering agent class registration, creation logic based on type and configuration, dependency injection (if applicable), retrieval of registered types, and error handling for unknown types. Mock <code>AgentRegistry</code> and the agent classes being instantiated.</li> <li><code>agent_registrar.py</code>: Unit tests verifying that the correct agent classes are registered with the provided registry mock.</li> <li>Specific Agents (<code>Coordinator</code>, <code>RequestAnalyzer</code>, etc.):</li> <li>Unit Tests: Test the agent's specific logic, decision-making processes, state management, and interaction with its direct dependencies. Mock dependencies like other agents, <code>ToolEnabledAI</code> (or its interface), <code>ToolManager</code>, <code>PromptTemplate</code>, etc. Test different input scenarios and expected outputs or state changes.</li> <li>Integration Tests: Test interactions between collaborating agents (e.g., <code>Coordinator</code> -&gt; <code>RequestAnalyzer</code> -&gt; <code>SpecificTaskAgent</code>). Test agents interacting with a real (but potentially configured with mock providers) <code>ToolEnabledAI</code> instance to verify the flow of data and control.</li> </ul> </li> <li><code>prompts/</code>:<ul> <li>Unit Tests: Test template loading (from file or string), rendering with various valid/invalid inputs (including missing variables), handling different template formats if supported, and potential error conditions during rendering.</li> </ul> </li> <li><code>conversation/</code>:<ul> <li>Unit Tests: Test conversation history management: adding user/assistant/tool messages, retrieving history (full or truncated), enforcing length limits (token count or message count), formatting history for different provider needs, serialization/deserialization if applicable.</li> </ul> </li> <li><code>utils/</code>:<ul> <li>Unit Tests: Test each utility function or class independently. Ensure pure functions are tested with various inputs and edge cases. Mock external dependencies for utilities that interact with I/O, network, etc. (e.g., file system wrappers, HTTP clients, logger backends).</li> </ul> </li> <li><code>metrics/</code>:<ul> <li>Unit Tests: Test metric collection logic (incrementing counters, recording timings), aggregation methods, and formatting/reporting logic. Mock the underlying storage or reporting mechanism (e.g., logging, database, external monitoring service).</li> </ul> </li> <li><code>exceptions.py</code>: No explicit tests needed (definitions only). Their correct usage and propagation are tested in other components.</li> <li><code>ui/</code>: (If a UI component exists)<ul> <li>Unit Tests: For any backend API handlers, data processing logic, or state management associated specifically with the UI. Mock interactions with other backend components (<code>agents</code>, <code>core</code>, etc.).</li> <li>E2E Tests: Use browser automation tools (e.g., Playwright, Selenium) to simulate user interactions and verify end-to-end flows through the UI and backend. Focus on critical paths.</li> </ul> </li> </ol>"},{"location":"development/testing_strategy/#iii-integration-test-priorities","title":"III. Integration Test Priorities","text":"<p>Start with integration tests that cover fundamental interactions:</p> <ol> <li>Config loading -&gt; Provider Factory -&gt; Provider Creation (using mock SDKs).</li> <li>Core AI (<code>ToolEnabledAI</code>) -&gt; Provider Interaction (using mock <code>ProviderInterface</code> simulating success, errors, tool calls).</li> <li>Core AI (<code>ToolEnabledAI</code>) -&gt; Tool Manager -&gt; Tool Executor (using mock tool functions).</li> <li>Full Tool Loop: Core AI -&gt; Mock Provider (returns tool call) -&gt; Tool Manager -&gt; Mock Tool -&gt; Core AI -&gt; Mock Provider (processes tool result).</li> <li>Agent (<code>Coordinator</code>) -&gt; Core AI (<code>ToolEnabledAI</code>) interaction (basic request/response).</li> <li>Agent (<code>Coordinator</code>) -&gt; Core AI (<code>ToolEnabledAI</code>) -&gt; Tool loop execution.</li> </ol>"},{"location":"development/testing_strategy/#iv-test-execution-and-ci","title":"IV. Test Execution and CI","text":"<ul> <li>Tests should be easily runnable locally via <code>pytest</code>.</li> <li>Integrate test execution into the Continuous Integration (CI) pipeline (e.g., GitHub Actions).</li> <li>Run linters (e.g., Ruff, MyPy) and formatters (e.g., Black, isort) alongside tests in CI.</li> <li>Consider running tests automatically on pull requests.</li> <li>Track test coverage and identify significant gaps in critical areas.</li> </ul>"},{"location":"development/testing_strategy/#v-prioritization","title":"V. Prioritization","text":"<ol> <li>Unit Tests - Foundational: <code>config</code>, <code>core/providers</code> (as per <code>testing_providers.md</code>), <code>tools</code> (core classes), <code>prompts</code>, <code>conversation</code>, <code>utils</code>.</li> <li>Unit Tests - Core Logic: <code>core/ai_core</code>, <code>agents</code> (base and specific).</li> <li>Integration Tests: Start with the priority list in Section III.</li> <li>Unit Tests - Others: <code>metrics</code>, <code>core/models</code> (if complex).</li> <li>E2E Tests: If UI exists.</li> </ol>"},{"location":"examples/configuration_example/","title":"Configuration Usage Examples","text":"<p>This document provides examples of how to use the new configuration system in various scenarios.</p>"},{"location":"examples/configuration_example/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>import sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.unified_config import UnifiedConfig\n\n# Get the configuration instance\nconfig = UnifiedConfig.get_instance()\n\n# Create an AI instance using the default configuration\nai = ToolEnabledAI()\nresponse = ai.request(\"Hello, who are you?\")\nprint(f\"Default AI Response: {response}\")\n</code></pre>"},{"location":"examples/configuration_example/#example-2-using-configuration-file","title":"Example 2: Using Configuration File","text":"<p>Create a <code>user_config.yml</code> file:</p> <pre><code># user_config.yml\nmodel: gpt-4o-mini\ntemperature: 0.8\nshow_thinking: true\n</code></pre> <p>Then load it:</p> <pre><code>import sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.unified_config import UnifiedConfig\nfrom src.config.user_config import UserConfig\n\n# Load user configuration from file\nuser_cfg = UserConfig(config_file=\"user_config.yml\")\nconfig = UnifiedConfig.get_instance(user_config=user_cfg)\n\n# Create an AI instance with the configured model\nai = ToolEnabledAI()\nresponse = ai.request(\"Explain the concept of recursion\")\nprint(f\"Configured AI Response: {response}\")\n</code></pre>"},{"location":"examples/configuration_example/#example-3-selecting-model-via-use-case","title":"Example 3: Selecting Model via Use Case","text":"<p>Assuming <code>use_cases.yml</code> defines a <code>solidity_coding</code> use case and potentially associates preferred models.</p> <pre><code>import sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom src.core.tool_enabled_ai import ToolEnabledAI\nfrom src.config.unified_config import UnifiedConfig\nfrom src.core.model_selector import ModelSelector, UseCase\n\nconfig = UnifiedConfig.get_instance()\nselector = ModelSelector(config=config)\n\n# Select model based on use case\nselected_model = selector.select_model(UseCase.SOLIDITY_CODING)\n\n# Create an AI instance optimized for Solidity coding\nai = ToolEnabledAI(model=selected_model.value if selected_model else None)\nresponse = ai.request(\"Write a simple Solidity smart contract for storing a value.\")\nprint(f\"Solidity AI Response: {response}\")\n</code></pre>"},{"location":"examples/configuration_example/#custom-configuration-with-multiple-options","title":"Custom Configuration with Multiple Options","text":"<pre><code>from src.config import configure\nfrom src.core.tool_enabled_ai import AI\n\n# Configure with multiple options\nconfigure(\n    model=\"claude-3-5-sonnet\",\n    temperature=0.8,\n    system_prompt=\"You are a helpful assistant specialized in Solidity smart contract development.\",\n    show_thinking=True,\n    max_tokens=2000\n)\n\n# Create an AI instance with the custom configuration\nai = AI()\nresponse = ai.request(\"What is the best way to implement a token vesting mechanism?\")\nprint(response)\n</code></pre>"},{"location":"examples/configuration_example/#loading-configuration-from-a-file","title":"Loading Configuration from a File","text":"<p>Create a file named <code>solidity_config.yml</code>:</p> <pre><code>model: claude-3-5-sonnet\nuse_case: solidity_coding\ntemperature: 0.8\nshow_thinking: true\nsystem_prompt: You are a helpful assistant specialized in Solidity smart contract development.\n</code></pre> <p>Then use it in your code:</p> <pre><code>from src.config import configure\nfrom src.core.tool_enabled_ai import AI\n\n# Load configuration from a file\nconfigure(config_file=\"solidity_config.yml\")\n\n# Create an AI instance using the loaded configuration\nai = AI()\nresponse = ai.request(\"Explain gas optimization techniques\")\nprint(response)\n</code></pre>"},{"location":"examples/configuration_example/#accessing-the-configuration-api-directly","title":"Accessing the Configuration API Directly","text":"<pre><code>from src.config import get_config, configure, get_available_models\nfrom src.core.tool_enabled_ai import AI\n\n# Configure the system\nconfigure(model=\"claude-3-5-sonnet\")\n\n# Get the configuration instance\nconfig = get_config()\n\n# Get all available models\navailable_models = get_available_models()\nprint(\"Available models:\", available_models)\n\n# Get the current default model\ndefault_model = config.get_default_model()\nprint(\"Default model:\", default_model)\n\n# Get the model configuration\nmodel_config = config.get_model_config(default_model)\nprint(\"Model name:\", model_config[\"name\"])\nprint(\"Provider:\", model_config[\"provider\"])\nprint(\"Quality:\", model_config[\"quality\"])\nprint(\"Speed:\", model_config[\"speed\"])\n\n# Create an AI instance\nai = AI()\nresponse = ai.request(\"Hello!\")\nprint(response)\n</code></pre>"},{"location":"examples/configuration_example/#using-with-the-orchestrator","title":"Using with the Orchestrator","text":"<pre><code>from src.config import configure, UseCasePreset\nfrom src.agents.orchestrator import Orchestrator\n\n# Configure for data analysis use case\nconfigure(\n    model=\"claude-3-5-sonnet\",\n    use_case=UseCasePreset.DATA_ANALYSIS,\n    show_thinking=True\n)\n\n# Create an orchestrator (it will use the configured settings)\norchestrator = Orchestrator()\n\n# Process a request\nresponse = orchestrator.process_request({\n    \"prompt\": \"Analyze the trends in this data: [1, 3, 6, 10, 15, 21, 28]\",\n    \"conversation_history\": []\n})\n\nprint(response[\"content\"])\n</code></pre>"},{"location":"examples/mcp_tool_usage/","title":"Example: Using Tools from an MCP Server","text":"<p>This example demonstrates how to configure Agentic-AI to connect to an external tool server using the Model Context Protocol (MCP) and utilize the tools it provides.</p>"},{"location":"examples/mcp_tool_usage/#1-configuration-configyml","title":"1. Configuration (<code>config.yml</code>)","text":"<p>First, you need to define the MCP server and the tools it declares it provides within your main configuration file (e.g., <code>config.yml</code>). This tells Agentic-AI how to connect to the server and what tools to expect.</p> <p>Assume we have an external weather service running as an MCP server accessible at <code>http://localhost:8005</code> which requires a bearer token for authentication.</p> <pre><code># Example snippet within your config.yml\n\n# ... other configurations ...\n\nmcp_servers:\n  weather_service_mcp:\n    description: \"External MCP server providing weather information.\"\n    url: \"http://localhost:8005\" # URL where the MCP server is running\n    auth:\n      type: \"bearer\"\n      token_env_var: \"WEATHER_MCP_TOKEN\" # Environment variable holding the auth token\n    provides_tools:\n      - name: \"get_current_weather_mcp\"\n        description: \"Retrieves the current weather conditions for a specified location from the external MCP server.\"\n        inputSchema: # Use inputSchema or parameters_schema\n          type: \"object\"\n          properties:\n            location:\n              type: \"string\"\n              description: \"The city and state/country (e.g., San Francisco, CA).\"\n            unit:\n              type: \"string\"\n              enum: [\"celsius\", \"fahrenheit\"]\n              default: \"celsius\"\n              description: \"Temperature unit.\"\n          required: [\"location\"]\n        # speed/safety will use defaults (medium/external)\n# ... other configurations ...\n</code></pre> <p>Key points:</p> <ul> <li><code>mcp_servers</code>: The top-level key containing all MCP server definitions.</li> <li><code>weather_service_mcp</code>: A unique name chosen for this server configuration.</li> <li><code>url</code>: The address where the Agentic-AI framework can reach the MCP server.</li> <li><code>auth</code>: Defines authentication. Here, <code>bearer</code> type indicates a token is needed, read from the environment variable specified by <code>token_env_var</code>.</li> <li><code>provides_tools</code>: A list of tools declared by this server. Agentic-AI uses this list to inform the LLM about available tools. The <code>name</code>, <code>description</code>, and <code>inputSchema</code> are crucial for the LLM. <code>source</code> and <code>mcp_server_name</code> are added internally by the framework.</li> </ul>"},{"location":"examples/mcp_tool_usage/#2-environment-setup","title":"2. Environment Setup","text":"<p>Since the configuration specifies <code>token_env_var: \"WEATHER_MCP_TOKEN\"</code>, you need to set this environment variable before running your Agentic-AI application.</p> <pre><code># Example in bash/zsh\nexport WEATHER_MCP_TOKEN=\"your_secret_mcp_api_token\"\n</code></pre> <p>Replace <code>\"your_secret_mcp_api_token\"</code> with the actual token provided by the weather service.</p>"},{"location":"examples/mcp_tool_usage/#3-code-example","title":"3. Code Example","text":"<p>Now, you can use the tools provided by the MCP server just like any other internal tool. The <code>ToolManager</code> (or an agent using it) handles the discovery and execution.</p> <pre><code># Example Python script (e.g., run_mcp_example.py)\n\nimport asyncio\nimport os\nfrom src.config import UnifiedConfig, get_config\nfrom src.tools import ToolManager\nfrom src.providers.openai import OpenAIProvider # Or any other provider\nfrom src.ai import ToolEnabledAI\n\n# Ensure the environment variable is set (replace with your actual token)\nos.environ[\"WEATHER_MCP_TOKEN\"] = \"your_secret_mcp_api_token\"\n# Replace with your OpenAI key if using OpenAI\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n\nasync def main():\n    # Load configuration (which includes mcp_servers)\n    # Assuming default config path or config object passed appropriately\n    config = get_config()\n\n    # Initialize components\n    tool_manager = ToolManager(config=config)\n    ai_provider = OpenAIProvider(config=config)\n    ai = ToolEnabledAI(provider=ai_provider, tool_manager=tool_manager, config=config)\n\n    # List all available tools (internal + MCP)\n    print(\"Available tools:\")\n    all_tools = tool_manager.list_tool_definitions()\n    for tool_def in all_tools:\n        print(f\"- {tool_def.name} (Source: {tool_def.source}, MCP Server: {tool_def.mcp_server_name or 'N/A'})\")\n\n    # Example prompt that should trigger the MCP tool\n    prompt = \"What's the current weather like in London?\"\n\n    print(f\"\\nSending prompt: '{prompt}'\")\n\n    # The AI will potentially call the 'get_current_weather_mcp' tool.\n    # The ToolManager will see source='mcp', get the client from MCPClientManager,\n    # and execute the call against the configured URL (http://localhost:8005).\n    # NOTE: This requires the MCP server to be running at that address.\n    #       The actual network call won't happen here unless a live server exists.\n    try:\n        response = await ai.process_prompt(prompt)\n        print(\"\\nFinal AI Response:\")\n        print(response.content)\n\n        # You can also inspect tool usage stats\n        stats = tool_manager.stats_manager.get_stats()\n        print(\"\\nTool Usage Stats:\")\n        print(stats)\n\n    except Exception as e:\n        print(f\"\\nAn error occurred: {e}\")\n        print(\"Ensure the MCP server is running at the configured URL and the API keys/tokens are correct.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/mcp_tool_usage/#4-explanation","title":"4. Explanation","text":"<p>When <code>ai.process_prompt</code> is called with a prompt like \"What's the current weather like in London?\":</p> <ol> <li>The <code>ToolEnabledAI</code> interacts with the AI Provider (e.g., OpenAI), providing the definition of <code>get_current_weather_mcp</code> (loaded from the config by <code>ToolManager</code> via <code>MCPClientManager</code>).</li> <li>The LLM identifies that <code>get_current_weather_mcp</code> is suitable and returns a request to call it with arguments like <code>{\"location\": \"London\"}</code>.</li> <li>The <code>ToolEnabledAI</code> passes this request to <code>ToolManager.execute_tool</code>.</li> <li><code>ToolManager</code> sees the tool's <code>source</code> is <code>mcp</code> and its <code>mcp_server_name</code> is <code>weather_service_mcp</code>.</li> <li><code>ToolManager</code> calls <code>MCPClientManager.get_tool_client(\"weather_service_mcp\")</code>.</li> <li><code>MCPClientManager</code> checks its configuration for <code>weather_service_mcp</code>, sees it's an HTTP URL with bearer auth, retrieves the token from the <code>WEATHER_MCP_TOKEN</code> environment variable, and creates/returns an internal HTTP client wrapper configured with the URL and authentication header.</li> <li><code>ToolManager</code> uses the returned client wrapper to make the actual HTTP POST request to <code>http://localhost:8005/call_tool</code> with the tool name and arguments. (If it were a WebSocket URL, <code>MCPClientManager</code> would establish a WS connection instead).</li> <li>The external MCP server (if running) processes the request and returns the weather data.</li> <li><code>ToolManager</code> receives the response, wraps it in a <code>ToolResult</code>, and returns it to <code>ToolEnabledAI</code>.</li> <li><code>ToolEnabledAI</code> sends the tool result back to the LLM.</li> <li>The LLM generates the final natural language response based on the weather data.</li> </ol> <p>This example illustrates how to integrate external functionalities into your AI agent by configuring access to MCP-compliant tool servers.</p>"},{"location":"examples/tool_stats_example/","title":"Tool Usage Statistics Example","text":"<p>This example demonstrates how to use the <code>ToolStatsManager</code> to track, analyze, and persist tool usage statistics.</p>"},{"location":"examples/tool_stats_example/#overview","title":"Overview","text":"<p>The <code>ToolStatsManager</code> allows you to:</p> <ul> <li>Track successful and failed tool executions</li> <li>Record and calculate performance metrics (execution duration)</li> <li>Save statistics to a JSON file for later analysis</li> <li>Load previously saved statistics</li> <li>Query statistics for specific tools or all tools</li> </ul>"},{"location":"examples/tool_stats_example/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/tool_stats_example/#initialization","title":"Initialization","text":"<pre><code>from src.tools.tool_stats_manager import ToolStatsManager\nfrom src.utils.logger import LoggerFactory\nfrom src.config import UnifiedConfig\n\n# Initialize with default configuration\nstats_manager = ToolStatsManager()\n\n# Or initialize with custom components\nstats_manager = ToolStatsManager(\n    logger=LoggerFactory.create(\"my_stats_logger\"),\n    unified_config=UnifiedConfig.get_instance()\n)\n</code></pre>"},{"location":"examples/tool_stats_example/#recording-tool-usage","title":"Recording Tool Usage","text":"<pre><code># Record a successful tool execution with duration\nstats_manager.update_stats(\n    tool_name=\"weather_tool\",\n    success=True,\n    duration_ms=250  # milliseconds\n)\n\n# Record a failed tool execution\nstats_manager.update_stats(\n    tool_name=\"search_tool\",\n    success=False\n)\n\n# You can optionally include a request ID for cross-referencing\nstats_manager.update_stats(\n    tool_name=\"calculator_tool\",\n    success=True,\n    duration_ms=50,\n    request_id=\"req-12345\"\n)\n</code></pre>"},{"location":"examples/tool_stats_example/#retrieving-statistics","title":"Retrieving Statistics","text":"<pre><code># Get statistics for a specific tool\nweather_stats = stats_manager.get_stats(\"weather_tool\")\nif weather_stats:\n    print(f\"Weather tool:\")\n    print(f\"- Used {weather_stats['uses']} times\")\n    print(f\"- Success rate: {weather_stats['successes']/weather_stats['uses']*100:.1f}%\")\n    print(f\"- Average duration: {weather_stats['avg_duration_ms']:.2f}ms\")\n\n# Get statistics for all tools\nall_stats = stats_manager.get_all_stats()\nfor tool_name, stats in all_stats.items():\n    print(f\"{tool_name}: {stats['uses']} uses, {stats['successes']} successes\")\n</code></pre>"},{"location":"examples/tool_stats_example/#saving-and-loading-statistics","title":"Saving and Loading Statistics","text":"<pre><code># Save statistics to the default path (from configuration)\nstats_manager.save_stats()\n\n# Save to a custom path\nstats_manager.save_stats(\"/path/to/custom_stats.json\")\n\n# Load statistics from the default path\nstats_manager.load_stats()\n\n# Load from a custom path\nstats_manager.load_stats(\"/path/to/custom_stats.json\")\n</code></pre>"},{"location":"examples/tool_stats_example/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example showing the full workflow:</p> <pre><code>import time\nimport random\nfrom src.tools.tool_stats_manager import ToolStatsManager\nfrom src.utils.logger import LoggerFactory\n\n# Create a ToolStatsManager with default configuration\nstats_manager = ToolStatsManager(\n    logger=LoggerFactory.create(\"example_logger\")\n)\n\n# Function to simulate tool executions\ndef simulate_tool_usage(tool_name, num_calls, success_rate):\n    print(f\"Simulating {num_calls} calls to {tool_name}...\")\n\n    for i in range(num_calls):\n        # Simulate success/failure based on success_rate\n        success = random.random() &lt; success_rate\n\n        # Simulate execution time (50-500ms)\n        duration_ms = int(random.uniform(50, 500))\n\n        # Update statistics\n        stats_manager.update_stats(\n            tool_name=tool_name,\n            success=success,\n            duration_ms=duration_ms\n        )\n\n        # Small delay for better timestamps\n        time.sleep(0.01)\n\n    print(f\"Completed simulation for {tool_name}\")\n\n# Simulate usage of different tools\nsimulate_tool_usage(\"weather_tool\", 5, 0.8)    # 80% success rate\nsimulate_tool_usage(\"calculator_tool\", 10, 0.9) # 90% success rate\nsimulate_tool_usage(\"search_tool\", 8, 0.75)     # 75% success rate\n\n# Save statistics\nstats_manager.save_stats()\nprint(f\"Statistics saved to: {stats_manager.stats_storage_path}\")\n\n# Print statistics for analysis\nall_stats = stats_manager.get_all_stats()\nprint(\"\\nTool Usage Summary:\")\nprint(\"-\" * 60)\nprint(f\"{'Tool Name':&lt;20} {'Uses':&lt;6} {'Success Rate':&lt;15} {'Avg Duration':&lt;15}\")\nprint(\"-\" * 60)\nfor tool_name, stats in all_stats.items():\n    success_rate = stats['successes'] / stats['uses'] * 100 if stats['uses'] &gt; 0 else 0\n    print(f\"{tool_name:&lt;20} {stats['uses']:&lt;6} {success_rate:&gt;6.1f}%        {stats['avg_duration_ms']:&gt;8.2f}ms\")\n</code></pre> <p>Example output:</p> <pre><code>Simulating 5 calls to weather_tool...\nCompleted simulation for weather_tool\nSimulating 10 calls to calculator_tool...\nCompleted simulation for calculator_tool\nSimulating 8 calls to search_tool...\nCompleted simulation for search_tool\nStatistics saved to: data/tool_stats.json\n\nTool Usage Summary:\n------------------------------------------------------------\nTool Name            Uses   Success Rate     Avg Duration\n------------------------------------------------------------\nweather_tool         5      80.0%            210.50ms\ncalculator_tool      10     90.0%            275.78ms\nsearch_tool          8      75.0%            226.33ms\n</code></pre>"},{"location":"examples/tool_stats_example/#integration-with-toolmanager","title":"Integration with ToolManager","text":"<p>In practice, you would normally use the <code>ToolStatsManager</code> indirectly through the <code>ToolManager</code>, which acts as a facade for the entire tools system:</p> <pre><code>from src.tools.tool_manager import ToolManager\n\ntool_manager = ToolManager()\n\n# Execute a tool - stats tracking happens automatically\nresult = tool_manager.execute_tool(\"weather_tool\", location=\"New York\")\n\n# Save statistics when needed\ntool_manager.save_usage_stats()\n\n# Get tool information including usage statistics\ntool_info = tool_manager.get_tool_info(\"weather_tool\")\nprint(f\"Weather tool used {tool_info['usage_stats']['uses']} times\")\n</code></pre>"},{"location":"examples/tool_stats_example/#configuration","title":"Configuration","text":"<p>Tool statistics tracking can be configured in the <code>tools.yml</code> configuration file:</p> <pre><code>stats:\n  track_usage: true # Set to false to disable tracking\n  storage_path: \"data/tool_stats.json\" # Custom storage path\n</code></pre> <p>For more details on how statistics are managed and stored, see the Tool Statistics Manager section in the main Tools documentation.</p>"},{"location":"prompt_templates/guide/","title":"Prompt Templates","text":"<p>This document describes the template system in Agentic-AI, which provides a way to create, manage, and reuse prompt templates with variable substitution and performance tracking.</p>"},{"location":"prompt_templates/guide/#overview","title":"Overview","text":"<p>The prompt template system allows you to:</p> <ol> <li>Define reusable prompt patterns with variable placeholders</li> <li>Version and track prompt performance</li> <li>Substitute variables into templates</li> <li>Collect metrics on template usage</li> </ol>"},{"location":"prompt_templates/guide/#template-format","title":"Template Format","text":"<p>Templates are defined in YAML files in the <code>src/prompts/templates</code> directory. Each template has:</p> <ul> <li>ID: Unique identifier for the template</li> <li>Name: Human-readable name</li> <li>Description: Description of the template's purpose</li> <li>Versions: Multiple versions of the template (for A/B testing or iteration)</li> <li>Default Version: Which version to use by default</li> </ul> <p>Example template file (<code>analysis.yml</code>):</p> <pre><code>analyze_request:\n  name: \"Request Analysis\"\n  description: \"Template for analyzing user requests to determine appropriate agents\"\n  default_version: \"v1\"\n  versions:\n    - version: \"v1\"\n      template: |\n        Analyze this user request and determine which specialized agents should handle it:\n\n        Request: {{prompt}}\n\n        Available agents:\n        {{agent_list}}\n\n        Return a JSON list of [agent_id, confidence] pairs, where confidence is 0.0-1.0.\n        Only include agents with confidence &gt; {{confidence_threshold}}. If no agents are appropriate, return [].\n</code></pre>"},{"location":"prompt_templates/guide/#variable-substitution","title":"Variable Substitution","text":"<p>Templates use a simple variable substitution syntax with double curly braces:</p> <ul> <li><code>{{variable_name}}</code> - Will be replaced with the value of <code>variable_name</code></li> </ul>"},{"location":"prompt_templates/guide/#using-templates-in-code","title":"Using Templates in Code","text":""},{"location":"prompt_templates/guide/#basic-usage","title":"Basic Usage","text":"<pre><code>from src.prompts.prompt_template import PromptTemplate\n\n# Create a template service\ntemplate_service = PromptTemplate()\n\n# Render a template with variables\nprompt, usage_id = template_service.render_prompt(\n    template_id=\"analyze_request\",\n    variables={\n        \"prompt\": \"What's the weather like in Paris?\",\n        \"agent_list\": \"- weather_agent: Gets weather information\\n- translator: Translates text\",\n        \"confidence_threshold\": 0.6\n    }\n)\n\nprint(prompt)\n\n# Record performance metrics\ntemplate_service.record_prompt_performance(\n    usage_id=usage_id,\n    metrics={\n        \"success\": True,\n        \"tokens\": 150,\n        \"latency\": 0.75\n    }\n)\n</code></pre>"},{"location":"prompt_templates/guide/#with-specific-version","title":"With Specific Version","text":"<pre><code>prompt, usage_id = template_service.render_prompt(\n    template_id=\"analyze_request\",\n    variables={\"prompt\": \"What's the weather like in Paris?\"},\n    version=\"v2\"  # Use a specific version\n)\n</code></pre>"},{"location":"prompt_templates/guide/#with-additional-context","title":"With Additional Context","text":"<pre><code>prompt, usage_id = template_service.render_prompt(\n    template_id=\"analyze_request\",\n    variables={\"prompt\": \"What's the weather like in Paris?\"},\n    context={\"model\": \"gpt-4\", \"user_id\": \"user123\"}  # Additional context\n)\n</code></pre>"},{"location":"prompt_templates/guide/#creating-new-templates","title":"Creating New Templates","text":"<p>To create a new template:</p> <ol> <li>Create or edit a YAML file in <code>src/prompts/templates/</code></li> <li>Define your template with an ID, name, description, and versions</li> <li>Use <code>{{variable_name}}</code> syntax for variables</li> <li>Set a default version if you have multiple versions</li> </ol> <p>Or programmatically:</p> <pre><code>template_service = PromptTemplate()\n\ntemplate_service.add_template(\n    template_id=\"my_template\",\n    template_data={\n        \"name\": \"My Template\",\n        \"description\": \"A template for...\",\n        \"default_version\": \"v1\",\n        \"versions\": [\n            {\n                \"version\": \"v1\",\n                \"template\": \"This is a template with {{variable}}\"\n            }\n        ]\n    }\n)\n</code></pre>"},{"location":"prompt_templates/guide/#performance-tracking","title":"Performance Tracking","text":"<p>The template system automatically tracks:</p> <ul> <li>Start time: When the template was rendered</li> <li>Variables used: What values were substituted</li> <li>Template version: Which version was used</li> </ul> <p>You can add additional metrics when recording performance:</p> <pre><code>template_service.record_prompt_performance(\n    usage_id=usage_id,\n    metrics={\n        \"success\": True,        # Whether the response was successful\n        \"latency\": 1.25,        # Processing time in seconds\n        \"tokens\": 150,          # Tokens used\n        \"model\": \"claude-3-5\",  # Model used\n        \"custom_metric\": 0.95   # Any custom metrics you want to track\n    }\n)\n</code></pre> <p>Metrics are saved to <code>src/prompts/templates/metrics/</code> as JSON files, which can be analyzed to improve prompts over time.</p>"},{"location":"prompt_templates/management/","title":"Prompt Management using PromptTemplate","text":""},{"location":"prompt_templates/management/#overview","title":"Overview","text":"<p>The primary way to manage and use prompts in Agentic AI is through the <code>PromptTemplate</code> service (<code>src/prompts/prompt_template.py</code>). This service allows you to:</p> <ul> <li>Define reusable prompt templates with variables in YAML files.</li> <li>Include multiple versions of a template within the same definition.</li> <li>Specify a default version for each template.</li> <li>Render specific template versions with variable substitution.</li> <li>Perform basic usage tracking for rendered prompts.</li> </ul>"},{"location":"prompt_templates/management/#defining-templates-in-yaml","title":"Defining Templates in YAML","text":"<p>Prompt templates are defined in YAML files located in a designated directory (default: <code>src/prompts/templates/</code>). Each file can contain definitions for one or more templates.</p> <p>The structure for a template definition is as follows:</p> <pre><code># Example: src/prompts/templates/coding_prompts.yaml\n\nexplain_code:\n  description: \"Explains a given code snippet.\"\n  default_version: \"v1.1\"\n  versions:\n    - version: \"v1.0\"\n      template: |\n        Explain the following {{language}} code:\n        ```{{language}}\n        {{code_snippet}}\n        ```\n    - version: \"v1.1\"\n      template: |\n        Act as an expert {{language}} programmer.\n        Provide a clear and concise explanation for this code snippet:\n        ```{{language}}\n        {{code_snippet}}\n        ```\n        Focus on the core logic and potential edge cases.\n\ngenerate_function:\n  description: \"Generates a function based on a description.\"\n  default_version: \"v1.0\"\n  versions:\n    - version: \"v1.0\"\n      template: |\n        Write a {{language}} function that does the following:\n        {{function_description}}\n</code></pre> <p>Key elements:</p> <ul> <li>Top-level key (<code>explain_code</code>, <code>generate_function</code>): This is the <code>template_id</code> used to reference the template.</li> <li><code>description</code>: A brief explanation of the template's purpose.</li> <li><code>default_version</code>: (Optional) The version string to use if no specific version is requested during rendering.</li> <li><code>versions</code>: A list of dictionaries, each representing a specific version.</li> <li><code>version</code>: A unique identifier string for this version (e.g., \"v1.0\", \"v1.1\", \"experimental\").</li> <li><code>template</code>: The actual prompt text for this version. Variables are enclosed in double curly braces (e.g., <code>{{language}}</code>, <code>{{code_snippet}}</code>).</li> </ul>"},{"location":"prompt_templates/management/#using-the-prompttemplate-service","title":"Using the PromptTemplate Service","text":"<p>The <code>PromptTemplate</code> service loads these YAML files upon initialization and provides methods to render them.</p> <pre><code>from src.core.tool_enabled_ai import ToolEnabledAI # Or your base AI class\nfrom src.prompts.prompt_template import PromptTemplate\n\n# Initialize the template service (loads templates from default directory)\n# You can optionally specify a different directory: PromptTemplate(templates_dir=\"path/to/custom/templates\")\ntemplate_service = PromptTemplate()\n\n# --- Integrate with AI instance ---\n# Pass the service instance during AI initialization\nai = ToolEnabledAI(\n    # ... other AI config like model, tools etc. ...\n    prompt_template=template_service\n)\n\n# --- Rendering a Prompt ---\n\n# 1. Define variables required by the template\nvariables = {\n    \"language\": \"Python\",\n    \"code_snippet\": \"def hello():\\n  print(\\\"Hello, World!\\\")\"\n}\n\n# 2. Request using the template ID (from the YAML file)\n# This will use the default version (\"v1.1\" in the example YAML)\nresponse_default = ai.request_with_template(\n    template_id=\"explain_code\",\n    variables=variables\n)\nprint(\"--- Default Version Response ---\")\nprint(response_default)\n\n# 3. Request a specific version\nresponse_v1 = ai.request_with_template(\n    template_id=\"explain_code\",\n    variables=variables,\n    version=\"v1.0\" # Specify the desired version string\n)\nprint(\"\\n--- Specific Version (v1.0) Response ---\")\nprint(response_v1)\n\n# --- Direct Rendering (Lower Level) ---\n# If you need to render without going through the AI's request method:\ntry:\n    rendered_prompt, usage_id = template_service.render_prompt(\n        template_id=\"explain_code\",\n        variables=variables,\n        version=\"v1.1\"\n    )\n    print(f\"\\n--- Directly Rendered Prompt (v1.1, Usage ID: {usage_id}) ---\")\n    print(rendered_prompt)\n\n    # Basic performance tracking can be done using the usage_id\n    # (Example - actual metrics depend on what you measure)\n    metrics = {\"tokens_used\": 150, \"latency_ms\": 550}\n    template_service.record_prompt_performance(usage_id, metrics)\n\nexcept ValueError as e:\n    print(f\"Error rendering prompt: {e}\")\n</code></pre>"},{"location":"prompt_templates/management/#key-methods-of-prompttemplate-service","title":"Key Methods of <code>PromptTemplate</code> Service","text":"<ul> <li><code>__init__(templates_dir=None, logger=None)</code>: Initializes the service, loading templates from the specified or default directory.</li> <li><code>render_prompt(template_id, variables=None, version=None, context=None)</code>: Finds the specified template and version (or default), substitutes variables, and returns the rendered prompt string along with a unique <code>usage_id</code> for tracking.</li> <li><code>record_prompt_performance(usage_id, metrics)</code>: Records performance data (like latency, token count, success status) associated with a specific <code>usage_id</code>. This data is typically saved to a file.</li> <li><code>get_template_ids()</code>: Returns a list of loaded template IDs.</li> <li><code>get_template_info(template_id)</code>: Returns the loaded data (description, versions, etc.) for a specific template ID.</li> <li><code>reload_templates()</code>: Clears the cache and reloads templates from the YAML files.</li> </ul>"},{"location":"prompt_templates/management/#note-on-promptmanager","title":"Note on <code>PromptManager</code>","text":"<p>There is another class, <code>PromptManager</code> (<code>src/prompts/prompt_manager.py</code>), which appears to be a more complex system focused on programmatic template/version creation, detailed metrics storage (in <code>metrics.json</code>), and A/B testing infrastructure. This system seems to operate independently of the YAML-based templates loaded by the <code>PromptTemplate</code> service. For standard usage involving predefined prompts, the <code>PromptTemplate</code> service described here is the recommended approach.</p>"},{"location":"providers/overview/","title":"AI Providers &amp; Models","text":""},{"location":"providers/overview/#supported-providers","title":"Supported Providers","text":"<p>Agentic-AI supports multiple AI model providers:</p> <ul> <li>Anthropic: Claude models (3.5 Haiku, 3.7 Sonnet, etc.)</li> <li>OpenAI: GPT models (4o, o3-mini, etc.)</li> <li>Google: Gemini models (2.5 Pro, 1.5 Pro, etc.)</li> <li>Ollama: Open-source models for local deployment</li> </ul> <p>Providers are supported via official SDKs.</p>"},{"location":"providers/overview/#models-configuration","title":"Models Configuration","text":"<p>Models are configured in the <code>config.yml</code> file:</p> <pre><code>models:\n  claude-3-7-sonnet:\n    name: \"Claude 3.7 Sonnet\"\n    model_id: claude-3-7-sonnet-20250219\n    provider: anthropic\n    privacy: EXTERNAL\n    quality: MAXIMUM\n    speed: STANDARD\n    parameters: 350000000000 # 350B\n    input_limit: 1000000\n    output_limit: 200000\n    temperature: 0.7\n    cost:\n      input_tokens: 3 # $3 per 1M input tokens\n      output_tokens: 15 # $15 per 1M output tokens\n      minimum_cost: 0.0001 # Minimum cost per request\n  ollama:\n    api_url: http://localhost:11434\n    # Ollama-specific settings\n</code></pre>"},{"location":"providers/overview/#model-selection","title":"Model Selection","text":"<p>Models are defined in the <code>models.py</code> file as an enumeration:</p> <pre><code>from enum import Enum\n\nclass Model(Enum):\n    # Anthropic models\n    CLAUDE_3_5_SONNET = \"claude-3-5-sonnet-20240620\"\n    CLAUDE_3_7_SONNET = \"claude-3-7-sonnet-20250219\"\n    CLAUDE_3_OPUS = \"claude-3-opus-20240229\"\n\n    # OpenAI models\n    GPT_4O = \"gpt-4o\"\n    GPT_4O_MINI = \"gpt-4o-mini\"\n    GPT_35_TURBO = \"gpt-3.5-turbo\"\n\n    # Google models\n    GEMINI_PRO = \"gemini-pro\"\n    GEMINI_2_5_PRO = \"gemini-2.5-pro\"\n\n    # Ollama models\n    LLAMA3_8B = \"llama3:8b\"\n    LLAMA3_70B = \"llama3:70b\"\n</code></pre> <p>And mapped to providers in the configuration:</p> <pre><code>models:\n  claude-3-5-sonnet-20240620:\n    provider: anthropic\n    model_id: claude-3-5-sonnet-20240620\n    ...\n  gpt-4o:\n    provider: openai\n    model_id: gpt-4o\n</code></pre>"},{"location":"providers/overview/#using-models","title":"Using Models","text":"<p>Create an AI instance with your chosen model:</p> <pre><code>from src.config.models import Model\nfrom src.core.tool_enabled_ai import ToolEnabledAI\n\n# Using an Anthropic model\nai = AI(model=Model.CLAUDE_3_7_SONNET)\n\n# Using an OpenAI model\nai = AI(model=Model.GPT_4O)\n\n# Using a Google model\nai = AI(model=Model.GEMINI_2_5_PRO)\n\n# Using an Ollama model\nai = AI(model=Model.GEMMA3-27B)\n</code></pre>"},{"location":"providers/overview/#model-selection-helper","title":"Model Selection Helper","text":"<p>The ModelSelector helps choose the appropriate model for different use cases:</p> <pre><code>from src.core.model_selector import ModelSelector, UseCase\n\n# Get a model recommendation\nrecommended_model = ModelSelector.select_model(\n    use_case=UseCase.CREATIVE_WRITING,\n    quality_preference=Quality.HIGH,\n    speed_preference=Speed.BALANCED\n)\n\n# Create AI with the recommended model\nai = AI(model=recommended_model)\n</code></pre>"},{"location":"tools/overview/","title":"Tool Integration","text":""},{"location":"tools/overview/#overview","title":"Overview","text":"<p>Tools allow the AI to perform actions and retrieve information beyond its training data. The Agentic-AI framework includes a tool management system that enables:</p> <ol> <li>Loading tool definitions from configuration (<code>tools.yml</code> for internal tools, the main <code>config.yml</code> for external MCP servers and their declared tools).</li> <li>Unified Discovery: Providing a single list of all available tools (both internal and MCP).</li> <li>Formatting tool definitions for specific AI providers.</li> <li>Executing tool calls requested by the AI asynchronously, dispatching to the correct executor (internal Python or MCP client) based on the tool's source, with error handling and timeout/retry logic.</li> </ol>"},{"location":"tools/overview/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TD\n    subgraph \"Users Of Tools\"\n        ToolEnabledAI -- \"Calls Execute (await)\" --&gt; ToolManager\n        BaseAgent -- \"Calls Execute (await)\" --&gt; ToolManager\n    end\n\n    subgraph \"Tools Subsystem\"\n        ToolManager -- \"Gets All Definitions\" --&gt; ToolRegistry[\"ToolRegistry (Internal Tools)\"]\n        ToolManager -- \"Gets All Definitions\" --&gt; MCPClientManager[\"MCPClientManager (MCP Tools)\"]\n\n        ToolManager -- \"Dispatches\" --&gt; ToolExecutor[\"ToolExecutor (Internal)\"]\n        ToolManager -- \"Dispatches\" --&gt; MCPClientManager\n\n        ToolManager -- \"Records Stats\" --&gt; ToolStatsManager\n\n        ToolExecutor -- \"Returns (awaitable)\" --&gt; ToolResult[\"ToolResult Model\"]\n        MCPClientManager -- \"Returns MCP Response\" --&gt; ToolManager\n\n        ToolRegistry -- \"Stores/Provides\" --&gt; InternalToolDef[\"ToolDefinition (source='internal')\"]\n        MCPClientManager -- \"Stores/Provides\" --&gt; MCPToolDef[\"ToolDefinition (source='mcp')\"]\n\n        ToolStatsManager -- \"Persists\" --&gt; StatsFile[\"Tool Stats JSON\"]\n\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; ToolRegistry\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; MCPClientManager\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; ToolExecutor\n        UnifiedConfigRef[\"UnifiedConfig\"] -- \"Provides Config\" --&gt; ToolStatsManager\n\n        ToolsYAML[\"tools.yml\"] -- \"Read By\" --&gt; UnifiedConfigRef\n        MCPYAML[\"mcp.yml\"] -- \"Read By\" --&gt; UnifiedConfigRef\n    end\n\n    subgraph \"Dependencies\"\n       ToolManager -- \"Uses\" --&gt; LoggerFactory[\"LoggerFactory\"]\n       ToolRegistry -- \"Uses\" --&gt; LoggerFactory\n       MCPClientManager -- \"Uses\" --&gt; LoggerFactory\n       ToolExecutor -- \"Uses\" --&gt; LoggerFactory\n       ToolStatsManager -- \"Uses\" --&gt; LoggerFactory\n       ToolManager -- \"Uses\" --&gt; UnifiedConfigRef\n    end\n\n    ToolEnabledAI -- \"Gets Formatted Tools\" --&gt; ToolManager\n\n    style ToolManager fill:#f9f,stroke:#333,stroke-width:2px\n    style ToolExecutor fill:#fdf,stroke:#333,stroke-width:1px\n    style MCPClientManager fill:#fdf,stroke:#333,stroke-width:1px\n    style ToolEnabledAI fill:#ccf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"tools/overview/#tool-components-async-aspects-highlighted","title":"Tool Components (Async Aspects Highlighted)","text":"<ul> <li>ToolManager: Central service coordinating asynchronous tool execution (<code>async def execute_tool</code>). It loads all tool definitions (internal and MCP) via <code>ToolRegistry</code> and <code>MCPClientManager</code>, provides unified discovery methods, handles formatting requests, and dispatches execution calls to <code>ToolExecutor</code> (for internal tools) or <code>MCPClientManager</code> (for MCP tools) based on the tool's <code>source</code>.</li> <li>ToolRegistry: Loads internal tool definitions (<code>ToolDefinition</code> with <code>source='internal'</code>) from <code>tools.yml</code> configuration, validates them, stores them, and provides methods to retrieve them.</li> <li>MCPClientManager: Loads MCP server configurations and declared MCP tool definitions (<code>ToolDefinition</code> with <code>source='mcp'</code>) from the main configuration file (<code>config.yml</code>). It manages connections to external MCP servers (using <code>aiohttp</code> for HTTP/S or <code>mcp.websocket_client</code> for WS) and handles the <code>call_tool</code> request for MCP tools.</li> <li>ToolExecutor: Executes only internal Python tool functions safely with asyncio-based timeout and retry logic (<code>async def execute</code>). Handles both synchronous and asynchronous tool functions.</li> <li>ToolStatsManager: Tracks and manages tool usage statistics for all tool types.</li> <li>Models: Defines core data structures (<code>ToolDefinition</code>, <code>ToolCall</code>, <code>ToolResult</code>).</li> <li>Configuration: Defines available internal tools (<code>tools.yml</code>) and external MCP servers/declared tools (within the main <code>config.yml</code> under the <code>mcp_servers</code> key).</li> <li>Users (e.g., <code>ToolEnabledAI</code>, <code>BaseAgent</code>): Components that utilize the tool system, typically interacting primarily with <code>ToolManager</code>. <code>ToolEnabledAI</code> handles the asynchronous interaction loop with the AI provider and the <code>ToolManager</code>.</li> </ul>"},{"location":"tools/overview/#defining-tools-configuration-based","title":"Defining Tools (Configuration-Based)","text":"<p>Tools are defined declaratively in YAML configuration files, specifying their source (\"internal\" or \"mcp\").</p>"},{"location":"tools/overview/#internal-tools-srcconfigtoolsyml","title":"Internal Tools (<code>src/config/tools.yml</code>)","text":"<p>These are Python functions defined within the project.</p> <ul> <li><code>name</code>: Unique name for the tool.</li> <li><code>description</code>: Clear description for the LLM.</li> <li><code>module</code>: Python module path (e.g., <code>src.tools.core.calculator_tool</code>).</li> <li><code>function</code>: Name of the Python function (sync or async) implementing the tool.</li> <li><code>parameters_schema</code>: JSON schema for input parameters.</li> <li><code>category</code> (optional): Grouping category.</li> <li><code>speed</code> (optional): Estimated speed (e.g., \"instant\", \"fast\", \"medium\", \"slow\"). Defaults to \"medium\".</li> <li><code>safety</code> (optional): Safety level (e.g., \"native\", \"sandboxed\", \"external\"). Defaults to \"native\".</li> <li><code>source</code>: Must be <code>\"internal\"</code>.</li> </ul> <p>Example (<code>tools.yml</code>):</p> <pre><code>tools:\n  - name: \"calculator\"\n    description: \"Perform mathematical calculations...\"\n    module: \"src.tools.core.calculator_tool\"\n    function: \"calculate\"\n    parameters_schema:\n      type: \"object\"\n      properties:\n        expression:\n          type: \"string\"\n          description: \"The mathematical expression...\"\n      required: [\"expression\"]\n    category: \"core_utils\"\n    source: \"internal\" # Explicitly internal\n    speed: \"fast\"\n    safety: \"sandboxed\"\n</code></pre>"},{"location":"tools/overview/#mcp-servers-and-declared-tools-in-configyml","title":"MCP Servers and Declared Tools (in <code>config.yml</code>)","text":"<p>The main configuration file (e.g., <code>config.yml</code> or loaded via <code>UnifiedConfig</code>) defines how to connect to external MCP servers under the <code>mcp_servers</code> key. It also lists the tools those servers declare they provide. The framework uses this declaration to inform the LLM; the actual tool list might differ when connecting to the server. The MCP server process must be running independently and accessible at the specified network address.</p> <ul> <li>The top-level key is <code>mcp_servers</code>: A dictionary where each key is a unique server name.</li> <li><code>description</code> (optional): Description of the server.</li> <li><code>url</code>: Network endpoint (e.g., <code>http://localhost:8001</code>) where the MCP server is listening. Supports <code>http</code>, <code>https</code>, <code>ws</code>, <code>wss</code> schemes. Required.</li> <li><code>auth</code> (optional): Authentication details for the server.<ul> <li><code>type</code>: The authentication type (e.g., <code>bearer</code>).</li> <li><code>token_env_var</code>: The name of the environment variable containing the authentication token.</li> </ul> </li> <li><code>provides_tools</code>: A list of declared tool definitions for this server.<ul> <li><code>name</code>: Unique name for the tool (must be unique across all tools).</li> <li><code>description</code>: Clear description for the LLM.</li> <li><code>inputSchema</code> (or <code>parameters_schema</code>): JSON schema for input parameters.</li> <li><code>speed</code> (optional): Estimated speed. Defaults to \"medium\".</li> <li><code>safety</code> (optional): Safety level. Defaults to \"external\".</li> <li><code>source</code>: Automatically set to <code>\"mcp\"</code> by <code>MCPClientManager</code>.</li> <li><code>mcp_server_name</code>: Automatically set to the server key by <code>MCPClientManager</code>.</li> </ul> </li> </ul> <p>Example (within <code>config.yml</code>):</p> <pre><code># Example within src/config/config.yml\n\nmcp_servers:\n  code_execution_server:\n    description: \"Server for executing Python code snippets securely.\"\n    url: \"http://localhost:8001\" # Example URL\n    auth:\n      type: \"bearer\"\n      token_env_var: \"CODE_EXEC_AUTH_TOKEN\"\n    provides_tools:\n      - name: \"execute_python_code\"\n        description: \"Executes a given Python code snippet in a restricted environment and returns the output.\"\n        inputSchema:\n          type: \"object\"\n          properties:\n            code:\n              type: \"string\"\n              description: \"The Python code to execute.\"\n            timeout_seconds:\n              type: \"integer\"\n              default: 10\n              description: \"Maximum execution time in seconds.\"\n          required: [\"code\"]\n        speed: \"medium\"\n        safety: \"sandboxed\" # Overrides default 'external'\n\n  web_search_server:\n    description: \"Server for performing web searches.\"\n    url: \"http://localhost:8002\" # Example URL\n    # No auth specified for this example\n    provides_tools:\n      - name: \"perform_web_search\"\n        description: \"Searches the web for a given query.\"\n        inputSchema:\n          type: \"object\"\n          properties:\n            query: { type: \"string\" }\n          required: [\"query\"]\n        # speed/safety use defaults (medium/external)\n</code></pre>"},{"location":"tools/overview/#tool-execution-flow-async","title":"Tool Execution Flow (Async)","text":"<p>The execution flow is now inherently asynchronous and handles dispatching based on the tool source:</p> <ol> <li>On startup, <code>ToolRegistry</code> loads internal tool definitions from <code>tools.yml</code> and <code>MCPClientManager</code> loads MCP server configs and declared tools from the main <code>config.yml</code>. <code>ToolManager</code> aggregates these into a unified list.</li> <li>When <code>ToolEnabledAI</code> needs to interact (<code>async def process_prompt</code>), it retrieves formatted tools for the target model via <code>ToolManager.format_tools_for_model(...)</code>.</li> <li><code>ToolEnabledAI</code> passes these definitions to the AI Provider during its asynchronous request (<code>await self._provider.request(...)</code>).</li> <li>The LLM decides if a tool needs to be called, returning <code>ToolCall</code> data in the <code>ProviderResponse</code>.</li> <li><code>ToolEnabledAI</code> parses the <code>ToolCall</code> data into <code>ToolCall</code> objects.</li> <li>For each <code>ToolCall</code>, <code>ToolEnabledAI</code> awaits <code>ToolManager.execute_tool(tool_call)</code>.</li> <li><code>ToolManager</code> retrieves the full <code>ToolDefinition</code> using the <code>tool_call.name</code>. If not found, it returns an error <code>ToolResult</code>.</li> <li><code>ToolManager</code> checks the <code>tool_definition.source</code>:<ul> <li>If <code>source == \"internal\"</code>:</li> <li><code>ToolManager</code> awaits <code>ToolExecutor.execute(tool_definition, **tool_call.arguments)</code>.</li> <li><code>ToolExecutor</code> resolves the function, handles sync/async execution with timeout/retry, and returns a <code>ToolResult</code>.</li> <li>If <code>source == \"mcp\"</code>:</li> <li><code>ToolManager</code> retrieves <code>mcp_server_name</code> from the definition.</li> <li><code>ToolManager</code> awaits <code>MCPClientManager.get_tool_client(mcp_server_name)</code> to get or create a client. This returns an internal <code>_HttpClientWrapper</code> for HTTP/S or the result from <code>mcp.websocket_client</code> context manager for WS.</li> <li>The client object (either the wrapper or the WebSocket client) is used to make the <code>call_tool</code> request with <code>tool_call.name</code> and <code>tool_call.arguments</code>. The wrapper handles the HTTP request internally, while the WebSocket client requires the appropriate protocol interaction (which <code>MCPClientManager</code> assumes the returned object supports).</li> <li><code>ToolManager</code> maps the raw MCP response (including potential errors) into a <code>ToolResult</code>.</li> </ul> </li> <li><code>ToolManager</code> updates usage statistics via <code>ToolStatsManager</code>.</li> <li><code>ToolManager</code> returns the <code>ToolResult</code> to <code>ToolEnabledAI</code>.</li> <li><code>ToolEnabledAI</code> formats the <code>ToolResult</code> into messages suitable for the AI provider (e.g., using helper methods in the provider implementation).</li> <li><code>ToolEnabledAI</code> awaits the provider again (<code>await self._provider.request(...)</code>) with the updated history including the tool result message.</li> <li>The loop continues or finishes, returning the final content.</li> </ol>"},{"location":"tools/overview/#provider-specific-tool-call-handling","title":"Provider-Specific Tool Call Handling","text":"<p>Different AI providers handle tool calls in slightly different ways. The framework normalizes these differences to provide a consistent experience:</p>"},{"location":"tools/overview/#openai-provider","title":"OpenAI Provider","text":"<p>The OpenAI provider automatically parses tool call arguments from JSON strings into Python dictionaries:</p> <ul> <li>Tool call arguments returned by the OpenAI API are JSON strings by default</li> <li>The <code>_convert_response</code> method automatically parses these strings into Python dictionaries</li> <li>If JSON parsing fails, the raw string is preserved in a <code>_raw_args</code> field</li> <li>This automatic parsing makes it easier to work with tool arguments in your code</li> </ul> <pre><code># Example of what happens internally when OpenAI returns a tool call\n# Original from OpenAI:\n# tool_call.function.arguments = '{\"location\": \"New York\", \"unit\": \"celsius\"}'\n\n# After _convert_response processing:\ntool_call.arguments = {\n    \"location\": \"New York\",\n    \"unit\": \"celsius\"\n}\n</code></pre> <p>This ensures that tool calls work consistently across different providers while taking advantage of each provider's specific capabilities.</p>"},{"location":"tools/overview/#tool-usage-statistics","title":"Tool Usage Statistics","text":"<p>The framework includes a dedicated component for tracking and analyzing tool usage:</p> <ul> <li>ToolStatsManager (<code>src/tools/tool_stats_manager.py</code>): Manages collection, storage, and retrieval of tool usage statistics.</li> </ul>"},{"location":"tools/overview/#features","title":"Features","text":"<ul> <li>Usage Tracking: Records each tool invocation, including success/failure status and execution duration.</li> <li>Performance Metrics: Calculates and maintains average execution durations for successful calls.</li> <li>Persistent Storage: Saves statistics to a JSON file for analysis and preservation between sessions.</li> <li>Configurable Behavior: Can be enabled/disabled and configured via the <code>UnifiedConfig</code> system.</li> </ul>"},{"location":"tools/overview/#configuration","title":"Configuration","text":"<p>Tool statistics tracking can be configured in `</p>"},{"location":"ui/gradio_interface/","title":"Gradio Chat Interface","text":"<p>The Agentic-AI framework includes a user-friendly chat interface built with Gradio (<code>src/ui/simple_chat.py</code>). This interface integrates with the agent system, primarily the <code>Coordinator</code>, to provide a seamless user experience.</p>"},{"location":"ui/gradio_interface/#features","title":"Features","text":"<ul> <li>Text-based chat interface</li> <li>Audio input support (microphone) with language selection</li> <li>Integration with the <code>Coordinator</code> agent for request processing</li> <li>Conversation history display</li> <li>Audio transcription status updates</li> </ul>"},{"location":"ui/gradio_interface/#architecture","title":"Architecture","text":"<p>The UI is built around the <code>SimpleChatUI</code> class, which:</p> <ol> <li>Takes an initialized <code>Coordinator</code> agent instance.</li> <li>Sets up the Gradio interface components (chatbot, text input, audio input, buttons).</li> <li>Handles message routing between the UI and the <code>Coordinator</code> agent.</li> <li>Processes both text input (<code>process_message</code>) and audio input (<code>process_audio</code>).</li> </ol>"},{"location":"ui/gradio_interface/#agent-integration","title":"Agent Integration","text":"<p>The chat interface primarily interacts with the agent system via the <code>Coordinator</code>:</p> <ul> <li><code>Coordinator</code> Agent: Receives text prompts or audio transcription requests from the UI and routes them appropriately (e.g., to a default chat agent, the <code>ListenerAgent</code> for audio, etc.).</li> </ul>"},{"location":"ui/gradio_interface/#example-usage-conceptual","title":"Example Usage (Conceptual)","text":"<p>While the UI can be run directly, here's a conceptual breakdown of its initialization:</p> <pre><code>from src.ui.simple_chat import SimpleChatUI\nfrom src.agents.coordinator import Coordinator\nfrom src.config import configure, UseCasePreset\nfrom src.agents.agent_factory import AgentFactory # Needed by Coordinator\nfrom src.agents.agent_registry import AgentRegistry # Needed by Factory\n\n# 1. Configure the framework (optional, defaults exist)\nconfigure(\n    model=\"claude-3-haiku\", # Example model\n    use_case=UseCasePreset.CHAT\n)\n\n# 2. Initialize dependencies for Coordinator\n# (These are often created internally by Coordinator if not provided)\nregistry = AgentRegistry()\n# Register necessary agents (like ListenerAgent, ChatAgent) in the registry...\n# Example: registry.register(\"listener_agent\", ListenerAgent)\n# Example: registry.register(\"chat_agent\", ChatAgent)\nagent_factory = AgentFactory(registry=registry)\n\n# 3. Create the Coordinator instance\n# It will use the globally configured settings and its dependencies\ncoordinator = Coordinator(agent_factory=agent_factory)\n\n# 4. Create the UI with the coordinator\nchat_ui = SimpleChatUI(coordinator=coordinator)\n\n# 5. Launch the interface\nchat_ui.launch(share=True) # share=True creates a public link\n</code></pre>"},{"location":"ui/gradio_interface/#running-the-ui","title":"Running the UI","text":"<p>The simplest way to run the UI is often via the main execution block within <code>simple_chat.py</code> itself, or a dedicated run script if provided.</p> <p>If run directly via <code>python src/ui/simple_chat.py</code>, the <code>run_simple_chat()</code> function within the file sets up a default configuration (e.g., using <code>claude-3-haiku</code> model and <code>CHAT</code> use case) and launches the interface.</p> <p>Check the <code>if __name__ == \"__main__\":</code> block in <code>src/ui/simple_chat.py</code> for potential command-line argument handling (though none seem implemented currently).</p>"},{"location":"ui/gradio_interface/#customization","title":"Customization","text":"<p>The UI can be customized in several ways:</p> <ol> <li>CSS Styling: Gradio interfaces support custom CSS. You might add CSS styling within the <code>build_interface</code> method.</li> <li>Component Layout: Customize the Gradio layout in the <code>build_interface</code> method within <code>SimpleChatUI</code>.</li> <li>Model Selection/Configuration: Modify the model, use case, etc., by calling <code>src.config.configure()</code> before initializing the UI or Coordinator, or by using environment variables/config files if supported by the configuration system.</li> <li>Agent Configuration: Update default agents used by the <code>Coordinator</code> in the <code>agents.yml</code> configuration file.</li> <li>API Keys: Ensure necessary API keys (e.g., <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>) are set as environment variables.</li> </ol>"},{"location":"ui/gradio_interface/#adding-new-agent-types","title":"Adding New Agent Types","text":"<p>Integrating new agents accessible via the UI typically involves:</p> <ol> <li>Implementing the new agent class.</li> <li>Registering the new agent with the <code>AgentRegistry</code>.</li> <li>Potentially modifying the <code>Coordinator</code> or <code>RequestAnalyzer</code> logic if the new agent needs specific routing beyond the default handling.</li> <li>If the agent requires unique UI elements, modifying the <code>build_interface</code> method in <code>SimpleChatUI</code>.</li> </ol>"},{"location":"ui/gradio_interface/#future-enhancements","title":"Future Enhancements","text":"<p>Planned enhancements for the UI include:</p> <ol> <li>File upload/download support</li> <li>Image/video display capabilities</li> <li>Custom visualization components for specialized agents</li> <li>Persistent conversation history</li> <li>User authentication and profiles</li> </ol>"}]}